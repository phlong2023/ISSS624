[
  {
    "objectID": "Take-Home_Ex/Take-Home_Ex1/Take-Home_Ex1.html",
    "href": "Take-Home_Ex/Take-Home_Ex1/Take-Home_Ex1.html",
    "title": "Take-Home_Ex1",
    "section": "",
    "text": "The bus system is one of Singapore’s two pillars of public transport aside from the MRT. The bus system ensures convenient and affordable short-, medium-, and long-distance travel for riders. Thanks to the widespread availability of bus stops as compared to MRT stations, it has a high level of accessibility. However, this also leaves the system prone to under- or over-investment in terms of the number of bus routes, leading some stops and routes to be under-served or over-served.\nThe objective of this study is to examine the distribution of bus trips in Singapore by analyzing the number of trips by originating bus stops. It will consist of two levels of analysis:\n\nGeoVisualisation and Analysis: Visualizing the number of trips by originating bus stops and provide descriptive statistics of the distribution of trips by bus stops.\nLocal Indicators of Spatial Association Analysis (LISA): This analysis involves the calculation of Local Moran’s I to determine local spatial autocorrelation between a bus stop and its neighbors. Additionally, visualizations such as a LISA cluster map will be created for easier comparison.\n\n\n\n\nFirst, the necessary R packages will be loaded using the p_load() function of the pacman package. p_load() will also install any package which is not already installed. The following packages will be loaded:\n\nsf: For handling of geospatial data.\nsfdep: For determining the spatial dependence of spatial features. The three main categories of functionality relates to the determination of geometry neighbors, weights, and LISA.\ntidyverse: For manipulation of non-spatial data. This package contains ggplot2 for plotting, dplyr and tidyr for dataframe manipulation, and readr for reading comma-separated values (CSV).\ntmap: For thematic mapping, especially the mapping of simple features data frame.\n\n\npacman::p_load(sf,sfdep,tidyverse,tmap)\n\n\n\n\nFor the purpose of this study, two types of data will be used: geospatial data which consists of spatial features and their coordinates information, and aspatial data which consists of attributes which can be ascribed to the geospatial data. Specifically, the following datasets will be used for each type:\n\nGeospatial Data:\n\nBusStop.shp: This shape file contains the location of the bus stops in Singapore as at July 2023. This file can be retrieved from the Land Transport Authority (LTA) Data Mall (link).\n\nAspatial Data:\n\norigin_destination_bus_202309.csv: This CSV file contains the detail of bus trips from an originating bus stop to a destination bus stop, identified by their unique codes, each hour of the day during September 2023. The data is further broken down into weekend or weekday, but not by the specific day of the week. This data can be retrieved by using the LTA Data Mall’s API (link).\n\n\nThe first steps taken will be to import these files into the R environment in a manipulable format.\n\n\nGeospatial data can be imported using the st_read() function of the sf package. This will import the file into the R environment as a sf (simple features) data frame. st_transform() is added to transform the Coordinate Reference System (CRS) to EPSG: 3414, which is the CRS of Singapore.\n\n\n\n\n\n\nNote\n\n\n\nIn st_read():\n\ndsn: the directory where the shape file is stored\nlayer: the name of the shape file\n\n\n\n\nbusstop &lt;- st_read(dsn = 'data/geospatial',\n                   layer = 'BusStop') %&gt;%\n  st_transform(crs = 3414)\n\nReading layer `BusStop' from data source \n  `D:\\phlong2023\\ISSS624\\Take-Home_Ex\\Take-Home_Ex1\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 5161 features and 3 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 3970.122 ymin: 26482.1 xmax: 48284.56 ymax: 52983.82\nProjected CRS: SVY21\n\n\nFrom the message provided by R, it can be seen that the busstop sf data frame has 5161 rows, 3 columns, and has a CRS of SVY 21.\nTo get a better grasp of the busstop data frame, glimpse() function can be used.\n\n\n\n\n\n\nNote\n\n\n\nThe data type for each column can be seen as well as some of their values. For sf data frames, there is a geometry column (POINT type) which contains the location information for each polygon.\n\n\n\nglimpse(busstop)\n\nRows: 5,161\nColumns: 4\n$ BUS_STOP_N &lt;chr&gt; \"22069\", \"32071\", \"44331\", \"96081\", \"11561\", \"66191\", \"2338…\n$ BUS_ROOF_N &lt;chr&gt; \"B06\", \"B23\", \"B01\", \"B05\", \"B05\", \"B03\", \"B02A\", \"B02\", \"B…\n$ LOC_DESC   &lt;chr&gt; \"OPP CEVA LOGISTICS\", \"AFT TRACK 13\", \"BLK 239\", \"GRACE IND…\n$ geometry   &lt;POINT [m]&gt; POINT (13576.31 32883.65), POINT (13228.59 44206.38),…\n\n\nAdditionally, busstop can be visualized in order to spot any anomaly. This can be done using the qtm() function in the tmap package for quick plotting.\n\nqtm(busstop)\n\n\n\n\nThe visualization shows us that there are four bus stops in Malaysia. Let’s remove them so that only bus stops in Singapore will be considered. This is because these special bus stops might exhibit different behaviors due to their different context from the rest of the bus stops in Singapore.\nfilter() can be used in conjunction with a dplyr step to remove these bus stops.\n\nbusstop &lt;- busstop %&gt;%\n  filter(!BUS_STOP_N %in% c('46609','47701', '46211', '46219', '46239'))\n\n\n\n\n\n\n\nNote\n\n\n\nqtm() can be used again to check that the bus stops have been removed.\n\n\n\n\n\nThe read_csv() function of readr can be used to import the origin_destination_bus_202309 CSV file into the R environment as a data frame.\n\npassenger &lt;- read_csv('data/aspatial/origin_destination_bus_202309.csv')\n\nFrom the message provided by R, it can be seen that the passenger has 5,714,196 rows and 7 columns.\nhead() can be used instead of glimpse() to view the top five rows of the passenger data frame. This will also allow us to see the data type of each of the column.\n\nhead(passenger)\n\n# A tibble: 6 × 7\n  YEAR_MONTH DAY_TYPE   TIME_PER_HOUR PT_TYPE ORIGIN_PT_CODE DESTINATION_PT_CODE\n  &lt;chr&gt;      &lt;chr&gt;              &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;          &lt;chr&gt;              \n1 2023-09    WEEKENDS/…            17 BUS     24499          22221              \n2 2023-09    WEEKENDS/…            10 BUS     65239          65159              \n3 2023-09    WEEKDAY               10 BUS     65239          65159              \n4 2023-09    WEEKDAY                7 BUS     23519          23311              \n5 2023-09    WEEKENDS/…             7 BUS     23519          23311              \n6 2023-09    WEEKENDS/…            11 BUS     52509          42041              \n# ℹ 1 more variable: TOTAL_TRIPS &lt;dbl&gt;\n\n\nNote that the ORIGIN_PT_CODE and DESTINATION_PT_CODE are in the character (“chr”) data type. However, we would like it to be in the factor (“fctr”) data type for easier categorization and sorting. This can be done by using the as.factor() function.\n\npassenger$ORIGIN_PT_CODE &lt;- as.factor(passenger$ORIGIN_PT_CODE)\npassenger$DESTINATION_PT_CODE &lt;- as.factor(passenger$DESTINATION_PT_CODE)\n\nWe can use head() to check the data type of the passenger data frame.\n\nhead(passenger)\n\n# A tibble: 6 × 7\n  YEAR_MONTH DAY_TYPE   TIME_PER_HOUR PT_TYPE ORIGIN_PT_CODE DESTINATION_PT_CODE\n  &lt;chr&gt;      &lt;chr&gt;              &lt;dbl&gt; &lt;chr&gt;   &lt;fct&gt;          &lt;fct&gt;              \n1 2023-09    WEEKENDS/…            17 BUS     24499          22221              \n2 2023-09    WEEKENDS/…            10 BUS     65239          65159              \n3 2023-09    WEEKDAY               10 BUS     65239          65159              \n4 2023-09    WEEKDAY                7 BUS     23519          23311              \n5 2023-09    WEEKENDS/…             7 BUS     23519          23311              \n6 2023-09    WEEKENDS/…            11 BUS     52509          42041              \n# ℹ 1 more variable: TOTAL_TRIPS &lt;dbl&gt;\n\n\n\n\n\n\nIn order to perform our analysis, certain manipulations must be made in order to prepare the data. Specifically, the passenger data set will be filtered and summarzied. Subsequently, it will be combined with the busstop data set based on the bus stop code variable present in both data frames.\n\n\n\n\nFor the purpose of this study, the passenger data set needs to be filtered to only contain trips falling within one of the following time frames:\n\n\n\nPeak hour period\nBus tap on time\n\n\n\n\nWeekday morning peak\n6am to 9am\n\n\nWeekday afternoon peak\n5pm to 8pm\n\n\nWeekend/holiday morning peak\n11am to 2pm\n\n\nWeekend/holiday evening peak\n4pm to 7pm\n\n\n\nThis can be accomplished using the filter() function and the dplyr steps. We can create four separate data frames to store the four different time frames\n\n# Weekday morning peak 6am - 9am\npassenger_wd_69 &lt;- passenger %&gt;%\n  filter(DAY_TYPE == 'WEEKDAY') %&gt;%\n  filter(TIME_PER_HOUR &gt;= 6 & TIME_PER_HOUR &lt;= 9)\n\n# Weekday afternoon peak 5pm - 8pm (17:00 - 20:00)\npassenger_wd_1720 &lt;- passenger %&gt;%\n  filter(DAY_TYPE == 'WEEKDAY') %&gt;%\n  filter(TIME_PER_HOUR &gt;= 17 & TIME_PER_HOUR &lt;= 20)\n\n# Weekend/holiday morning peak 11am - 2pm (11:00 - 14:00)\npassenger_weh_1114 &lt;- passenger %&gt;%\n  filter(DAY_TYPE == 'WEEKENDS/HOLIDAY') %&gt;%\n  filter(TIME_PER_HOUR &gt;= 11 & TIME_PER_HOUR &lt;= 14)\n\n# Weekend/holiday evening peak 4pm - 7pm (16:00 - 19:00)\npassenger_weh_1619 &lt;- passenger %&gt;%\n  filter(DAY_TYPE == 'WEEKENDS/HOLIDAY') %&gt;%\n  filter(TIME_PER_HOUR &gt;= 16 & TIME_PER_HOUR &lt;= 19)\n\nAfter the different trips have been categorized into their separate data frames, the total number trips for each origin bus stop can be tallied into a single statistic for the study period. This can be accomplished using the summarize() function. The example below shows this operation using passenger_wd_69.\n\n\n\n\n\n\nNote\n\n\n\nThe group_by() function is used to instruct R to conduct operations based on the groups created by group_by(). In this case, the summary operations will be done based on the origin bus stop codes.\n\n\n\n# Tallying the trips by origin bus stop for Weekday morning peak 6am - 9am\npassenger_wd_69_tallied &lt;- passenger_wd_69 %&gt;%\n  group_by(ORIGIN_PT_CODE) %&gt;%\n  summarise(TRIPS = sum(TOTAL_TRIPS))\n\npassenger_wd_69_tallied\n\n# A tibble: 5,020 × 2\n   ORIGIN_PT_CODE TRIPS\n   &lt;fct&gt;          &lt;dbl&gt;\n 1 01012           1640\n 2 01013            764\n 3 01019           1322\n 4 01029           2373\n 5 01039           2562\n 6 01059           1582\n 7 01109            144\n 8 01112           7993\n 9 01113           6734\n10 01119           3736\n# ℹ 5,010 more rows\n\n\nAs can be seen, the newly created data frame consists only of the total trip numbers for each origin bus stop. This can be repeated for the other time frames.\n\n# Tallying the trips by origin bus stop for Weekday afternoon peak 5pm - 8pm (17:00 - 20:00)\npassenger_wd_1720_tallied &lt;- passenger_wd_1720 %&gt;%\n  group_by(ORIGIN_PT_CODE) %&gt;%\n  summarise(TRIPS = sum(TOTAL_TRIPS))\n\n# Tallying the trips by origin bus stop for Weekend/holiday morning peak 11am - 2pm (11:00 - 14:00)\npassenger_weh_1114_tallied &lt;- passenger_weh_1114 %&gt;%\n  group_by(ORIGIN_PT_CODE) %&gt;%\n  summarise(TRIPS = sum(TOTAL_TRIPS))\n\n# Tallying the trips by origin bus stop for Weekend/holiday evening peak 4pm - 7pm (16:00 - 19:00)\npassenger_weh_1619_tallied &lt;- passenger_weh_1619 %&gt;%\n  group_by(ORIGIN_PT_CODE) %&gt;%\n  summarise(TRIPS = sum(TOTAL_TRIPS))\n\n\n\n\n\nIn order to adequately visualize the busstop sf data frame, we need to define a mapping layer. An example of a mapping layer would be to use the Master Plan 2019 Planning Sub-zone created by the Urban Redevelopment Authority (URA). However, for the purpose of this study, a hexagon layer will be used to ensure standardization of the size of each polygon and the evenly spaced gaps between a polygon and its neighbors.\nThe steps in this section will detail the creation of the hexagon layer using the busstop data frame and visualize the layer on a map of Singapore.\n\n\n\nThe steps taken in this section is based on the guide provided by Kenneth Wong of Urban Data Palette (link).\nFirstly, a hexagon or honeycomb grid can be created based on the busstop data frame using the st_make_grid() function.\n\n\n\n\n\n\nNote\n\n\n\nThere are some notable arguments in the st_make_grid() function:\n\ncellsize = c(100,100): This argument indicates the size of each hexagon. If the cell size is large, each hexagon can encompasses multiple bus stops, whereas if a smaller cell size can help us differentiate between individual bus stop. However, a smaller cell size with many hexagons will take more time to create.\nwhat = ‘polygons’: We would like to create polygons on a grid.\nsquare = FALSE: The default argument is TRUE, which would create a square grid. FALSE is specified in order to create a hexagon grid.\n\n\n\n\narea_honeycomb_grid = st_make_grid(busstop, cellsize = c(500,500), what = 'polygons', square = FALSE)\n\narea_honeycomb_grid\n\nGeometry set for 5040 features \nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 3470.122 ymin: 26193.43 xmax: 48720.12 ymax: 50586.47\nProjected CRS: SVY21 / Singapore TM\nFirst 5 geometries:\n\n\nThe area_honeycomb_grid contains 136906 features of the same Projected CRS as the busstop data frame. If the plot() function is used, the hexagon grid will be displayed. However, this grid contains no information and might be too small to discern the individual cell.\n\n#qtm(area_honeycomb_grid)\n\nThe area_honey_comb needs to be converted to a sf data frame for further manipulation using st_sf(). Additionally, we can assign a unique id to each of the hexagon cell in area_honey_comb using mutate().\n\nhoneycomb_grid_sf = st_sf(area_honeycomb_grid) %&gt;%\n  # add grid ID\n  mutate(grid_id = 1:length(lengths(area_honeycomb_grid)))\n\nFollowing this, we can use lengths() and st_intersect() to determine the allocation of bus stop in each cell. The goal is to create a new column, consisting of the number of bus stop in each of the cell. The filter() function can then be added to remove all cells with no bus stop and create the final sf data frame.\n\n# Counting the number of bus stop in each cell\nhoneycomb_grid_sf$n_busstop = lengths(st_intersects(honeycomb_grid_sf,busstop))\n\n# Removing all cells without bus stop\nhoneycomb_count = filter(honeycomb_grid_sf, n_busstop &gt; 0)\n\nAt this point, the hexagon grid of bus stop can be drawn onto a map of Singapore using the functions of the tmap package. Additionally, the n_busstop column can be passed to the tm_fill() function to shade the cell based on the number of bus stops in it.\n\n\n\n\n\n\nNote\n\n\n\nSeveral functions are added to make the map interactive and aesthetically pleasing\n\ntmap_mode(‘view’): Creates an interactive map which allow zooming and interacting with cells on the map\npop.vars: Identifying the legend and value which pops up when a cell is selected. In this case, it is the number of bus stops.\npopup.format: Specifying the format of the variable to be displayed when selecting a cell.\ntm_basemap: Choosing the basemap layer on which the hexagon grid will be drawn. OpenStreetMap is chosen due to its high fidelity while not being overly crowded. Additionally, OpenStreetMap displays icon for bus stops in Singapore, allowing user to visually check any cell.\n\nIf an incorrect CRS was specified in the earlier steps, the basemap will be of an incorrect location or alignment.\n\n\n\n\n\ntmap_mode('view')\n\nbushexmap &lt;- tm_shape(honeycomb_count)+\n  tm_fill(\n    col = \"n_busstop\",\n    palette = \"Blues\",\n    style = \"cont\",\n    title = \"Number of bus stop\",\n    id = \"grid_id\",\n    showNA = FALSE,\n    alpha = 0.7,\n    popup.vars = c(\n      \"Number of bus stop: \" = \"n_busstop\"\n    ),\n    popup.format = list(\n      n_busstop = list(format = \"f\", digits = 0)\n    )\n  ) +\n  tm_borders(col = \"grey40\", lwd = 0.7)+\n  tm_basemap(server = c('Esri.WorldGrayCanvas'))\n\nbushexmap\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nSearch for the bus stop near your place and compare the numbers between the four maps by zooming in! Do you think it’s accurate?\n\n\nBy looking at the illustration, we can see that each cell might contain up to ten bus stops. A bar chat can be drawn with the ggplot2 package to visualize the distribution of number of bus stop in each cell.\n\nggplot(honeycomb_count, aes(x=n_busstop))+\n  geom_bar()+\n  theme_classic()+\n  geom_text(aes(label = ..count..), stat = \"count\", vjust = -0.5, colour = \"black\")\n\n\n\n\nAs can be seen, the majority of cells contain 1-2 bus stop with only 214 cells containing more than 6 bus stops. This shows that the cells adequately capture solitary bus stop, as well as pairs of bus stops (bus stops which are opposite each other, served by the same bus services).\n\n\n\nIn order to conduct geospatial analysis, a data frame which contains the hexagon cells as well as the number of bus trips for each cells must be created. This can be done using the left_join argument.\n\n\n\n\n\n\nNote\n\n\n\nThere are important arguments which can be used to create a cleaner combined data frame.\n\nby = join_by(BUS_STOP_N == ORIGIN_PT_CODE)): Indicate the column by which the two data frames can be matched and joined. In this case, the bus stop code will be used.\nselect(1,4,5): Indicate the index number of the columns to be kept in the final data frame. Only the bus stop number (column 1), total number of trips (column 4), and geometry (column 5) will be kept.\nreplace(is.na(.),0): Replace all value of NA with 0. This is to ensure that bus stop with no trips in a given time frame is accurately tallied at 0.\n\n\n\n\n# Weekday morning peak 6am - 9am\npassenger_wd_69_combined &lt;- left_join(busstop, passenger_wd_69_tallied, by = join_by(BUS_STOP_N == ORIGIN_PT_CODE))%&gt;%\n  select(1,4,5)%&gt;%\n  replace(is.na(.),0)\n\n# Weekday afternoon peak 5pm - 8pm (17:00 - 20:00)\npassenger_wd_1720_combined &lt;- left_join(busstop, passenger_wd_1720_tallied, by = join_by(BUS_STOP_N == ORIGIN_PT_CODE))%&gt;%\n  select(1,4,5)%&gt;%\n  replace(is.na(.),0)\n\n# Weekend/holiday morning peak 11am - 2pm (11:00 - 14:00)\npassenger_weh_1114_combined &lt;- left_join(busstop, passenger_weh_1114_tallied, by = join_by(BUS_STOP_N == ORIGIN_PT_CODE))%&gt;%\n  select(1,4,5)%&gt;%\n  replace(is.na(.),0)\n\n# Weekend/holiday evening peak 4pm - 7pm (16:00 - 19:00)\npassenger_weh_1619_combined &lt;- left_join(busstop, passenger_weh_1619_tallied, by = join_by(BUS_STOP_N == ORIGIN_PT_CODE))%&gt;%\n  select(1,4,5)%&gt;%\n  replace(is.na(.),0)\n\nIt is important to note that the bus stops and their total trips have not been tallied into the hexagon cells. st_join() can be used to accomplish this for each time frame.\n\n\n\n\n\n\nNote\n\n\n\nThe by argument in st_join() can be passed the function st_within to specify that we would like to join the two data frames where the geometry in the latter is within the geometry of the former. In this case, it would mean that two rows will be joined where the bus stop lies within a particular polygon.\n\nThe group_by() and summarise() functions here are used similarly to before, they sums up the total number of trips for all the bus stops in the hexagon, based on its grid_id, and create a new column called TOTAL_TRIP.\n\n\n\n\n# Weekday morning peak 6am - 9am\nhex_passenger_wd_69 &lt;- st_join(honeycomb_count, passenger_wd_69_combined, by = st_within(sparse = FALSE), largest = TRUE) %&gt;%\n  group_by(grid_id)%&gt;%\n  summarise(TOTAL_TRIP = sum(TRIPS))\n\n# Weekday afternoon peak 5pm - 8pm (17:00 - 20:00)\nhex_passenger_wd_1720 &lt;- st_join(honeycomb_count, passenger_wd_1720_combined, by = st_within(sparse = FALSE), largest = TRUE) %&gt;%\n  group_by(grid_id)%&gt;%\n  summarise(TOTAL_TRIP = sum(TRIPS))\n\n# Weekend/holiday morning peak 11am - 2pm (11:00 - 14:00)\nhex_passenger_weh_1114 &lt;- st_join(honeycomb_count, passenger_weh_1114_combined, by = st_within(sparse = FALSE), largest = TRUE) %&gt;%\n  group_by(grid_id)%&gt;%\n  summarise(TOTAL_TRIP = sum(TRIPS))\n\n# Weekend/holiday evening peak 4pm - 7pm (16:00 - 19:00)\nhex_passenger_weh_1619 &lt;- st_join(honeycomb_count, passenger_weh_1619_combined, by = st_within(sparse = FALSE), largest = TRUE) %&gt;%\n  group_by(grid_id)%&gt;%\n  summarise(TOTAL_TRIP = sum(TRIPS))\n\nIn sum, the analysis will revolve around the four following data frames which contain the spatial information of the hexagon as well as the total number of trips for each interested time frame:\n\nhex_passenger_wd_69: Weekday morning peak 6am - 9am\nhex_passenger_wd_1720: Weekday afternoon peak 5pm - 8pm (17:00 - 20:00)\nhex_passenger_weh_1114: Weekend/holiday morning peak 11am - 2pm (11:00 - 14:00)\nhex_passenger_weh_1619: Weekend/holiday evening peak 4pm - 7pm (16:00 - 19:00)\n\n\n\n\n\nBefore we move on to Geovisualization and the analysis of LISA, it would be wise to remove objects which will no longer be used. This will help to free up memory for other tasks. rm() can be used to perform this task\n\nrm(list = c('area_honeycomb_grid', 'bushexmap', 'honeycomb_count', 'honeycomb_grid_sf',       'passenger_wd_1720', 'passenger_wd_1720_tallied', 'passenger_wd_1720_combined',    'passenger_wd_69', 'passenger_wd_69_tallied', 'passenger_wd_69_combined',      'passenger_weh_1114', 'passenger_weh_1114_tallied', 'passenger_weh_1114_combined',      'passenger_weh_1619', 'passenger_weh_1619_tallied', 'passenger_weh_1619_combined'))\n\n\n\n\nThe beginning step of the analysis would be to visualize the distribution of bus trips on the hexagon layer. This can be accomplished with the mapping functions of the tmap package. However, unlike the geovisualization of bus stop per hexagon, the number of trips for each hexagon depending on the time frame varies widely. Let’s confirm this by drawing a histogram of the distribution of trips in each time frame.\n\nweekday_morning_hist &lt;- ggplot(hex_passenger_wd_69, aes(x=TOTAL_TRIP))+\n  geom_histogram()+\n  scale_x_continuous(labels = scales::comma)+\n  ggtitle('Weekday Morning')+\n  theme_classic()\n\nweekday_evening_hist &lt;- ggplot(hex_passenger_wd_1720, aes(x=TOTAL_TRIP))+\n  geom_histogram()+\n  scale_x_continuous(labels = scales::comma)+\n  ggtitle('Weekday Evening')+\n  theme_classic()\n  \nweekend_noon_hist &lt;- ggplot(hex_passenger_weh_1114, aes(x=TOTAL_TRIP))+\n  geom_histogram()+\n  scale_x_continuous(labels = scales::comma)+\n  ggtitle('Weekend Noon')+\n  theme_classic()\n  \nweekend_evening_hist &lt;- ggplot(hex_passenger_weh_1619, aes(x=TOTAL_TRIP))+\n  geom_histogram()+\n  scale_x_continuous(labels = scales::comma)+\n  ggtitle('Weekend Evening')+\n  theme_classic()\n  \n  \ngridExtra::grid.arrange(weekday_morning_hist, weekday_evening_hist, weekend_noon_hist, weekend_evening_hist,\n                        nrow = 2, ncol = 2)\n\n\n\n\nUpon a brief inspection, it is possible to see that the range of trips between the different time periods are very different from each other, with Weekday Evening trips going above 300,000 for some hexagons, while Weekend Noon only ranging around 80,000 for its hexagons. By plotting this on map, we will get to see the geospatial distribution of the number of trips for each time frame.\nBefore we map all four time frames, however, it is important to consider that the ‘style’ argument of tm_fill() can take on many values (pretty, quantile, equal, etc.). In order to pick an appropriate ‘style’, we can pick one time frame and plot it using three different styles to determine the best way to depict the distribution.\n\ntmap_mode('view')\n\n### Weekday morning peak 6am - 9am\n# Quantile style\nweekday_morning_quantile &lt;- tm_shape(hex_passenger_wd_69) +\n  tm_fill(\n    col = \"TOTAL_TRIP\",\n    palette = \"Blues\",\n    style = \"quantile\",\n    title = \"Number of Trips\",\n    id = \"grid_id\",\n    showNA = FALSE,\n    alpha = 0.7,\n    popup.vars = c(\n      \"Number of Trips: \" = \"TOTAL_TRIP\"\n    ),\n    popup.format = list(\n      TOTAL_TRIP = list(format = \"f\", digits = 0)\n    )\n  ) +\n  tm_borders(col = \"grey40\", lwd = 0.7)+\n  tm_basemap(server = c('Esri.WorldGrayCanvas'))+\n  tm_layout(title = 'Weekday Morning Peak (Quantile)', title.position = c('right', 'top'))\n\n# Jenks style\nweekday_morning_jenks &lt;- tm_shape(hex_passenger_wd_69) +\n  tm_fill(\n    col = \"TOTAL_TRIP\",\n    palette = \"Blues\",\n    style = \"jenks\",\n    title = \"Number of Trips\",\n    id = \"grid_id\",\n    showNA = FALSE,\n    alpha = 0.7,\n    popup.vars = c(\n      \"Number of Trips: \" = \"TOTAL_TRIP\"\n    ),\n    popup.format = list(\n      TOTAL_TRIP = list(format = \"f\", digits = 0)\n    )\n  ) +\n  tm_borders(col = \"grey40\", lwd = 0.7)+\n  tm_basemap(server = c('Esri.WorldGrayCanvas'))+\n  tm_layout(title = 'Weekday Morning Peak (Jenks)', title.position = c('right', 'top'))\n\n# Equal style\nweekday_morning_equal &lt;- tm_shape(hex_passenger_wd_69) +\n  tm_fill(\n    col = \"TOTAL_TRIP\",\n    palette = \"Blues\",\n    style = \"equal\",\n    title = \"Number of Trips\",\n    id = \"grid_id\",\n    showNA = FALSE,\n    alpha = 0.7,\n    popup.vars = c(\n      \"Number of Trips: \" = \"TOTAL_TRIP\"\n    ),\n    popup.format = list(\n      TOTAL_TRIP = list(format = \"f\", digits = 0)\n    )\n  ) +\n  tm_borders(col = \"grey40\", lwd = 0.7)+\n  tm_basemap(server = c('Esri.WorldGrayCanvas'))+\n  tm_layout(title = 'Weekday Morning Peak (Equal)', title.position = c('right', 'top'))\n\ntmap_arrange(weekday_morning_quantile, weekday_morning_jenks, weekday_morning_equal, asp = 1, ncol = 3)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAs can be seen, the ‘quantile’ and ‘jenks’ style can better display the difference in distribution of trips between the different hexagons. This can be attributed to the fact that the range of trips between the hexagons are too large. However, the ‘quantile’ function suffers from its final grouping, containing values from roughly 5,539 to 136,490; this resulted int the oversaturation of the high value hexagons. On the other hand, the ‘jenks’ method “divides the features into classes whose boundaries are where there are relatively big differences in the data values” (Reference). Therefore, it is possible to move forward using the ‘jenks’ style for visualization, but by adjusting the breaks to provide more stratification in the hexagon colors for better visualization..\n\nIt is good to remove the 3 plots created to plot the different styles since they will not be used and will only take up memory.\n\nrm(list=c('weekday_morning_equal','weekday_morning_quantile','weekday_morning_jenks'))\n\n\nThe functions of the tmap package will be used to create the maps.\n\ntmap_mode('view')\n\n# Weekday morning peak 6am - 9am\nweekday_morning &lt;- tm_shape(hex_passenger_wd_69) +\n  tm_fill(\n    col = \"TOTAL_TRIP\",\n    palette = \"Blues\",\n    style = \"jenks\",\n    title = \"Number of Trips\",\n    id = \"grid_id\",\n    showNA = FALSE,\n    alpha = 0.7,\n    popup.vars = c(\n      \"Number of Trips: \" = \"TOTAL_TRIP\"\n    ),\n    popup.format = list(\n      TOTAL_TRIP = list(format = \"f\", digits = 0)\n    )\n  ) +\n  tm_borders(col = \"grey40\", lwd = 0.7)+\n  tm_basemap(server = c('Esri.WorldGrayCanvas'))+\n  tm_layout(title = 'Weekday Morning Peak', title.position = c('right', 'top'))\n\n\n# Weekday afternoon peak 5pm - 8pm (17:00 - 20:00)\nweekday_afternoon &lt;- tm_shape(hex_passenger_wd_1720) +\n  tm_fill(\n    col = \"TOTAL_TRIP\",\n    palette = \"Blues\",\n    style = \"jenks\",\n    title = \"Number of Trips\",\n    id = \"grid_id\",\n    showNA = FALSE,\n    alpha = 0.7,\n    popup.vars = c(\n      \"Number of Trips: \" = \"TOTAL_TRIP\"\n    ),\n    popup.format = list(\n      TOTAL_TRIP = list(format = \"f\", digits = 0)\n    )\n  ) +\n  tm_borders(col = \"grey40\", lwd = 0.7)+\n  tm_basemap(server = c('Esri.WorldGrayCanvas'))+\n  tm_layout(title = 'Weekday Afternoon Peak', title.position = c('right', 'top'))\n\n# Weekend/holiday morning peak 11am - 2pm (11:00 - 14:00)\nweekend_noon &lt;- tm_shape(hex_passenger_weh_1114) +\n  tm_fill(\n    col = \"TOTAL_TRIP\",\n    palette = \"Blues\",\n    style = \"jenks\",\n    title = \"Number of Trips\",\n    id = \"grid_id\",\n    showNA = FALSE,\n    alpha = 0.7,\n    popup.vars = c(\n      \"Number of Trips: \" = \"TOTAL_TRIP\"\n    ),\n    popup.format = list(\n      TOTAL_TRIP = list(format = \"f\", digits = 0)\n    )\n  ) +\n  tm_borders(col = \"grey40\", lwd = 0.7)+\n  tm_basemap(server = c('Esri.WorldGrayCanvas'))+\n  tm_layout(title = 'Weekend/holiday Morning Peak', title.position = c('right', 'top'))\n\n# Weekend/holiday evening peak 4pm - 7pm (16:00 - 19:00)\nweekend_evening &lt;- tm_shape(hex_passenger_weh_1619) +\n  tm_fill(\n    col = \"TOTAL_TRIP\",\n    palette = \"Blues\",\n    style = \"jenks\",\n    title = \"Number of Trips\",\n    id = \"grid_id\",\n    showNA = FALSE,\n    alpha = 0.7,\n    popup.vars = c(\n      \"Number of Trips: \" = \"TOTAL_TRIP\"\n    ),\n    popup.format = list(\n      TOTAL_TRIP = list(format = \"f\", digits = 0)\n    )\n  ) +\n  tm_borders(col = \"grey40\", lwd = 0.7)+\n  tm_basemap(server = c('Esri.WorldGrayCanvas'))+\n  tm_layout(title = 'Weekend/holiday Evening Peak', title.position = c('right', 'top'))\n\ntmap_arrange(weekday_morning, weekday_afternoon, weekend_noon, weekend_evening, nrow = 2, ncol = 2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nSearch for the bus stop near your place and compare the numbers between the four maps by zooming in! Do you think it’s accurate?\n\n\nFrom the rough visualization, it’s clear that not all bus stops experience a similar level of traffic throughout different timing of the days. Additionally, based on the quantiles created by tmap, it seems that the ranges of passenger traffic are radically different between the four time windows. For example, certain grids during Weekday Morning Peak, a hexagon could reach 328,545 passenger trips, whereas the highest number during Weekend/holiday Morning Peak only reaches 112,330. It appears that Weekend Evening Peak 17:00 - 20:00 is the period with the highest level of activity.\nAn important observation seems to be that there darker shaded hexagons tend to be clustered, indicating a positive spatial autocorrelation. However, there are clearly hexagons which are much darker than its surrounding tiles (such as the one near Tampines), indicating negative spatial autocorrelation. By conducting LISA analysis, it will be possible to determine the level of spatial autocorrelation for each hexagon, visualize them, as well as to depict the relationship between a hexagon and its neighbors through a LISA cluster map.\n\n\n\n\n\nBefore the LISA analysis can be conducted, it is important to define our neighborhood, or the neighbors of each polygon. This is based on the ideas that neighbors, or spatial objects which are related to other spatial objects based on sharing a common boundary or lying with a certain distance of one another, might affect each other.\nIn this case, we would like to identify the neighbors of each hexagon so that LISA analysis can determine if the number of trips in a hexagon in a time frame is correlated to the number of trips of the hexagons around it, either in the same direction or opposite direction.\nThe first step to determining the neighborhood is to choose a method by which neighbors are classified:\n\nContiguity-based Method: Based on the sharing of boundaries, either edges and/or points (Queen and Rook method).\nDistance-based Method: Based on the distances between the centroid (central point of each hexagon) of each polygon. This can either be set to a distance where each hexagon has at least one neighbor (Fixed Distance) or where each polygon has a certain number of neighbors (Adaptive Distance).\n\nIt is important to choose the appropriate method according to each situation. However, in this study, Contiguity-based methods can be preemptively ruled out due to the fact that some cells have no contiguous neighbors, as can be seen below.\n\n\n\nIsolated Cells\n\n\nIf the contiguity method is used, the LISA calculation for these cells would not be conducive for analysis as they technically have no neighbors. This can be confirmed by using the st_contiguity() function to create a Queen contiguity matrix for one time frame\n\nwm_q &lt;- st_contiguity(st_geometry(hex_passenger_wd_1720))\n\nsummary(wm_q)\n\nNeighbour list object:\nNumber of regions: 1520 \nNumber of nonzero links: 6874 \nPercentage nonzero weights: 0.2975242 \nAverage number of links: 4.522368 \n9 regions with no links:\n561 726 980 1047 1415 1505 1508 1512 1520\nLink number distribution:\n\n  0   1   2   3   4   5   6 \n  9  39 109 205 291 364 503 \n39 least connected regions:\n1 7 22 38 98 169 187 195 211 218 258 259 264 267 287 454 562 607 642 696 708 732 751 784 869 1021 1022 1046 1086 1214 1464 1471 1482 1500 1501 1503 1506 1510 1519 with 1 link\n503 most connected regions:\n10 13 16 17 24 25 31 35 42 43 48 53 55 60 63 67 73 77 80 81 84 85 87 88 91 92 97 102 107 111 117 121 127 133 140 141 143 148 149 150 154 155 156 157 163 164 165 173 174 175 183 184 185 191 192 193 194 200 201 202 205 206 207 208 216 229 239 243 244 246 257 266 271 278 279 283 284 291 292 298 300 301 302 304 309 310 312 313 316 321 324 325 327 337 338 339 340 343 352 355 363 368 390 391 400 402 403 407 414 418 423 425 431 436 437 438 440 443 450 451 452 461 466 467 468 469 473 477 480 481 485 489 493 494 496 502 503 507 513 514 517 518 523 529 534 539 543 548 549 550 552 556 558 564 568 573 574 576 577 581 590 591 594 598 599 604 605 609 615 619 624 626 633 636 637 638 648 649 650 654 655 657 658 659 669 670 671 677 680 681 682 687 688 690 691 700 701 704 705 706 713 716 717 724 727 728 729 740 741 755 757 758 760 771 774 775 776 777 782 783 787 788 789 792 793 794 795 799 800 806 807 810 811 812 813 819 820 823 824 825 830 831 832 841 843 844 846 847 848 850 851 852 853 854 860 863 865 866 867 871 872 876 877 878 880 881 882 884 885 887 888 891 893 896 899 902 905 906 910 914 919 921 926 927 928 930 931 935 937 943 944 945 946 947 948 954 958 959 962 963 968 969 971 972 973 977 984 985 986 987 988 990 996 997 998 999 1004 1011 1012 1013 1014 1024 1025 1026 1028 1029 1036 1037 1038 1042 1050 1051 1054 1056 1057 1062 1063 1064 1066 1067 1068 1069 1076 1078 1079 1080 1083 1089 1093 1100 1101 1102 1105 1106 1110 1111 1117 1120 1121 1122 1128 1133 1134 1135 1136 1141 1142 1144 1145 1146 1147 1148 1150 1156 1157 1158 1162 1163 1164 1166 1169 1170 1171 1172 1176 1177 1178 1179 1180 1184 1186 1190 1191 1192 1193 1194 1201 1202 1203 1204 1205 1206 1207 1210 1211 1217 1218 1219 1220 1221 1227 1233 1234 1235 1239 1244 1245 1251 1253 1254 1255 1261 1265 1266 1271 1272 1273 1277 1281 1283 1289 1299 1301 1302 1303 1304 1316 1318 1324 1325 1326 1327 1329 1330 1331 1334 1335 1336 1337 1343 1344 1345 1352 1353 1355 1356 1361 1365 1366 1368 1369 1371 1372 1377 1380 1381 1382 1384 1388 1391 1393 1395 1398 1406 1408 1412 1417 1418 1420 1424 1425 1426 1427 1428 1433 1434 1435 1436 1440 1441 1442 1446 1447 1449 1451 1453 1456 1457 1459 1460 1461 1462 1469 with 6 links\n\n\nAs can be seen from the weight matrix, 10 hexagon cells have 0 neighbor. Therefore, a Distance-based Method would be suitable for analysis. Both Fixed Distance and Adaptive Distance Weight Matrix can be created for comparison to find the most appropriate method.\n\n\n\n\n\nst_dist_band() of sfdep is incredibly powerful in that it can create a neighbor list based on distance between the centroid of polygons and a lower and upper bound distance to other centroid. The default arguments for st_dist_band() will find neighbors with a lower and upper bound so that each hexagon will have at least one neighbor. This is the equivalent of the steps in spdep of the function knearneigh() of k=1.\nst_inverse_distance() of a sfdep can be combined with st_dist_band() in a dplyr step to create a new column in each data frame of the different time frames to create a neighbor list and a inverse distance weight list. Additionally, st_lag() can be used to create a spatially lagged value column for total trips based on the weight of neighbors.\n\n# Weekday morning peak 6am - 9am\nwd69_nb &lt;- hex_passenger_wd_69 %&gt;%\n  mutate(nb = st_dist_band(area_honeycomb_grid),\n         wt = st_inverse_distance(nb, area_honeycomb_grid),\n         lag_trip = st_lag(TOTAL_TRIP,nb,wt),\n         .before = 1) # to put them in the front\n\n# Weekday afternoon peak 5pm - 8pm (17:00 - 20:00)\nwd1720_nb &lt;- hex_passenger_wd_1720 %&gt;%\n  mutate(nb = st_dist_band(area_honeycomb_grid),\n         wt = st_inverse_distance(nb, area_honeycomb_grid),\n         lag_trip = st_lag(TOTAL_TRIP,nb,wt),\n         .before = 1) # to put them in the front\n\n# Weekend/holiday morning peak 11am - 2pm (11:00 - 14:00)\nweh1114_nb &lt;- hex_passenger_weh_1114 %&gt;%\n  mutate(nb = st_dist_band(area_honeycomb_grid),\n         wt = st_inverse_distance(nb, area_honeycomb_grid),\n         lag_trip = st_lag(TOTAL_TRIP,nb,wt),\n         .before = 1) # to put them in the front\n\n# Weekend/holiday evening peak 4pm - 7pm (16:00 - 19:00)\nweh1619_nb &lt;- hex_passenger_weh_1619 %&gt;%\n  mutate(nb = st_dist_band(area_honeycomb_grid),\n         wt = st_inverse_distance(nb, area_honeycomb_grid),\n         lag_trip = st_lag(TOTAL_TRIP,nb,wt),\n         .before = 1) # to put them in the front\n\nSince the hexagon tiles is stable across all time frame, it is possible to plot the neighbor relationship for only one time frame in order to visualize the neighbors list created. Let’s take Weekday morning peak 6am - 9am. By visualizing, the appropriateness of the Fixed Distance Method can be roughly determined.\n\nplot(wd69_nb$area_honeycomb_grid, border = 'lightgrey')\nplot(st_knn(wd69_nb$area_honeycomb_grid, k = 1), st_centroid(wd69_nb$area_honeycomb_grid), add=TRUE, col = 'red', length = 0.08)\n\n\n\n\nBetween the Fixed Distance and Adaptive Distance Matrix, an Adaptive Distance Matrix would be more appropriate to the non-uniform nature of bus stop spatial distribution across the map. Using Adaptive Distance would allow for the specification of the number of neighbors, allowing for the standardisation of the LISA analysis process across hexagons.\n\n\n\nlocal_moran() of the sfdep package can be used to calculate Local Moran’s I Statistic and other related statistics.\n\n# Weekday morning peak 6am - 9am\nwd69_lisa &lt;- wd69_nb %&gt;%\n  mutate(local_moran = local_moran(TOTAL_TRIP, nb, wt, nsim = 199),\n         .before = 1) %&gt;%\n  unnest(local_moran)\n\n# Weekday afternoon peak 5pm - 8pm (17:00 - 20:00)\nwd1720_lisa &lt;- wd1720_nb %&gt;%\n  mutate(local_moran = local_moran(TOTAL_TRIP, nb, wt, nsim = 199),\n         .before = 1) %&gt;%\n  unnest(local_moran)\n\n# Weekend/holiday morning peak 11am - 2pm (11:00 - 14:00)\nweh1114_lisa &lt;- weh1114_nb %&gt;%\n  mutate(local_moran = local_moran(TOTAL_TRIP, nb, wt, nsim = 199),\n         .before = 1) %&gt;%\n  unnest(local_moran)\n\n# Weekend/holiday evening peak 4pm - 7pm (16:00 - 19:00)\nweh1619_lisa &lt;- weh1619_nb %&gt;%\n  mutate(local_moran = local_moran(TOTAL_TRIP, nb, wt, nsim = 199),\n         .before = 1) %&gt;%\n  unnest(local_moran)\n\n\ntmap_mode('view')\ntm_shape(wd1720_lisa)+\n  tm_fill(col = 'mean',\n          palette = \"RdBu\",\n    style = \"cat\",\n    title = \"ii\"\n  ) +\n  tm_borders(col = \"grey40\", lwd = 0.7)+\n  tm_basemap(server = c('Esri.WorldGrayCanvas'))"
  },
  {
    "objectID": "Take-Home_Ex/Take-Home_Ex1/Take-Home_Ex1.html#objectives",
    "href": "Take-Home_Ex/Take-Home_Ex1/Take-Home_Ex1.html#objectives",
    "title": "Take-Home_Ex1",
    "section": "",
    "text": "The bus system is one of Singapore’s two pillars of public transport aside from the MRT. The bus system ensures convenient and affordable short-, medium-, and long-distance travel for riders. Thanks to the widespread availability of bus stops as compared to MRT stations, it has a high level of accessibility. However, this also leaves the system prone to under- or over-investment in terms of the number of bus routes, leading some stops and routes to be under-served or over-served.\nThe objective of this study is to examine the distribution of bus trips in Singapore by analyzing the number of trips by originating bus stops. It will consist of two levels of analysis:\n\nGeoVisualisation and Analysis: Visualizing the number of trips by originating bus stops and provide descriptive statistics of the distribution of trips by bus stops.\nLocal Indicators of Spatial Association Analysis (LISA): This analysis involves the calculation of Local Moran’s I to determine local spatial autocorrelation between a bus stop and its neighbors. Additionally, visualizations such as a LISA cluster map will be created for easier comparison."
  },
  {
    "objectID": "Take-Home_Ex/Take-Home_Ex1/Take-Home_Ex1.html#getting-started",
    "href": "Take-Home_Ex/Take-Home_Ex1/Take-Home_Ex1.html#getting-started",
    "title": "Take-Home_Ex1",
    "section": "",
    "text": "First, the necessary R packages will be loaded using the p_load() function of the pacman package. p_load() will also install any package which is not already installed. The following packages will be loaded:\n\nsf: For handling of geospatial data.\nsfdep: For determining the spatial dependence of spatial features. The three main categories of functionality relates to the determination of geometry neighbors, weights, and LISA.\ntidyverse: For manipulation of non-spatial data. This package contains ggplot2 for plotting, dplyr and tidyr for dataframe manipulation, and readr for reading comma-separated values (CSV).\ntmap: For thematic mapping, especially the mapping of simple features data frame.\n\n\npacman::p_load(sf,sfdep,tidyverse,tmap)"
  },
  {
    "objectID": "Take-Home_Ex/Take-Home_Ex1/Take-Home_Ex1.html#importing-required-data",
    "href": "Take-Home_Ex/Take-Home_Ex1/Take-Home_Ex1.html#importing-required-data",
    "title": "Take-Home_Ex1",
    "section": "",
    "text": "For the purpose of this study, two types of data will be used: geospatial data which consists of spatial features and their coordinates information, and aspatial data which consists of attributes which can be ascribed to the geospatial data. Specifically, the following datasets will be used for each type:\n\nGeospatial Data:\n\nBusStop.shp: This shape file contains the location of the bus stops in Singapore as at July 2023. This file can be retrieved from the Land Transport Authority (LTA) Data Mall (link).\n\nAspatial Data:\n\norigin_destination_bus_202309.csv: This CSV file contains the detail of bus trips from an originating bus stop to a destination bus stop, identified by their unique codes, each hour of the day during September 2023. The data is further broken down into weekend or weekday, but not by the specific day of the week. This data can be retrieved by using the LTA Data Mall’s API (link).\n\n\nThe first steps taken will be to import these files into the R environment in a manipulable format.\n\n\nGeospatial data can be imported using the st_read() function of the sf package. This will import the file into the R environment as a sf (simple features) data frame. st_transform() is added to transform the Coordinate Reference System (CRS) to EPSG: 3414, which is the CRS of Singapore.\n\n\n\n\n\n\nNote\n\n\n\nIn st_read():\n\ndsn: the directory where the shape file is stored\nlayer: the name of the shape file\n\n\n\n\nbusstop &lt;- st_read(dsn = 'data/geospatial',\n                   layer = 'BusStop') %&gt;%\n  st_transform(crs = 3414)\n\nReading layer `BusStop' from data source \n  `D:\\phlong2023\\ISSS624\\Take-Home_Ex\\Take-Home_Ex1\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 5161 features and 3 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 3970.122 ymin: 26482.1 xmax: 48284.56 ymax: 52983.82\nProjected CRS: SVY21\n\n\nFrom the message provided by R, it can be seen that the busstop sf data frame has 5161 rows, 3 columns, and has a CRS of SVY 21.\nTo get a better grasp of the busstop data frame, glimpse() function can be used.\n\n\n\n\n\n\nNote\n\n\n\nThe data type for each column can be seen as well as some of their values. For sf data frames, there is a geometry column (POINT type) which contains the location information for each polygon.\n\n\n\nglimpse(busstop)\n\nRows: 5,161\nColumns: 4\n$ BUS_STOP_N &lt;chr&gt; \"22069\", \"32071\", \"44331\", \"96081\", \"11561\", \"66191\", \"2338…\n$ BUS_ROOF_N &lt;chr&gt; \"B06\", \"B23\", \"B01\", \"B05\", \"B05\", \"B03\", \"B02A\", \"B02\", \"B…\n$ LOC_DESC   &lt;chr&gt; \"OPP CEVA LOGISTICS\", \"AFT TRACK 13\", \"BLK 239\", \"GRACE IND…\n$ geometry   &lt;POINT [m]&gt; POINT (13576.31 32883.65), POINT (13228.59 44206.38),…\n\n\nAdditionally, busstop can be visualized in order to spot any anomaly. This can be done using the qtm() function in the tmap package for quick plotting.\n\nqtm(busstop)\n\n\n\n\nThe visualization shows us that there are four bus stops in Malaysia. Let’s remove them so that only bus stops in Singapore will be considered. This is because these special bus stops might exhibit different behaviors due to their different context from the rest of the bus stops in Singapore.\nfilter() can be used in conjunction with a dplyr step to remove these bus stops.\n\nbusstop &lt;- busstop %&gt;%\n  filter(!BUS_STOP_N %in% c('46609','47701', '46211', '46219', '46239'))\n\n\n\n\n\n\n\nNote\n\n\n\nqtm() can be used again to check that the bus stops have been removed.\n\n\n\n\n\nThe read_csv() function of readr can be used to import the origin_destination_bus_202309 CSV file into the R environment as a data frame.\n\npassenger &lt;- read_csv('data/aspatial/origin_destination_bus_202309.csv')\n\nFrom the message provided by R, it can be seen that the passenger has 5,714,196 rows and 7 columns.\nhead() can be used instead of glimpse() to view the top five rows of the passenger data frame. This will also allow us to see the data type of each of the column.\n\nhead(passenger)\n\n# A tibble: 6 × 7\n  YEAR_MONTH DAY_TYPE   TIME_PER_HOUR PT_TYPE ORIGIN_PT_CODE DESTINATION_PT_CODE\n  &lt;chr&gt;      &lt;chr&gt;              &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;          &lt;chr&gt;              \n1 2023-09    WEEKENDS/…            17 BUS     24499          22221              \n2 2023-09    WEEKENDS/…            10 BUS     65239          65159              \n3 2023-09    WEEKDAY               10 BUS     65239          65159              \n4 2023-09    WEEKDAY                7 BUS     23519          23311              \n5 2023-09    WEEKENDS/…             7 BUS     23519          23311              \n6 2023-09    WEEKENDS/…            11 BUS     52509          42041              \n# ℹ 1 more variable: TOTAL_TRIPS &lt;dbl&gt;\n\n\nNote that the ORIGIN_PT_CODE and DESTINATION_PT_CODE are in the character (“chr”) data type. However, we would like it to be in the factor (“fctr”) data type for easier categorization and sorting. This can be done by using the as.factor() function.\n\npassenger$ORIGIN_PT_CODE &lt;- as.factor(passenger$ORIGIN_PT_CODE)\npassenger$DESTINATION_PT_CODE &lt;- as.factor(passenger$DESTINATION_PT_CODE)\n\nWe can use head() to check the data type of the passenger data frame.\n\nhead(passenger)\n\n# A tibble: 6 × 7\n  YEAR_MONTH DAY_TYPE   TIME_PER_HOUR PT_TYPE ORIGIN_PT_CODE DESTINATION_PT_CODE\n  &lt;chr&gt;      &lt;chr&gt;              &lt;dbl&gt; &lt;chr&gt;   &lt;fct&gt;          &lt;fct&gt;              \n1 2023-09    WEEKENDS/…            17 BUS     24499          22221              \n2 2023-09    WEEKENDS/…            10 BUS     65239          65159              \n3 2023-09    WEEKDAY               10 BUS     65239          65159              \n4 2023-09    WEEKDAY                7 BUS     23519          23311              \n5 2023-09    WEEKENDS/…             7 BUS     23519          23311              \n6 2023-09    WEEKENDS/…            11 BUS     52509          42041              \n# ℹ 1 more variable: TOTAL_TRIPS &lt;dbl&gt;"
  },
  {
    "objectID": "Take-Home_Ex/Take-Home_Ex1/Take-Home_Ex1.html#data-preparation",
    "href": "Take-Home_Ex/Take-Home_Ex1/Take-Home_Ex1.html#data-preparation",
    "title": "Take-Home_Ex1",
    "section": "",
    "text": "In order to perform our analysis, certain manipulations must be made in order to prepare the data. Specifically, the passenger data set will be filtered and summarzied. Subsequently, it will be combined with the busstop data set based on the bus stop code variable present in both data frames.\n\n\n\n\nFor the purpose of this study, the passenger data set needs to be filtered to only contain trips falling within one of the following time frames:\n\n\n\nPeak hour period\nBus tap on time\n\n\n\n\nWeekday morning peak\n6am to 9am\n\n\nWeekday afternoon peak\n5pm to 8pm\n\n\nWeekend/holiday morning peak\n11am to 2pm\n\n\nWeekend/holiday evening peak\n4pm to 7pm\n\n\n\nThis can be accomplished using the filter() function and the dplyr steps. We can create four separate data frames to store the four different time frames\n\n# Weekday morning peak 6am - 9am\npassenger_wd_69 &lt;- passenger %&gt;%\n  filter(DAY_TYPE == 'WEEKDAY') %&gt;%\n  filter(TIME_PER_HOUR &gt;= 6 & TIME_PER_HOUR &lt;= 9)\n\n# Weekday afternoon peak 5pm - 8pm (17:00 - 20:00)\npassenger_wd_1720 &lt;- passenger %&gt;%\n  filter(DAY_TYPE == 'WEEKDAY') %&gt;%\n  filter(TIME_PER_HOUR &gt;= 17 & TIME_PER_HOUR &lt;= 20)\n\n# Weekend/holiday morning peak 11am - 2pm (11:00 - 14:00)\npassenger_weh_1114 &lt;- passenger %&gt;%\n  filter(DAY_TYPE == 'WEEKENDS/HOLIDAY') %&gt;%\n  filter(TIME_PER_HOUR &gt;= 11 & TIME_PER_HOUR &lt;= 14)\n\n# Weekend/holiday evening peak 4pm - 7pm (16:00 - 19:00)\npassenger_weh_1619 &lt;- passenger %&gt;%\n  filter(DAY_TYPE == 'WEEKENDS/HOLIDAY') %&gt;%\n  filter(TIME_PER_HOUR &gt;= 16 & TIME_PER_HOUR &lt;= 19)\n\nAfter the different trips have been categorized into their separate data frames, the total number trips for each origin bus stop can be tallied into a single statistic for the study period. This can be accomplished using the summarize() function. The example below shows this operation using passenger_wd_69.\n\n\n\n\n\n\nNote\n\n\n\nThe group_by() function is used to instruct R to conduct operations based on the groups created by group_by(). In this case, the summary operations will be done based on the origin bus stop codes.\n\n\n\n# Tallying the trips by origin bus stop for Weekday morning peak 6am - 9am\npassenger_wd_69_tallied &lt;- passenger_wd_69 %&gt;%\n  group_by(ORIGIN_PT_CODE) %&gt;%\n  summarise(TRIPS = sum(TOTAL_TRIPS))\n\npassenger_wd_69_tallied\n\n# A tibble: 5,020 × 2\n   ORIGIN_PT_CODE TRIPS\n   &lt;fct&gt;          &lt;dbl&gt;\n 1 01012           1640\n 2 01013            764\n 3 01019           1322\n 4 01029           2373\n 5 01039           2562\n 6 01059           1582\n 7 01109            144\n 8 01112           7993\n 9 01113           6734\n10 01119           3736\n# ℹ 5,010 more rows\n\n\nAs can be seen, the newly created data frame consists only of the total trip numbers for each origin bus stop. This can be repeated for the other time frames.\n\n# Tallying the trips by origin bus stop for Weekday afternoon peak 5pm - 8pm (17:00 - 20:00)\npassenger_wd_1720_tallied &lt;- passenger_wd_1720 %&gt;%\n  group_by(ORIGIN_PT_CODE) %&gt;%\n  summarise(TRIPS = sum(TOTAL_TRIPS))\n\n# Tallying the trips by origin bus stop for Weekend/holiday morning peak 11am - 2pm (11:00 - 14:00)\npassenger_weh_1114_tallied &lt;- passenger_weh_1114 %&gt;%\n  group_by(ORIGIN_PT_CODE) %&gt;%\n  summarise(TRIPS = sum(TOTAL_TRIPS))\n\n# Tallying the trips by origin bus stop for Weekend/holiday evening peak 4pm - 7pm (16:00 - 19:00)\npassenger_weh_1619_tallied &lt;- passenger_weh_1619 %&gt;%\n  group_by(ORIGIN_PT_CODE) %&gt;%\n  summarise(TRIPS = sum(TOTAL_TRIPS))\n\n\n\n\n\nIn order to adequately visualize the busstop sf data frame, we need to define a mapping layer. An example of a mapping layer would be to use the Master Plan 2019 Planning Sub-zone created by the Urban Redevelopment Authority (URA). However, for the purpose of this study, a hexagon layer will be used to ensure standardization of the size of each polygon and the evenly spaced gaps between a polygon and its neighbors.\nThe steps in this section will detail the creation of the hexagon layer using the busstop data frame and visualize the layer on a map of Singapore.\n\n\n\nThe steps taken in this section is based on the guide provided by Kenneth Wong of Urban Data Palette (link).\nFirstly, a hexagon or honeycomb grid can be created based on the busstop data frame using the st_make_grid() function.\n\n\n\n\n\n\nNote\n\n\n\nThere are some notable arguments in the st_make_grid() function:\n\ncellsize = c(100,100): This argument indicates the size of each hexagon. If the cell size is large, each hexagon can encompasses multiple bus stops, whereas if a smaller cell size can help us differentiate between individual bus stop. However, a smaller cell size with many hexagons will take more time to create.\nwhat = ‘polygons’: We would like to create polygons on a grid.\nsquare = FALSE: The default argument is TRUE, which would create a square grid. FALSE is specified in order to create a hexagon grid.\n\n\n\n\narea_honeycomb_grid = st_make_grid(busstop, cellsize = c(500,500), what = 'polygons', square = FALSE)\n\narea_honeycomb_grid\n\nGeometry set for 5040 features \nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 3470.122 ymin: 26193.43 xmax: 48720.12 ymax: 50586.47\nProjected CRS: SVY21 / Singapore TM\nFirst 5 geometries:\n\n\nThe area_honeycomb_grid contains 136906 features of the same Projected CRS as the busstop data frame. If the plot() function is used, the hexagon grid will be displayed. However, this grid contains no information and might be too small to discern the individual cell.\n\n#qtm(area_honeycomb_grid)\n\nThe area_honey_comb needs to be converted to a sf data frame for further manipulation using st_sf(). Additionally, we can assign a unique id to each of the hexagon cell in area_honey_comb using mutate().\n\nhoneycomb_grid_sf = st_sf(area_honeycomb_grid) %&gt;%\n  # add grid ID\n  mutate(grid_id = 1:length(lengths(area_honeycomb_grid)))\n\nFollowing this, we can use lengths() and st_intersect() to determine the allocation of bus stop in each cell. The goal is to create a new column, consisting of the number of bus stop in each of the cell. The filter() function can then be added to remove all cells with no bus stop and create the final sf data frame.\n\n# Counting the number of bus stop in each cell\nhoneycomb_grid_sf$n_busstop = lengths(st_intersects(honeycomb_grid_sf,busstop))\n\n# Removing all cells without bus stop\nhoneycomb_count = filter(honeycomb_grid_sf, n_busstop &gt; 0)\n\nAt this point, the hexagon grid of bus stop can be drawn onto a map of Singapore using the functions of the tmap package. Additionally, the n_busstop column can be passed to the tm_fill() function to shade the cell based on the number of bus stops in it.\n\n\n\n\n\n\nNote\n\n\n\nSeveral functions are added to make the map interactive and aesthetically pleasing\n\ntmap_mode(‘view’): Creates an interactive map which allow zooming and interacting with cells on the map\npop.vars: Identifying the legend and value which pops up when a cell is selected. In this case, it is the number of bus stops.\npopup.format: Specifying the format of the variable to be displayed when selecting a cell.\ntm_basemap: Choosing the basemap layer on which the hexagon grid will be drawn. OpenStreetMap is chosen due to its high fidelity while not being overly crowded. Additionally, OpenStreetMap displays icon for bus stops in Singapore, allowing user to visually check any cell.\n\nIf an incorrect CRS was specified in the earlier steps, the basemap will be of an incorrect location or alignment.\n\n\n\n\n\ntmap_mode('view')\n\nbushexmap &lt;- tm_shape(honeycomb_count)+\n  tm_fill(\n    col = \"n_busstop\",\n    palette = \"Blues\",\n    style = \"cont\",\n    title = \"Number of bus stop\",\n    id = \"grid_id\",\n    showNA = FALSE,\n    alpha = 0.7,\n    popup.vars = c(\n      \"Number of bus stop: \" = \"n_busstop\"\n    ),\n    popup.format = list(\n      n_busstop = list(format = \"f\", digits = 0)\n    )\n  ) +\n  tm_borders(col = \"grey40\", lwd = 0.7)+\n  tm_basemap(server = c('Esri.WorldGrayCanvas'))\n\nbushexmap\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nSearch for the bus stop near your place and compare the numbers between the four maps by zooming in! Do you think it’s accurate?\n\n\nBy looking at the illustration, we can see that each cell might contain up to ten bus stops. A bar chat can be drawn with the ggplot2 package to visualize the distribution of number of bus stop in each cell.\n\nggplot(honeycomb_count, aes(x=n_busstop))+\n  geom_bar()+\n  theme_classic()+\n  geom_text(aes(label = ..count..), stat = \"count\", vjust = -0.5, colour = \"black\")\n\n\n\n\nAs can be seen, the majority of cells contain 1-2 bus stop with only 214 cells containing more than 6 bus stops. This shows that the cells adequately capture solitary bus stop, as well as pairs of bus stops (bus stops which are opposite each other, served by the same bus services).\n\n\n\nIn order to conduct geospatial analysis, a data frame which contains the hexagon cells as well as the number of bus trips for each cells must be created. This can be done using the left_join argument.\n\n\n\n\n\n\nNote\n\n\n\nThere are important arguments which can be used to create a cleaner combined data frame.\n\nby = join_by(BUS_STOP_N == ORIGIN_PT_CODE)): Indicate the column by which the two data frames can be matched and joined. In this case, the bus stop code will be used.\nselect(1,4,5): Indicate the index number of the columns to be kept in the final data frame. Only the bus stop number (column 1), total number of trips (column 4), and geometry (column 5) will be kept.\nreplace(is.na(.),0): Replace all value of NA with 0. This is to ensure that bus stop with no trips in a given time frame is accurately tallied at 0.\n\n\n\n\n# Weekday morning peak 6am - 9am\npassenger_wd_69_combined &lt;- left_join(busstop, passenger_wd_69_tallied, by = join_by(BUS_STOP_N == ORIGIN_PT_CODE))%&gt;%\n  select(1,4,5)%&gt;%\n  replace(is.na(.),0)\n\n# Weekday afternoon peak 5pm - 8pm (17:00 - 20:00)\npassenger_wd_1720_combined &lt;- left_join(busstop, passenger_wd_1720_tallied, by = join_by(BUS_STOP_N == ORIGIN_PT_CODE))%&gt;%\n  select(1,4,5)%&gt;%\n  replace(is.na(.),0)\n\n# Weekend/holiday morning peak 11am - 2pm (11:00 - 14:00)\npassenger_weh_1114_combined &lt;- left_join(busstop, passenger_weh_1114_tallied, by = join_by(BUS_STOP_N == ORIGIN_PT_CODE))%&gt;%\n  select(1,4,5)%&gt;%\n  replace(is.na(.),0)\n\n# Weekend/holiday evening peak 4pm - 7pm (16:00 - 19:00)\npassenger_weh_1619_combined &lt;- left_join(busstop, passenger_weh_1619_tallied, by = join_by(BUS_STOP_N == ORIGIN_PT_CODE))%&gt;%\n  select(1,4,5)%&gt;%\n  replace(is.na(.),0)\n\nIt is important to note that the bus stops and their total trips have not been tallied into the hexagon cells. st_join() can be used to accomplish this for each time frame.\n\n\n\n\n\n\nNote\n\n\n\nThe by argument in st_join() can be passed the function st_within to specify that we would like to join the two data frames where the geometry in the latter is within the geometry of the former. In this case, it would mean that two rows will be joined where the bus stop lies within a particular polygon.\n\nThe group_by() and summarise() functions here are used similarly to before, they sums up the total number of trips for all the bus stops in the hexagon, based on its grid_id, and create a new column called TOTAL_TRIP.\n\n\n\n\n# Weekday morning peak 6am - 9am\nhex_passenger_wd_69 &lt;- st_join(honeycomb_count, passenger_wd_69_combined, by = st_within(sparse = FALSE), largest = TRUE) %&gt;%\n  group_by(grid_id)%&gt;%\n  summarise(TOTAL_TRIP = sum(TRIPS))\n\n# Weekday afternoon peak 5pm - 8pm (17:00 - 20:00)\nhex_passenger_wd_1720 &lt;- st_join(honeycomb_count, passenger_wd_1720_combined, by = st_within(sparse = FALSE), largest = TRUE) %&gt;%\n  group_by(grid_id)%&gt;%\n  summarise(TOTAL_TRIP = sum(TRIPS))\n\n# Weekend/holiday morning peak 11am - 2pm (11:00 - 14:00)\nhex_passenger_weh_1114 &lt;- st_join(honeycomb_count, passenger_weh_1114_combined, by = st_within(sparse = FALSE), largest = TRUE) %&gt;%\n  group_by(grid_id)%&gt;%\n  summarise(TOTAL_TRIP = sum(TRIPS))\n\n# Weekend/holiday evening peak 4pm - 7pm (16:00 - 19:00)\nhex_passenger_weh_1619 &lt;- st_join(honeycomb_count, passenger_weh_1619_combined, by = st_within(sparse = FALSE), largest = TRUE) %&gt;%\n  group_by(grid_id)%&gt;%\n  summarise(TOTAL_TRIP = sum(TRIPS))\n\nIn sum, the analysis will revolve around the four following data frames which contain the spatial information of the hexagon as well as the total number of trips for each interested time frame:\n\nhex_passenger_wd_69: Weekday morning peak 6am - 9am\nhex_passenger_wd_1720: Weekday afternoon peak 5pm - 8pm (17:00 - 20:00)\nhex_passenger_weh_1114: Weekend/holiday morning peak 11am - 2pm (11:00 - 14:00)\nhex_passenger_weh_1619: Weekend/holiday evening peak 4pm - 7pm (16:00 - 19:00)"
  },
  {
    "objectID": "Take-Home_Ex/Take-Home_Ex1/Take-Home_Ex1.html#cleanup-step",
    "href": "Take-Home_Ex/Take-Home_Ex1/Take-Home_Ex1.html#cleanup-step",
    "title": "Take-Home_Ex1",
    "section": "",
    "text": "Before we move on to Geovisualization and the analysis of LISA, it would be wise to remove objects which will no longer be used. This will help to free up memory for other tasks. rm() can be used to perform this task\n\nrm(list = c('area_honeycomb_grid', 'bushexmap', 'honeycomb_count', 'honeycomb_grid_sf',       'passenger_wd_1720', 'passenger_wd_1720_tallied', 'passenger_wd_1720_combined',    'passenger_wd_69', 'passenger_wd_69_tallied', 'passenger_wd_69_combined',      'passenger_weh_1114', 'passenger_weh_1114_tallied', 'passenger_weh_1114_combined',      'passenger_weh_1619', 'passenger_weh_1619_tallied', 'passenger_weh_1619_combined'))"
  },
  {
    "objectID": "Take-Home_Ex/Take-Home_Ex1/Take-Home_Ex1.html#geovisualization-and-analysis",
    "href": "Take-Home_Ex/Take-Home_Ex1/Take-Home_Ex1.html#geovisualization-and-analysis",
    "title": "Take-Home_Ex1",
    "section": "",
    "text": "The beginning step of the analysis would be to visualize the distribution of bus trips on the hexagon layer. This can be accomplished with the mapping functions of the tmap package. However, unlike the geovisualization of bus stop per hexagon, the number of trips for each hexagon depending on the time frame varies widely. Let’s confirm this by drawing a histogram of the distribution of trips in each time frame.\n\nweekday_morning_hist &lt;- ggplot(hex_passenger_wd_69, aes(x=TOTAL_TRIP))+\n  geom_histogram()+\n  scale_x_continuous(labels = scales::comma)+\n  ggtitle('Weekday Morning')+\n  theme_classic()\n\nweekday_evening_hist &lt;- ggplot(hex_passenger_wd_1720, aes(x=TOTAL_TRIP))+\n  geom_histogram()+\n  scale_x_continuous(labels = scales::comma)+\n  ggtitle('Weekday Evening')+\n  theme_classic()\n  \nweekend_noon_hist &lt;- ggplot(hex_passenger_weh_1114, aes(x=TOTAL_TRIP))+\n  geom_histogram()+\n  scale_x_continuous(labels = scales::comma)+\n  ggtitle('Weekend Noon')+\n  theme_classic()\n  \nweekend_evening_hist &lt;- ggplot(hex_passenger_weh_1619, aes(x=TOTAL_TRIP))+\n  geom_histogram()+\n  scale_x_continuous(labels = scales::comma)+\n  ggtitle('Weekend Evening')+\n  theme_classic()\n  \n  \ngridExtra::grid.arrange(weekday_morning_hist, weekday_evening_hist, weekend_noon_hist, weekend_evening_hist,\n                        nrow = 2, ncol = 2)\n\n\n\n\nUpon a brief inspection, it is possible to see that the range of trips between the different time periods are very different from each other, with Weekday Evening trips going above 300,000 for some hexagons, while Weekend Noon only ranging around 80,000 for its hexagons. By plotting this on map, we will get to see the geospatial distribution of the number of trips for each time frame.\nBefore we map all four time frames, however, it is important to consider that the ‘style’ argument of tm_fill() can take on many values (pretty, quantile, equal, etc.). In order to pick an appropriate ‘style’, we can pick one time frame and plot it using three different styles to determine the best way to depict the distribution.\n\ntmap_mode('view')\n\n### Weekday morning peak 6am - 9am\n# Quantile style\nweekday_morning_quantile &lt;- tm_shape(hex_passenger_wd_69) +\n  tm_fill(\n    col = \"TOTAL_TRIP\",\n    palette = \"Blues\",\n    style = \"quantile\",\n    title = \"Number of Trips\",\n    id = \"grid_id\",\n    showNA = FALSE,\n    alpha = 0.7,\n    popup.vars = c(\n      \"Number of Trips: \" = \"TOTAL_TRIP\"\n    ),\n    popup.format = list(\n      TOTAL_TRIP = list(format = \"f\", digits = 0)\n    )\n  ) +\n  tm_borders(col = \"grey40\", lwd = 0.7)+\n  tm_basemap(server = c('Esri.WorldGrayCanvas'))+\n  tm_layout(title = 'Weekday Morning Peak (Quantile)', title.position = c('right', 'top'))\n\n# Jenks style\nweekday_morning_jenks &lt;- tm_shape(hex_passenger_wd_69) +\n  tm_fill(\n    col = \"TOTAL_TRIP\",\n    palette = \"Blues\",\n    style = \"jenks\",\n    title = \"Number of Trips\",\n    id = \"grid_id\",\n    showNA = FALSE,\n    alpha = 0.7,\n    popup.vars = c(\n      \"Number of Trips: \" = \"TOTAL_TRIP\"\n    ),\n    popup.format = list(\n      TOTAL_TRIP = list(format = \"f\", digits = 0)\n    )\n  ) +\n  tm_borders(col = \"grey40\", lwd = 0.7)+\n  tm_basemap(server = c('Esri.WorldGrayCanvas'))+\n  tm_layout(title = 'Weekday Morning Peak (Jenks)', title.position = c('right', 'top'))\n\n# Equal style\nweekday_morning_equal &lt;- tm_shape(hex_passenger_wd_69) +\n  tm_fill(\n    col = \"TOTAL_TRIP\",\n    palette = \"Blues\",\n    style = \"equal\",\n    title = \"Number of Trips\",\n    id = \"grid_id\",\n    showNA = FALSE,\n    alpha = 0.7,\n    popup.vars = c(\n      \"Number of Trips: \" = \"TOTAL_TRIP\"\n    ),\n    popup.format = list(\n      TOTAL_TRIP = list(format = \"f\", digits = 0)\n    )\n  ) +\n  tm_borders(col = \"grey40\", lwd = 0.7)+\n  tm_basemap(server = c('Esri.WorldGrayCanvas'))+\n  tm_layout(title = 'Weekday Morning Peak (Equal)', title.position = c('right', 'top'))\n\ntmap_arrange(weekday_morning_quantile, weekday_morning_jenks, weekday_morning_equal, asp = 1, ncol = 3)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAs can be seen, the ‘quantile’ and ‘jenks’ style can better display the difference in distribution of trips between the different hexagons. This can be attributed to the fact that the range of trips between the hexagons are too large. However, the ‘quantile’ function suffers from its final grouping, containing values from roughly 5,539 to 136,490; this resulted int the oversaturation of the high value hexagons. On the other hand, the ‘jenks’ method “divides the features into classes whose boundaries are where there are relatively big differences in the data values” (Reference). Therefore, it is possible to move forward using the ‘jenks’ style for visualization, but by adjusting the breaks to provide more stratification in the hexagon colors for better visualization..\n\nIt is good to remove the 3 plots created to plot the different styles since they will not be used and will only take up memory.\n\nrm(list=c('weekday_morning_equal','weekday_morning_quantile','weekday_morning_jenks'))\n\n\nThe functions of the tmap package will be used to create the maps.\n\ntmap_mode('view')\n\n# Weekday morning peak 6am - 9am\nweekday_morning &lt;- tm_shape(hex_passenger_wd_69) +\n  tm_fill(\n    col = \"TOTAL_TRIP\",\n    palette = \"Blues\",\n    style = \"jenks\",\n    title = \"Number of Trips\",\n    id = \"grid_id\",\n    showNA = FALSE,\n    alpha = 0.7,\n    popup.vars = c(\n      \"Number of Trips: \" = \"TOTAL_TRIP\"\n    ),\n    popup.format = list(\n      TOTAL_TRIP = list(format = \"f\", digits = 0)\n    )\n  ) +\n  tm_borders(col = \"grey40\", lwd = 0.7)+\n  tm_basemap(server = c('Esri.WorldGrayCanvas'))+\n  tm_layout(title = 'Weekday Morning Peak', title.position = c('right', 'top'))\n\n\n# Weekday afternoon peak 5pm - 8pm (17:00 - 20:00)\nweekday_afternoon &lt;- tm_shape(hex_passenger_wd_1720) +\n  tm_fill(\n    col = \"TOTAL_TRIP\",\n    palette = \"Blues\",\n    style = \"jenks\",\n    title = \"Number of Trips\",\n    id = \"grid_id\",\n    showNA = FALSE,\n    alpha = 0.7,\n    popup.vars = c(\n      \"Number of Trips: \" = \"TOTAL_TRIP\"\n    ),\n    popup.format = list(\n      TOTAL_TRIP = list(format = \"f\", digits = 0)\n    )\n  ) +\n  tm_borders(col = \"grey40\", lwd = 0.7)+\n  tm_basemap(server = c('Esri.WorldGrayCanvas'))+\n  tm_layout(title = 'Weekday Afternoon Peak', title.position = c('right', 'top'))\n\n# Weekend/holiday morning peak 11am - 2pm (11:00 - 14:00)\nweekend_noon &lt;- tm_shape(hex_passenger_weh_1114) +\n  tm_fill(\n    col = \"TOTAL_TRIP\",\n    palette = \"Blues\",\n    style = \"jenks\",\n    title = \"Number of Trips\",\n    id = \"grid_id\",\n    showNA = FALSE,\n    alpha = 0.7,\n    popup.vars = c(\n      \"Number of Trips: \" = \"TOTAL_TRIP\"\n    ),\n    popup.format = list(\n      TOTAL_TRIP = list(format = \"f\", digits = 0)\n    )\n  ) +\n  tm_borders(col = \"grey40\", lwd = 0.7)+\n  tm_basemap(server = c('Esri.WorldGrayCanvas'))+\n  tm_layout(title = 'Weekend/holiday Morning Peak', title.position = c('right', 'top'))\n\n# Weekend/holiday evening peak 4pm - 7pm (16:00 - 19:00)\nweekend_evening &lt;- tm_shape(hex_passenger_weh_1619) +\n  tm_fill(\n    col = \"TOTAL_TRIP\",\n    palette = \"Blues\",\n    style = \"jenks\",\n    title = \"Number of Trips\",\n    id = \"grid_id\",\n    showNA = FALSE,\n    alpha = 0.7,\n    popup.vars = c(\n      \"Number of Trips: \" = \"TOTAL_TRIP\"\n    ),\n    popup.format = list(\n      TOTAL_TRIP = list(format = \"f\", digits = 0)\n    )\n  ) +\n  tm_borders(col = \"grey40\", lwd = 0.7)+\n  tm_basemap(server = c('Esri.WorldGrayCanvas'))+\n  tm_layout(title = 'Weekend/holiday Evening Peak', title.position = c('right', 'top'))\n\ntmap_arrange(weekday_morning, weekday_afternoon, weekend_noon, weekend_evening, nrow = 2, ncol = 2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nSearch for the bus stop near your place and compare the numbers between the four maps by zooming in! Do you think it’s accurate?\n\n\nFrom the rough visualization, it’s clear that not all bus stops experience a similar level of traffic throughout different timing of the days. Additionally, based on the quantiles created by tmap, it seems that the ranges of passenger traffic are radically different between the four time windows. For example, certain grids during Weekday Morning Peak, a hexagon could reach 328,545 passenger trips, whereas the highest number during Weekend/holiday Morning Peak only reaches 112,330. It appears that Weekend Evening Peak 17:00 - 20:00 is the period with the highest level of activity.\nAn important observation seems to be that there darker shaded hexagons tend to be clustered, indicating a positive spatial autocorrelation. However, there are clearly hexagons which are much darker than its surrounding tiles (such as the one near Tampines), indicating negative spatial autocorrelation. By conducting LISA analysis, it will be possible to determine the level of spatial autocorrelation for each hexagon, visualize them, as well as to depict the relationship between a hexagon and its neighbors through a LISA cluster map."
  },
  {
    "objectID": "Take-Home_Ex/Take-Home_Ex1/Take-Home_Ex1.html#local-indicators-of-spatial-autocorrelation-lisa",
    "href": "Take-Home_Ex/Take-Home_Ex1/Take-Home_Ex1.html#local-indicators-of-spatial-autocorrelation-lisa",
    "title": "Take-Home_Ex1",
    "section": "",
    "text": "Before the LISA analysis can be conducted, it is important to define our neighborhood, or the neighbors of each polygon. This is based on the ideas that neighbors, or spatial objects which are related to other spatial objects based on sharing a common boundary or lying with a certain distance of one another, might affect each other.\nIn this case, we would like to identify the neighbors of each hexagon so that LISA analysis can determine if the number of trips in a hexagon in a time frame is correlated to the number of trips of the hexagons around it, either in the same direction or opposite direction.\nThe first step to determining the neighborhood is to choose a method by which neighbors are classified:\n\nContiguity-based Method: Based on the sharing of boundaries, either edges and/or points (Queen and Rook method).\nDistance-based Method: Based on the distances between the centroid (central point of each hexagon) of each polygon. This can either be set to a distance where each hexagon has at least one neighbor (Fixed Distance) or where each polygon has a certain number of neighbors (Adaptive Distance).\n\nIt is important to choose the appropriate method according to each situation. However, in this study, Contiguity-based methods can be preemptively ruled out due to the fact that some cells have no contiguous neighbors, as can be seen below.\n\n\n\nIsolated Cells\n\n\nIf the contiguity method is used, the LISA calculation for these cells would not be conducive for analysis as they technically have no neighbors. This can be confirmed by using the st_contiguity() function to create a Queen contiguity matrix for one time frame\n\nwm_q &lt;- st_contiguity(st_geometry(hex_passenger_wd_1720))\n\nsummary(wm_q)\n\nNeighbour list object:\nNumber of regions: 1520 \nNumber of nonzero links: 6874 \nPercentage nonzero weights: 0.2975242 \nAverage number of links: 4.522368 \n9 regions with no links:\n561 726 980 1047 1415 1505 1508 1512 1520\nLink number distribution:\n\n  0   1   2   3   4   5   6 \n  9  39 109 205 291 364 503 \n39 least connected regions:\n1 7 22 38 98 169 187 195 211 218 258 259 264 267 287 454 562 607 642 696 708 732 751 784 869 1021 1022 1046 1086 1214 1464 1471 1482 1500 1501 1503 1506 1510 1519 with 1 link\n503 most connected regions:\n10 13 16 17 24 25 31 35 42 43 48 53 55 60 63 67 73 77 80 81 84 85 87 88 91 92 97 102 107 111 117 121 127 133 140 141 143 148 149 150 154 155 156 157 163 164 165 173 174 175 183 184 185 191 192 193 194 200 201 202 205 206 207 208 216 229 239 243 244 246 257 266 271 278 279 283 284 291 292 298 300 301 302 304 309 310 312 313 316 321 324 325 327 337 338 339 340 343 352 355 363 368 390 391 400 402 403 407 414 418 423 425 431 436 437 438 440 443 450 451 452 461 466 467 468 469 473 477 480 481 485 489 493 494 496 502 503 507 513 514 517 518 523 529 534 539 543 548 549 550 552 556 558 564 568 573 574 576 577 581 590 591 594 598 599 604 605 609 615 619 624 626 633 636 637 638 648 649 650 654 655 657 658 659 669 670 671 677 680 681 682 687 688 690 691 700 701 704 705 706 713 716 717 724 727 728 729 740 741 755 757 758 760 771 774 775 776 777 782 783 787 788 789 792 793 794 795 799 800 806 807 810 811 812 813 819 820 823 824 825 830 831 832 841 843 844 846 847 848 850 851 852 853 854 860 863 865 866 867 871 872 876 877 878 880 881 882 884 885 887 888 891 893 896 899 902 905 906 910 914 919 921 926 927 928 930 931 935 937 943 944 945 946 947 948 954 958 959 962 963 968 969 971 972 973 977 984 985 986 987 988 990 996 997 998 999 1004 1011 1012 1013 1014 1024 1025 1026 1028 1029 1036 1037 1038 1042 1050 1051 1054 1056 1057 1062 1063 1064 1066 1067 1068 1069 1076 1078 1079 1080 1083 1089 1093 1100 1101 1102 1105 1106 1110 1111 1117 1120 1121 1122 1128 1133 1134 1135 1136 1141 1142 1144 1145 1146 1147 1148 1150 1156 1157 1158 1162 1163 1164 1166 1169 1170 1171 1172 1176 1177 1178 1179 1180 1184 1186 1190 1191 1192 1193 1194 1201 1202 1203 1204 1205 1206 1207 1210 1211 1217 1218 1219 1220 1221 1227 1233 1234 1235 1239 1244 1245 1251 1253 1254 1255 1261 1265 1266 1271 1272 1273 1277 1281 1283 1289 1299 1301 1302 1303 1304 1316 1318 1324 1325 1326 1327 1329 1330 1331 1334 1335 1336 1337 1343 1344 1345 1352 1353 1355 1356 1361 1365 1366 1368 1369 1371 1372 1377 1380 1381 1382 1384 1388 1391 1393 1395 1398 1406 1408 1412 1417 1418 1420 1424 1425 1426 1427 1428 1433 1434 1435 1436 1440 1441 1442 1446 1447 1449 1451 1453 1456 1457 1459 1460 1461 1462 1469 with 6 links\n\n\nAs can be seen from the weight matrix, 10 hexagon cells have 0 neighbor. Therefore, a Distance-based Method would be suitable for analysis. Both Fixed Distance and Adaptive Distance Weight Matrix can be created for comparison to find the most appropriate method.\n\n\n\n\n\nst_dist_band() of sfdep is incredibly powerful in that it can create a neighbor list based on distance between the centroid of polygons and a lower and upper bound distance to other centroid. The default arguments for st_dist_band() will find neighbors with a lower and upper bound so that each hexagon will have at least one neighbor. This is the equivalent of the steps in spdep of the function knearneigh() of k=1.\nst_inverse_distance() of a sfdep can be combined with st_dist_band() in a dplyr step to create a new column in each data frame of the different time frames to create a neighbor list and a inverse distance weight list. Additionally, st_lag() can be used to create a spatially lagged value column for total trips based on the weight of neighbors.\n\n# Weekday morning peak 6am - 9am\nwd69_nb &lt;- hex_passenger_wd_69 %&gt;%\n  mutate(nb = st_dist_band(area_honeycomb_grid),\n         wt = st_inverse_distance(nb, area_honeycomb_grid),\n         lag_trip = st_lag(TOTAL_TRIP,nb,wt),\n         .before = 1) # to put them in the front\n\n# Weekday afternoon peak 5pm - 8pm (17:00 - 20:00)\nwd1720_nb &lt;- hex_passenger_wd_1720 %&gt;%\n  mutate(nb = st_dist_band(area_honeycomb_grid),\n         wt = st_inverse_distance(nb, area_honeycomb_grid),\n         lag_trip = st_lag(TOTAL_TRIP,nb,wt),\n         .before = 1) # to put them in the front\n\n# Weekend/holiday morning peak 11am - 2pm (11:00 - 14:00)\nweh1114_nb &lt;- hex_passenger_weh_1114 %&gt;%\n  mutate(nb = st_dist_band(area_honeycomb_grid),\n         wt = st_inverse_distance(nb, area_honeycomb_grid),\n         lag_trip = st_lag(TOTAL_TRIP,nb,wt),\n         .before = 1) # to put them in the front\n\n# Weekend/holiday evening peak 4pm - 7pm (16:00 - 19:00)\nweh1619_nb &lt;- hex_passenger_weh_1619 %&gt;%\n  mutate(nb = st_dist_band(area_honeycomb_grid),\n         wt = st_inverse_distance(nb, area_honeycomb_grid),\n         lag_trip = st_lag(TOTAL_TRIP,nb,wt),\n         .before = 1) # to put them in the front\n\nSince the hexagon tiles is stable across all time frame, it is possible to plot the neighbor relationship for only one time frame in order to visualize the neighbors list created. Let’s take Weekday morning peak 6am - 9am. By visualizing, the appropriateness of the Fixed Distance Method can be roughly determined.\n\nplot(wd69_nb$area_honeycomb_grid, border = 'lightgrey')\nplot(st_knn(wd69_nb$area_honeycomb_grid, k = 1), st_centroid(wd69_nb$area_honeycomb_grid), add=TRUE, col = 'red', length = 0.08)\n\n\n\n\nBetween the Fixed Distance and Adaptive Distance Matrix, an Adaptive Distance Matrix would be more appropriate to the non-uniform nature of bus stop spatial distribution across the map. Using Adaptive Distance would allow for the specification of the number of neighbors, allowing for the standardisation of the LISA analysis process across hexagons.\n\n\n\nlocal_moran() of the sfdep package can be used to calculate Local Moran’s I Statistic and other related statistics.\n\n# Weekday morning peak 6am - 9am\nwd69_lisa &lt;- wd69_nb %&gt;%\n  mutate(local_moran = local_moran(TOTAL_TRIP, nb, wt, nsim = 199),\n         .before = 1) %&gt;%\n  unnest(local_moran)\n\n# Weekday afternoon peak 5pm - 8pm (17:00 - 20:00)\nwd1720_lisa &lt;- wd1720_nb %&gt;%\n  mutate(local_moran = local_moran(TOTAL_TRIP, nb, wt, nsim = 199),\n         .before = 1) %&gt;%\n  unnest(local_moran)\n\n# Weekend/holiday morning peak 11am - 2pm (11:00 - 14:00)\nweh1114_lisa &lt;- weh1114_nb %&gt;%\n  mutate(local_moran = local_moran(TOTAL_TRIP, nb, wt, nsim = 199),\n         .before = 1) %&gt;%\n  unnest(local_moran)\n\n# Weekend/holiday evening peak 4pm - 7pm (16:00 - 19:00)\nweh1619_lisa &lt;- weh1619_nb %&gt;%\n  mutate(local_moran = local_moran(TOTAL_TRIP, nb, wt, nsim = 199),\n         .before = 1) %&gt;%\n  unnest(local_moran)\n\n\ntmap_mode('view')\ntm_shape(wd1720_lisa)+\n  tm_fill(col = 'mean',\n          palette = \"RdBu\",\n    style = \"cat\",\n    title = \"ii\"\n  ) +\n  tm_borders(col = \"grey40\", lwd = 0.7)+\n  tm_basemap(server = c('Esri.WorldGrayCanvas'))"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "ISSS624",
    "section": "",
    "text": "Welcome to ISSS624 Geospatial Analytics Applications!\nIn this webpage, I am going to share with you my learning journey of geospatial analytics."
  },
  {
    "objectID": "In-Class_Ex/In-Class_Ex3/data/geospatial/MPSZ-2019.html",
    "href": "In-Class_Ex/In-Class_Ex3/data/geospatial/MPSZ-2019.html",
    "title": "ISSS624",
    "section": "",
    "text": "&lt;!DOCTYPE qgis PUBLIC ‘http://mrcc.com/qgis.dtd’ ‘SYSTEM’&gt;     dataset\n\n\n        0 0     false"
  },
  {
    "objectID": "In-Class_Ex/In-Class_Ex2/In-class_Ex2_EHSA.html",
    "href": "In-Class_Ex/In-Class_Ex2/In-class_Ex2_EHSA.html",
    "title": "In-Class_Ex2_EHSA",
    "section": "",
    "text": "Loading R packages. The plotly library is added to create interactive maps.\n\npacman::p_load(sf, sfdep, tmap, plotly, tidyverse, knitr)"
  },
  {
    "objectID": "In-Class_Ex/In-Class_Ex2/In-class_Ex2_EHSA.html#getting-started",
    "href": "In-Class_Ex/In-Class_Ex2/In-class_Ex2_EHSA.html#getting-started",
    "title": "In-Class_Ex2_EHSA",
    "section": "",
    "text": "Loading R packages. The plotly library is added to create interactive maps.\n\npacman::p_load(sf, sfdep, tmap, plotly, tidyverse, knitr)"
  },
  {
    "objectID": "In-Class_Ex/In-Class_Ex2/In-class_Ex2_EHSA.html#the-data",
    "href": "In-Class_Ex/In-Class_Ex2/In-class_Ex2_EHSA.html#the-data",
    "title": "In-Class_Ex2_EHSA",
    "section": "The Data",
    "text": "The Data\n\nImporting geospatial data\nst_read() can be used to read the shape file data set into an R sf data frame\n\nhunan &lt;- st_read(dsn = 'data/geospatial',\n                 layer = 'Hunan')\n\nReading layer `Hunan' from data source \n  `D:\\phlong2023\\ISSS624\\In-Class_Ex\\In-Class_Ex2\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 88 features and 7 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 108.7831 ymin: 24.6342 xmax: 114.2544 ymax: 30.12812\nGeodetic CRS:  WGS 84\n\n\n\n\nImporting time series file\nread_csv() can be used to read the time series file into an R data frame\n\nGDPPC &lt;- read_csv('data/aspatial/Hunan_GDPPC.csv')"
  },
  {
    "objectID": "In-Class_Ex/In-Class_Ex2/In-class_Ex2_EHSA.html#creating-a-time-series",
    "href": "In-Class_Ex/In-Class_Ex2/In-class_Ex2_EHSA.html#creating-a-time-series",
    "title": "In-Class_Ex2_EHSA",
    "section": "Creating a Time Series",
    "text": "Creating a Time Series\nspacetime() can be used to create a Space Time Cube\n\nGDPPC_st &lt;- spacetime(GDPPC, hunan,\n                      .loc_col = 'County',\n                      .time_col = 'Year')\n\nis_spacetime_cube() can be used to check whether the created object is actually a spacetime cube\n\nis_spacetime_cube(GDPPC_st)\n\n[1] TRUE\n\n\n\nCreating Inverse Distance Weight Matrix Columns for GI*\n\nGDPPC_nb &lt;- GDPPC_st %&gt;%\n  activate('geometry') %&gt;%\n  mutate(nb = include_self(st_contiguity(geometry)),\n         wt = st_inverse_distance(nb, geometry,\n                                  scale = 1,\n                                  alpha = 1),\n         .before = 1)%&gt;%\n  set_nbs('nb') %&gt;%\n  set_wts('wt')\n\n\n\nComputing GI*\nComputing GI* using the newly created data frame with the neighbor list and weight matrix for each county for each year\n\ngi_stars &lt;- GDPPC_nb %&gt;%\n  group_by(Year) %&gt;%\n  mutate(gi_star = local_gstar_perm(GDPPC, nb, wt)) %&gt;%\n  unnest(gi_star)"
  },
  {
    "objectID": "In-Class_Ex/In-Class_Ex2/In-class_Ex2_EHSA.html#man-kendall-test",
    "href": "In-Class_Ex/In-Class_Ex2/In-class_Ex2_EHSA.html#man-kendall-test",
    "title": "In-Class_Ex2_EHSA",
    "section": "Man-Kendall Test",
    "text": "Man-Kendall Test\n\nPerforming Emerging Hotspot Analysis\nemerging_hotspot_analysis() can be used to perform the Emerging Hotspot Analysis using the space time cube object\n\nehsa &lt;- emerging_hotspot_analysis(\n  x = GDPPC_st,\n  .var = 'GDPPC',\n  k = 1, #Comparing the Time Series sequentially (e.g. 2012 vs 2013)\n  nsim = 99\n)\n\nPlotting the distribution of hotspot type\n\nggplot(ehsa, aes(x=classification))+\n  geom_bar()+\n  theme_classic()\n\n\n\n\n\n\nVisualizing EHSA\n\nehsa_rename &lt;- ehsa %&gt;%\n  rename(County = location)\n\nhunan_ehsa &lt;- left_join(hunan, ehsa_rename,\n                        by = 'County')\n\nehsa_sig &lt;- hunan_ehsa %&gt;%\n  filter(p_value &lt; 0.05)\ntmap_mode('plot')\ntm_shape(hunan_ehsa) +\n  tm_polygons()+\n  tm_borders(alpha = 0.5)+\n  tm_shape(ehsa_sig)+\n  tm_fill('classification')+\n  tm_borders(alpha = 0.4)"
  },
  {
    "objectID": "In-Class_Ex/In-Class_Ex1/In-Class_Ex1.html",
    "href": "In-Class_Ex/In-Class_Ex1/In-Class_Ex1.html",
    "title": "In-Class Exercise 1",
    "section": "",
    "text": "To prepare a choropleth map showing the distribution of passenger trips at planning sub-zone by integrating Passenger Volume by Origin Destination Bus Stops and bus stop data sets downloaded from LTA DataMall and Planning Sub-zone boundary of URA Master Plan 2019 from data.gov.sg."
  },
  {
    "objectID": "In-Class_Ex/In-Class_Ex1/In-Class_Ex1.html#the-task",
    "href": "In-Class_Ex/In-Class_Ex1/In-Class_Ex1.html#the-task",
    "title": "In-Class Exercise 1",
    "section": "",
    "text": "To prepare a choropleth map showing the distribution of passenger trips at planning sub-zone by integrating Passenger Volume by Origin Destination Bus Stops and bus stop data sets downloaded from LTA DataMall and Planning Sub-zone boundary of URA Master Plan 2019 from data.gov.sg."
  },
  {
    "objectID": "In-Class_Ex/In-Class_Ex1/In-Class_Ex1.html#getting-started",
    "href": "In-Class_Ex/In-Class_Ex1/In-Class_Ex1.html#getting-started",
    "title": "In-Class Exercise 1",
    "section": "Getting Started",
    "text": "Getting Started\nLoading the necessary packages in R:\n\ntmap: for thematic mapping\nsf: for geospatial data handling\ntidyverse: for non-spatial data handling\n\n\npacman::p_load(tmap, tidyverse, sf, knitr)"
  },
  {
    "objectID": "In-Class_Ex/In-Class_Ex1/In-Class_Ex1.html#importing-the-od-data",
    "href": "In-Class_Ex/In-Class_Ex1/In-Class_Ex1.html#importing-the-od-data",
    "title": "In-Class Exercise 1",
    "section": "Importing the OD Data",
    "text": "Importing the OD Data\nFirstly we will import the Passenger Volume by Origin Destination Bus Stops data downloaed from LTA DataMall by using read_csv() of readr package\n\n# eval:false\nodbus &lt;- read_csv('data/aspatial/origin_destination_bus_202308.csv')\n\nWe can check the odbus tibble dataframe to explore the data types\n\nglimpse(odbus)\n\nRows: 5,709,512\nColumns: 7\n$ YEAR_MONTH          &lt;chr&gt; \"2023-08\", \"2023-08\", \"2023-08\", \"2023-08\", \"2023-…\n$ DAY_TYPE            &lt;chr&gt; \"WEEKDAY\", \"WEEKENDS/HOLIDAY\", \"WEEKENDS/HOLIDAY\",…\n$ TIME_PER_HOUR       &lt;dbl&gt; 16, 16, 14, 14, 17, 17, 17, 17, 7, 17, 14, 10, 10,…\n$ PT_TYPE             &lt;chr&gt; \"BUS\", \"BUS\", \"BUS\", \"BUS\", \"BUS\", \"BUS\", \"BUS\", \"…\n$ ORIGIN_PT_CODE      &lt;chr&gt; \"04168\", \"04168\", \"80119\", \"80119\", \"44069\", \"4406…\n$ DESTINATION_PT_CODE &lt;chr&gt; \"10051\", \"10051\", \"90079\", \"90079\", \"17229\", \"1722…\n$ TOTAL_TRIPS         &lt;dbl&gt; 7, 2, 3, 10, 5, 4, 3, 22, 3, 3, 7, 1, 3, 1, 3, 1, …\n\n\nWe can convert ORIGIN_PT_CODE and DESTINATION_PT_CODE into Factor data, a data type unique to R, in order to speed up sorting\n\nodbus$ORIGIN_PT_CODE &lt;- as.factor(odbus$ORIGIN_PT_CODE)\nodbus$DESTINATION_PT_CODE &lt;- as.factor(odbus$DESTINATION_PT_CODE)\n\nWe can confirm that the data types of ORIGIN_PT_CODE and DESTINATION_PT_CODE using glimpse()\n\nglimpse(odbus)\n\nRows: 5,709,512\nColumns: 7\n$ YEAR_MONTH          &lt;chr&gt; \"2023-08\", \"2023-08\", \"2023-08\", \"2023-08\", \"2023-…\n$ DAY_TYPE            &lt;chr&gt; \"WEEKDAY\", \"WEEKENDS/HOLIDAY\", \"WEEKENDS/HOLIDAY\",…\n$ TIME_PER_HOUR       &lt;dbl&gt; 16, 16, 14, 14, 17, 17, 17, 17, 7, 17, 14, 10, 10,…\n$ PT_TYPE             &lt;chr&gt; \"BUS\", \"BUS\", \"BUS\", \"BUS\", \"BUS\", \"BUS\", \"BUS\", \"…\n$ ORIGIN_PT_CODE      &lt;fct&gt; 04168, 04168, 80119, 80119, 44069, 44069, 20281, 2…\n$ DESTINATION_PT_CODE &lt;fct&gt; 10051, 10051, 90079, 90079, 17229, 17229, 20141, 2…\n$ TOTAL_TRIPS         &lt;dbl&gt; 7, 2, 3, 10, 5, 4, 3, 22, 3, 3, 7, 1, 3, 1, 3, 1, …"
  },
  {
    "objectID": "In-Class_Ex/In-Class_Ex1/In-Class_Ex1.html#extracting-the-study-data",
    "href": "In-Class_Ex/In-Class_Ex1/In-Class_Ex1.html#extracting-the-study-data",
    "title": "In-Class Exercise 1",
    "section": "Extracting the study data",
    "text": "Extracting the study data\nIf we want to pick out the commuter data between 7 and 9 o clock on weekdays\n\norigin_7_9 &lt;- odbus %&gt;%   filter(DAY_TYPE == 'WEEKDAY') %&gt;%   filter(TIME_PER_HOUR &gt;=7 & TIME_PER_HOUR &lt;=9) %&gt;%   group_by(ORIGIN_PT_CODE)%&gt;%   summarise(TRIPS = sum(TOTAL_TRIPS))\n\nWe can check the data table using the code below\n\nkable(head(origin_7_9))\n\n\n\n\nORIGIN_PT_CODE\nTRIPS\n\n\n\n\n01012\n1617\n\n\n01013\n813\n\n\n01019\n1620\n\n\n01029\n2383\n\n\n01039\n2727\n\n\n01059\n1415\n\n\n\n\n\nWe will save the output in rds format for future use\n\nwrite_rds(origin_7_9, 'data/rds/origin_7_9.rds')\n\nWe can read the origin7_9.rds into R using the code below\n\norigin_7_9 &lt;- read_rds('data/rds/origin_7_9.rds')"
  },
  {
    "objectID": "In-Class_Ex/In-Class_Ex1/In-Class_Ex1.html#working-with-geospatial-data",
    "href": "In-Class_Ex/In-Class_Ex1/In-Class_Ex1.html#working-with-geospatial-data",
    "title": "In-Class Exercise 1",
    "section": "Working with Geospatial Data",
    "text": "Working with Geospatial Data\nTwo geospatial data will be used in this study:\n\nBusStop: Location of bus stops in the last quarter of 2022\nMPSZ-2019: Master Plan Boundary (No Sea) of Singapore in 2019\n\n\nImporting geospatial data\nWe can import the BusStop shape file into an R simple feature dataframe using st_read()\n\nbusstop &lt;- st_read(dsn = 'data/geospatial',\n                   layer = 'BusStop')\n\nReading layer `BusStop' from data source \n  `D:\\phlong2023\\ISSS624\\In-Class_Ex\\In-Class_Ex1\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 5161 features and 3 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 3970.122 ymin: 26482.1 xmax: 48284.56 ymax: 52983.82\nProjected CRS: SVY21\n\n\nWe can check the structure and data types of the new busstop dataframe using glimpse()\n\nglimpse(busstop)\n\nRows: 5,161\nColumns: 4\n$ BUS_STOP_N &lt;chr&gt; \"22069\", \"32071\", \"44331\", \"96081\", \"11561\", \"66191\", \"2338…\n$ BUS_ROOF_N &lt;chr&gt; \"B06\", \"B23\", \"B01\", \"B05\", \"B05\", \"B03\", \"B02A\", \"B02\", \"B…\n$ LOC_DESC   &lt;chr&gt; \"OPP CEVA LOGISTICS\", \"AFT TRACK 13\", \"BLK 239\", \"GRACE IND…\n$ geometry   &lt;POINT [m]&gt; POINT (13576.31 32883.65), POINT (13228.59 44206.38),…\n\n\nAs the busstop data frame has a CRS of SVY21, we want to transform it into a CRS of SVY21 / Singapore TM (EPSG 3414) using st_transform()\n\nbusstop &lt;- st_transform(busstop, crs = 3414)\n\nWe can next import the Master Plan Sub-zone Boundary 2019 shape file into a simple feature dataframe using st_read()\n\nmpsz &lt;- st_read(dsn = 'data/geospatial',\n                layer = 'MPSZ-2019')\n\nReading layer `MPSZ-2019' from data source \n  `D:\\phlong2023\\ISSS624\\In-Class_Ex\\In-Class_Ex1\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 332 features and 6 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 103.6057 ymin: 1.158699 xmax: 104.0885 ymax: 1.470775\nGeodetic CRS:  WGS 84\n\n\nWe can see that the CRS for the mpsz dataframe is WGS 84 (or EPSG 4326), we want it to be SVY21 (or EPSG 3414). We can do this by using st_transform()\n\nmpsz &lt;- st_transform(mpsz, 3414)\n\nWe can double check the CRS of mpsz using st_geometry(). We can see that the Projected CRS is now SVY21\n\nst_geometry(mpsz)\n\nGeometry set for 332 features \nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21 / Singapore TM\nFirst 5 geometries:"
  },
  {
    "objectID": "In-Class_Ex/In-Class_Ex1/In-Class_Ex1.html#geospatial-data-wrangling",
    "href": "In-Class_Ex/In-Class_Ex1/In-Class_Ex1.html#geospatial-data-wrangling",
    "title": "In-Class Exercise 1",
    "section": "Geospatial Data Wrangling",
    "text": "Geospatial Data Wrangling\n\nCombining busstop and mpsz\nThis code below populates the planning subzone code (SUBZONE_C) of mpsz data frame into the busstop data frame. st_intersection() is used to perform point and polygon overlap and the output will be in point simple feature object.\n\nbusstop_mpsz &lt;- st_intersection(busstop, mpsz) %&gt;%\n  select(BUS_STOP_N, SUBZONE_C) %&gt;%\n  st_drop_geometry()\n\nWe will save the new data frame into rds format\n\nwrite_rds(busstop_mpsz, 'data/rds/busstop_mpsz.csv')\n\nNext, we are going to append the planning subzone code from busstop_mpsz data frame onto origin_7_9 data frame\n\norigin_data &lt;- left_join(origin_7_9, busstop_mpsz,\n                         by = c('ORIGIN_PT_CODE' = 'BUS_STOP_N')) %&gt;%\n  rename(ORIGIN_BS = ORIGIN_PT_CODE,\n         ORIGIN_SZ = SUBZONE_C)\n\nIt is good practice to check for duplicate records\n\nduplicate &lt;- origin_data %&gt;%\n  group_by_all() %&gt;%\n  filter(n()&gt;1) %&gt;%\n  ungroup()\n\nIf duplicated records are found, the code chunk below will be used to retain only the unique records\n\norigin_data &lt;- unique(origin_data)\n\nWe can re-reun the code chunk to check for duplicate records in the new data frame. We will now see that the duplicate dataframe contains 0 observation.\n\nduplicate &lt;- origin_data %&gt;%\n  group_by_all() %&gt;%\n  filter(n()&gt;1) %&gt;%\n  ungroup()\n\nNow, we can append the bus stop code and number of trips starting from that code onto the original mpsz data frame (which contains the geometry information for mapping)\n\nmpsz_origtrip &lt;- left_join(mpsz, origin_data,\n                         by = c('SUBZONE_C' = 'ORIGIN_SZ'))"
  },
  {
    "objectID": "In-Class_Ex/In-Class_Ex1/In-Class_Ex1.html#choropleth-visualization",
    "href": "In-Class_Ex/In-Class_Ex1/In-Class_Ex1.html#choropleth-visualization",
    "title": "In-Class Exercise 1",
    "section": "Choropleth Visualization",
    "text": "Choropleth Visualization\nTo create a choropleth visualization, we can using the tmap package\n\ntm_shape(mpsz_origtrip)+\n  tm_fill('TRIPS',\n          n = 6,\n          style = 'quantile',\n          palette = 'Blues')+\n  tm_layout(main.title = 'Passenger Trips Generated at Planning Sub-zone Level',\n            main.title.position = 'center',\n            main.title.size = 1.2,\n            legend.height = 0.45,\n            legend.width = 0.35,\n            frame = TRUE)+\n  tm_borders(alpha = 0.5)+\n  tm_compass(type = '8star', size = 2)+\n  tm_scale_bar()+\n  tm_grid(alpha = 0.2)+\n  tm_credits('Source: Planning Sub-zone Boundary from Urban Redevelopment Authority (URA) \\n and Population Data from Department of Statistics (DOS)',\n             position = c('left','bottom'))\n\n\n\n\nWe can use a map using custom breaks for comparison. Before that, we can use the summary function to determine appropriate breakpoints\n\nsummary(mpsz_origtrip$TRIPS)\n\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max.     NA's \n     1.0    386.5   1773.0   4191.5   5002.0 295128.0       21 \n\n\nThen, we can draw the map using the tmap package\n\ntm_shape(mpsz_origtrip)+\n  tm_fill('TRIPS',\n          breaks = c(0, 1000, 1500, 2000, 2500, 3000, 3500, 4000, 4500, 5000),\n          palette = 'Blues')+\n  tm_layout(main.title = 'Passenger Trips Generated at Planning Sub-zone Level',\n            main.title.position = 'center',\n            main.title.size = 1.2,\n            legend.height = 0.45,\n            legend.width = 0.35,\n            frame = TRUE)+\n  tm_borders(alpha = 0.5)+\n  tm_compass(type = '8star', size = 2)+\n  tm_scale_bar()+\n  tm_grid(alpha = 0.2)+\n  tm_credits('Source: Planning Sub-zone Boundary from Urban Redevelopment Authority (URA) \\n and Population Data from Department of Statistics (DOS)',\n             position = c('left','bottom'))\n\n\n\n\nIt can be seen that due to the large variations in number of trips between different planning sub-zones, the custom breaks are not as insightful as the ‘quantile’ style built into tmap."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex3/Hands-on_Ex3.html",
    "href": "Hands-on_Ex/Hands-on_Ex3/Hands-on_Ex3.html",
    "title": "Hands-on_Ex3",
    "section": "",
    "text": "Spatial interaction represents the flow of people, material, or information between locations in geographical space. It encompasses everything from freight shipments, energy flows, and the global trade in rare antiquities, to flight schedules, rush hour woes, and pedestrian foot traffic.\nEach spatial interaction, as an analogy for a set of movements, is composed of a discrete origin/destination pair. Each pair can be represented as a cell in a matrix where rows are related to the locations (centroids) of origin, while columns are related to locations (centroids) of destination. Such a matrix is commonly known as an origin/destination matrix, or spatial interaction matrix.\nWe will by an OD Matrix by using Passenger Volume by Origin Destination Bus Stops data set downloaded in the previous exercises.\n\n\n\nThe following package will be loaded in with the p_load() function of pacman: sf, tidyverse, tmap, DT, stplanr.\n\npacman::p_load(sf, tidyverse, tmap, DT, stplanr)\n\n\n\n\n\n\nWe will import the Passenger Volume by Origin Destination Bus Stops using read_csv().\n\nodbus &lt;- read_csv('data/aspatial/origin_destination_bus_202310.csv')\n\nglimpse() can be used to display the data type and some rows of the odbus data frame.\n\nglimpse(odbus)\n\nRows: 5,694,297\nColumns: 7\n$ YEAR_MONTH          &lt;chr&gt; \"2023-10\", \"2023-10\", \"2023-10\", \"2023-10\", \"2023-…\n$ DAY_TYPE            &lt;chr&gt; \"WEEKENDS/HOLIDAY\", \"WEEKDAY\", \"WEEKENDS/HOLIDAY\",…\n$ TIME_PER_HOUR       &lt;dbl&gt; 16, 16, 14, 14, 17, 17, 17, 7, 14, 14, 10, 20, 20,…\n$ PT_TYPE             &lt;chr&gt; \"BUS\", \"BUS\", \"BUS\", \"BUS\", \"BUS\", \"BUS\", \"BUS\", \"…\n$ ORIGIN_PT_CODE      &lt;chr&gt; \"04168\", \"04168\", \"80119\", \"80119\", \"44069\", \"2028…\n$ DESTINATION_PT_CODE &lt;chr&gt; \"10051\", \"10051\", \"90079\", \"90079\", \"17229\", \"2014…\n$ TOTAL_TRIPS         &lt;dbl&gt; 3, 5, 3, 5, 4, 1, 24, 2, 1, 7, 3, 2, 5, 1, 1, 1, 1…\n\n\nORIGIN_PT_CODE and DESTINATION_PT_CODE are in character data type whereas they should be converted into factor data type for easier manipulation.\n\nodbus$ORIGIN_PT_CODE &lt;- as.factor(odbus$ORIGIN_PT_CODE)\n\nodbus$DESTINATION_PT_CODE &lt;- as.factor(odbus$DESTINATION_PT_CODE)\n\n\n\n\nWe will only look at commuting flows on weekday between 6 and 9 o’clock. This can be extracted using the filter() function.\n\nodbus6_9 &lt;- odbus %&gt;%\n  filter(DAY_TYPE == 'WEEKDAY') %&gt;%\n  filter(TIME_PER_HOUR &gt;= 6 &\n           TIME_PER_HOUR &lt;= 9) %&gt;%\n  group_by(ORIGIN_PT_CODE,\n           DESTINATION_PT_CODE) %&gt;%\n  summarise(TRIPS = sum(TOTAL_TRIPS))\n\ndatatable() can be used to view odbus6_9\n\ndatatable(odbus6_9)\n\n\n\n\n\n\nWe will save the output in rds format for future use.\n\nwrite_rds(odbus6_9, 'data/rds/odbus6_9.rds')\n\nread_rds() can be used to import the rds file into R.\n\nodbus6_9 &lt;- read_rds('data/rds/odbus6_9.rds')\n\n\n\n\n\nTwo geospatial data sets will be used:\n\nBusStop: This data provides the location of bus stops as at July 2023\nMPSZ-2019: this data provides the sub-zone boundary of URA Master Plan 2019.\n\nBoth data sets are in ESRI shapefile format.\n\n\n\nbusstop &lt;- st_read(dsn = 'data/geospatial',\n                   layer = 'BusStop') %&gt;%\n  st_transform(crs = 3414)\n\nReading layer `BusStop' from data source \n  `D:\\phlong2023\\ISSS624\\Hands-on_Ex\\Hands-on_Ex3\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 5161 features and 3 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 3970.122 ymin: 26482.1 xmax: 48284.56 ymax: 52983.82\nProjected CRS: SVY21\n\n\n\nmpsz &lt;- st_read(dsn = 'data/geospatial',\n                layer = 'MPSZ-2019') %&gt;%\n  st_transform(crs = 3414)\n\nReading layer `MPSZ-2019' from data source \n  `D:\\phlong2023\\ISSS624\\Hands-on_Ex\\Hands-on_Ex3\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 332 features and 6 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 103.6057 ymin: 1.158699 xmax: 104.0885 ymax: 1.470775\nGeodetic CRS:  WGS 84\n\n\n\nmpsz\n\nSimple feature collection with 332 features and 6 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21 / Singapore TM\nFirst 10 features:\n                 SUBZONE_N SUBZONE_C       PLN_AREA_N PLN_AREA_C       REGION_N\n1              MARINA EAST    MESZ01      MARINA EAST         ME CENTRAL REGION\n2         INSTITUTION HILL    RVSZ05     RIVER VALLEY         RV CENTRAL REGION\n3           ROBERTSON QUAY    SRSZ01  SINGAPORE RIVER         SR CENTRAL REGION\n4  JURONG ISLAND AND BUKOM    WISZ01  WESTERN ISLANDS         WI    WEST REGION\n5             FORT CANNING    MUSZ02           MUSEUM         MU CENTRAL REGION\n6         MARINA EAST (MP)    MPSZ05    MARINE PARADE         MP CENTRAL REGION\n7                   SUDONG    WISZ03  WESTERN ISLANDS         WI    WEST REGION\n8                  SEMAKAU    WISZ02  WESTERN ISLANDS         WI    WEST REGION\n9           SOUTHERN GROUP    SISZ02 SOUTHERN ISLANDS         SI CENTRAL REGION\n10                 SENTOSA    SISZ01 SOUTHERN ISLANDS         SI CENTRAL REGION\n   REGION_C                       geometry\n1        CR MULTIPOLYGON (((33222.98 29...\n2        CR MULTIPOLYGON (((28481.45 30...\n3        CR MULTIPOLYGON (((28087.34 30...\n4        WR MULTIPOLYGON (((14557.7 304...\n5        CR MULTIPOLYGON (((29542.53 31...\n6        CR MULTIPOLYGON (((35279.55 30...\n7        WR MULTIPOLYGON (((15772.59 21...\n8        WR MULTIPOLYGON (((19843.41 21...\n9        CR MULTIPOLYGON (((30870.53 22...\n10       CR MULTIPOLYGON (((26879.04 26...\n\n\nWe can write the mpsz sf tibble data frame into an rds file for future use.\n\nwrite_rds(mpsz, \"data/rds/mpsz.rds\")\n\n\n\n\n\n\n\nWe can populate the planning subzone code (SUBZONE_C) of mpsz sf data frame into busstop sf data frame\n\nbusstop_mpsz &lt;- st_intersection(busstop, mpsz) %&gt;%\n  select(BUS_STOP_N, SUBZONE_C) %&gt;%\n  st_drop_geometry()\n\n\ndatatable(busstop_mpsz)\n\n\n\n\n\n\nBefore moving to the next step, it is wise to save the output into rds format.\n\nwrite_rds(busstop_mpsz, 'data/rds/busstop_mpsz.rds')\n\nWe are going to append the planning subzone code from busstop_mpsz data frame onto odbus6_9 data frame.\n\nod_data &lt;- left_join(odbus6_9, busstop_mpsz, by = c('ORIGIN_PT_CODE' = 'BUS_STOP_N')) %&gt;%\n  rename(ORGIN_BS = ORIGIN_PT_CODE,\n         ORIGIN_SZ = SUBZONE_C,\n         DESTIN_BS = DESTINATION_PT_CODE)\n\nBefore continuing, it is a good practice for us to check for duplicating records\n\nduplicate &lt;- od_data %&gt;%\n  group_by_all() %&gt;%\n  filter(n()&gt;1) %&gt;%\n  ungroup\n\nIf duplicated records are found, we can remove them using unique().\n\nod_data &lt;- unique(od_data)\n\nWe can reconfirm whether the duplicated records have been removed.\n\nduplicate &lt;- od_data %&gt;%\n  group_by_all() %&gt;%\n  filter(n()&gt;1) %&gt;%\n  ungroup()\n\nWe can confirm that there is no more duplicate records.\nNow, we can update od_data with the planning subzone codes for destination subzone.\n\nod_data &lt;- left_join(od_data, busstop_mpsz, by = c('DESTIN_BS'= 'BUS_STOP_N'))\n\nNext, we can check for duplicate record and proceed to remove them\n\nduplicate &lt;- od_data %&gt;%\n  group_by_all() %&gt;%\n  filter(n()&gt;1) %&gt;%\n  ungroup()\n\nod_data &lt;- unique(od_data)\n\nNext, we can rename the new subzone column and summarise the total number of trips by each origin and destination subzones.\n\nod_data &lt;- od_data %&gt;%\n  rename(DESTIN_SZ = SUBZONE_C) %&gt;%\n  drop_na() %&gt;%\n  group_by(ORIGIN_SZ, DESTIN_SZ) %&gt;%\n  summarise(MORNING_PEAK = sum(TRIPS))\n\nNow, we can save the output into an rds file format.\n\nwrite_rds(od_data, 'data/rds/od_data.rds')\n\n\nod_data &lt;- read_rds('data/rds/od_data.rds')\n\n\n\n\n\nWe can prepare a desire line by using the stplanr package.\n\n\nWe will not plot the intra-zonal flows.\n\nod_data1 &lt;- od_data[od_data$ORIGIN_SZ != od_data$DESTIN_SZ,]\n\n\n\n\nod2line() is used to create the desire lines.\n\nflowLine &lt;- od2line(flow = od_data1, \n                    zones = mpsz,\n                    zone_code = 'SUBZONE_C')\n\n\n\n\nWe can use the tmap package to visualize the desire line.\n\ntmap_mode('plot')\ntm_shape(mpsz)+\n  tm_polygons() +\n  flowLine %&gt;%\n  tm_shape()+\n  tm_lines(lwd = 'MORNING_PEAK',\n           style = 'quantile',\n           scale = c(0.1, 1, 3, 5, 7, 10),\n           n = 6,\n           alpha = 0.3)\n\n\n\n\nWhen the flow data are very messy, and highly skewed like the one shown above, it is wiser to focus on selected flows. For example, flow greater than or equal to 5000 as shown below.\n\ntm_shape(mpsz)+\n  tm_polygons()+\n  flowLine %&gt;%\n  filter(MORNING_PEAK &gt;= 5000) %&gt;%\n  tm_shape()+\n  tm_lines(lwd = 'MORNING_PEAK',\n           style = 'quantile',\n           scale = c(0.1, 1, 3, 5, 7, 10),\n           n = 6,\n           alpha = 0.3)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex3/Hands-on_Ex3.html#overview",
    "href": "Hands-on_Ex/Hands-on_Ex3/Hands-on_Ex3.html#overview",
    "title": "Hands-on_Ex3",
    "section": "",
    "text": "Spatial interaction represents the flow of people, material, or information between locations in geographical space. It encompasses everything from freight shipments, energy flows, and the global trade in rare antiquities, to flight schedules, rush hour woes, and pedestrian foot traffic.\nEach spatial interaction, as an analogy for a set of movements, is composed of a discrete origin/destination pair. Each pair can be represented as a cell in a matrix where rows are related to the locations (centroids) of origin, while columns are related to locations (centroids) of destination. Such a matrix is commonly known as an origin/destination matrix, or spatial interaction matrix.\nWe will by an OD Matrix by using Passenger Volume by Origin Destination Bus Stops data set downloaded in the previous exercises."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex3/Hands-on_Ex3.html#getting-started",
    "href": "Hands-on_Ex/Hands-on_Ex3/Hands-on_Ex3.html#getting-started",
    "title": "Hands-on_Ex3",
    "section": "",
    "text": "The following package will be loaded in with the p_load() function of pacman: sf, tidyverse, tmap, DT, stplanr.\n\npacman::p_load(sf, tidyverse, tmap, DT, stplanr)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex3/Hands-on_Ex3.html#preparing-the-flow-data",
    "href": "Hands-on_Ex/Hands-on_Ex3/Hands-on_Ex3.html#preparing-the-flow-data",
    "title": "Hands-on_Ex3",
    "section": "",
    "text": "We will import the Passenger Volume by Origin Destination Bus Stops using read_csv().\n\nodbus &lt;- read_csv('data/aspatial/origin_destination_bus_202310.csv')\n\nglimpse() can be used to display the data type and some rows of the odbus data frame.\n\nglimpse(odbus)\n\nRows: 5,694,297\nColumns: 7\n$ YEAR_MONTH          &lt;chr&gt; \"2023-10\", \"2023-10\", \"2023-10\", \"2023-10\", \"2023-…\n$ DAY_TYPE            &lt;chr&gt; \"WEEKENDS/HOLIDAY\", \"WEEKDAY\", \"WEEKENDS/HOLIDAY\",…\n$ TIME_PER_HOUR       &lt;dbl&gt; 16, 16, 14, 14, 17, 17, 17, 7, 14, 14, 10, 20, 20,…\n$ PT_TYPE             &lt;chr&gt; \"BUS\", \"BUS\", \"BUS\", \"BUS\", \"BUS\", \"BUS\", \"BUS\", \"…\n$ ORIGIN_PT_CODE      &lt;chr&gt; \"04168\", \"04168\", \"80119\", \"80119\", \"44069\", \"2028…\n$ DESTINATION_PT_CODE &lt;chr&gt; \"10051\", \"10051\", \"90079\", \"90079\", \"17229\", \"2014…\n$ TOTAL_TRIPS         &lt;dbl&gt; 3, 5, 3, 5, 4, 1, 24, 2, 1, 7, 3, 2, 5, 1, 1, 1, 1…\n\n\nORIGIN_PT_CODE and DESTINATION_PT_CODE are in character data type whereas they should be converted into factor data type for easier manipulation.\n\nodbus$ORIGIN_PT_CODE &lt;- as.factor(odbus$ORIGIN_PT_CODE)\n\nodbus$DESTINATION_PT_CODE &lt;- as.factor(odbus$DESTINATION_PT_CODE)\n\n\n\n\nWe will only look at commuting flows on weekday between 6 and 9 o’clock. This can be extracted using the filter() function.\n\nodbus6_9 &lt;- odbus %&gt;%\n  filter(DAY_TYPE == 'WEEKDAY') %&gt;%\n  filter(TIME_PER_HOUR &gt;= 6 &\n           TIME_PER_HOUR &lt;= 9) %&gt;%\n  group_by(ORIGIN_PT_CODE,\n           DESTINATION_PT_CODE) %&gt;%\n  summarise(TRIPS = sum(TOTAL_TRIPS))\n\ndatatable() can be used to view odbus6_9\n\ndatatable(odbus6_9)\n\n\n\n\n\n\nWe will save the output in rds format for future use.\n\nwrite_rds(odbus6_9, 'data/rds/odbus6_9.rds')\n\nread_rds() can be used to import the rds file into R.\n\nodbus6_9 &lt;- read_rds('data/rds/odbus6_9.rds')"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex3/Hands-on_Ex3.html#working-with-geospatial-data",
    "href": "Hands-on_Ex/Hands-on_Ex3/Hands-on_Ex3.html#working-with-geospatial-data",
    "title": "Hands-on_Ex3",
    "section": "",
    "text": "Two geospatial data sets will be used:\n\nBusStop: This data provides the location of bus stops as at July 2023\nMPSZ-2019: this data provides the sub-zone boundary of URA Master Plan 2019.\n\nBoth data sets are in ESRI shapefile format.\n\n\n\nbusstop &lt;- st_read(dsn = 'data/geospatial',\n                   layer = 'BusStop') %&gt;%\n  st_transform(crs = 3414)\n\nReading layer `BusStop' from data source \n  `D:\\phlong2023\\ISSS624\\Hands-on_Ex\\Hands-on_Ex3\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 5161 features and 3 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 3970.122 ymin: 26482.1 xmax: 48284.56 ymax: 52983.82\nProjected CRS: SVY21\n\n\n\nmpsz &lt;- st_read(dsn = 'data/geospatial',\n                layer = 'MPSZ-2019') %&gt;%\n  st_transform(crs = 3414)\n\nReading layer `MPSZ-2019' from data source \n  `D:\\phlong2023\\ISSS624\\Hands-on_Ex\\Hands-on_Ex3\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 332 features and 6 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 103.6057 ymin: 1.158699 xmax: 104.0885 ymax: 1.470775\nGeodetic CRS:  WGS 84\n\n\n\nmpsz\n\nSimple feature collection with 332 features and 6 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21 / Singapore TM\nFirst 10 features:\n                 SUBZONE_N SUBZONE_C       PLN_AREA_N PLN_AREA_C       REGION_N\n1              MARINA EAST    MESZ01      MARINA EAST         ME CENTRAL REGION\n2         INSTITUTION HILL    RVSZ05     RIVER VALLEY         RV CENTRAL REGION\n3           ROBERTSON QUAY    SRSZ01  SINGAPORE RIVER         SR CENTRAL REGION\n4  JURONG ISLAND AND BUKOM    WISZ01  WESTERN ISLANDS         WI    WEST REGION\n5             FORT CANNING    MUSZ02           MUSEUM         MU CENTRAL REGION\n6         MARINA EAST (MP)    MPSZ05    MARINE PARADE         MP CENTRAL REGION\n7                   SUDONG    WISZ03  WESTERN ISLANDS         WI    WEST REGION\n8                  SEMAKAU    WISZ02  WESTERN ISLANDS         WI    WEST REGION\n9           SOUTHERN GROUP    SISZ02 SOUTHERN ISLANDS         SI CENTRAL REGION\n10                 SENTOSA    SISZ01 SOUTHERN ISLANDS         SI CENTRAL REGION\n   REGION_C                       geometry\n1        CR MULTIPOLYGON (((33222.98 29...\n2        CR MULTIPOLYGON (((28481.45 30...\n3        CR MULTIPOLYGON (((28087.34 30...\n4        WR MULTIPOLYGON (((14557.7 304...\n5        CR MULTIPOLYGON (((29542.53 31...\n6        CR MULTIPOLYGON (((35279.55 30...\n7        WR MULTIPOLYGON (((15772.59 21...\n8        WR MULTIPOLYGON (((19843.41 21...\n9        CR MULTIPOLYGON (((30870.53 22...\n10       CR MULTIPOLYGON (((26879.04 26...\n\n\nWe can write the mpsz sf tibble data frame into an rds file for future use.\n\nwrite_rds(mpsz, \"data/rds/mpsz.rds\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex3/Hands-on_Ex3.html#geospatial-data-wrangling",
    "href": "Hands-on_Ex/Hands-on_Ex3/Hands-on_Ex3.html#geospatial-data-wrangling",
    "title": "Hands-on_Ex3",
    "section": "",
    "text": "We can populate the planning subzone code (SUBZONE_C) of mpsz sf data frame into busstop sf data frame\n\nbusstop_mpsz &lt;- st_intersection(busstop, mpsz) %&gt;%\n  select(BUS_STOP_N, SUBZONE_C) %&gt;%\n  st_drop_geometry()\n\n\ndatatable(busstop_mpsz)\n\n\n\n\n\n\nBefore moving to the next step, it is wise to save the output into rds format.\n\nwrite_rds(busstop_mpsz, 'data/rds/busstop_mpsz.rds')\n\nWe are going to append the planning subzone code from busstop_mpsz data frame onto odbus6_9 data frame.\n\nod_data &lt;- left_join(odbus6_9, busstop_mpsz, by = c('ORIGIN_PT_CODE' = 'BUS_STOP_N')) %&gt;%\n  rename(ORGIN_BS = ORIGIN_PT_CODE,\n         ORIGIN_SZ = SUBZONE_C,\n         DESTIN_BS = DESTINATION_PT_CODE)\n\nBefore continuing, it is a good practice for us to check for duplicating records\n\nduplicate &lt;- od_data %&gt;%\n  group_by_all() %&gt;%\n  filter(n()&gt;1) %&gt;%\n  ungroup\n\nIf duplicated records are found, we can remove them using unique().\n\nod_data &lt;- unique(od_data)\n\nWe can reconfirm whether the duplicated records have been removed.\n\nduplicate &lt;- od_data %&gt;%\n  group_by_all() %&gt;%\n  filter(n()&gt;1) %&gt;%\n  ungroup()\n\nWe can confirm that there is no more duplicate records.\nNow, we can update od_data with the planning subzone codes for destination subzone.\n\nod_data &lt;- left_join(od_data, busstop_mpsz, by = c('DESTIN_BS'= 'BUS_STOP_N'))\n\nNext, we can check for duplicate record and proceed to remove them\n\nduplicate &lt;- od_data %&gt;%\n  group_by_all() %&gt;%\n  filter(n()&gt;1) %&gt;%\n  ungroup()\n\nod_data &lt;- unique(od_data)\n\nNext, we can rename the new subzone column and summarise the total number of trips by each origin and destination subzones.\n\nod_data &lt;- od_data %&gt;%\n  rename(DESTIN_SZ = SUBZONE_C) %&gt;%\n  drop_na() %&gt;%\n  group_by(ORIGIN_SZ, DESTIN_SZ) %&gt;%\n  summarise(MORNING_PEAK = sum(TRIPS))\n\nNow, we can save the output into an rds file format.\n\nwrite_rds(od_data, 'data/rds/od_data.rds')\n\n\nod_data &lt;- read_rds('data/rds/od_data.rds')"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex3/Hands-on_Ex3.html#visualizing-spatial-interaction",
    "href": "Hands-on_Ex/Hands-on_Ex3/Hands-on_Ex3.html#visualizing-spatial-interaction",
    "title": "Hands-on_Ex3",
    "section": "",
    "text": "We can prepare a desire line by using the stplanr package.\n\n\nWe will not plot the intra-zonal flows.\n\nod_data1 &lt;- od_data[od_data$ORIGIN_SZ != od_data$DESTIN_SZ,]\n\n\n\n\nod2line() is used to create the desire lines.\n\nflowLine &lt;- od2line(flow = od_data1, \n                    zones = mpsz,\n                    zone_code = 'SUBZONE_C')\n\n\n\n\nWe can use the tmap package to visualize the desire line.\n\ntmap_mode('plot')\ntm_shape(mpsz)+\n  tm_polygons() +\n  flowLine %&gt;%\n  tm_shape()+\n  tm_lines(lwd = 'MORNING_PEAK',\n           style = 'quantile',\n           scale = c(0.1, 1, 3, 5, 7, 10),\n           n = 6,\n           alpha = 0.3)\n\n\n\n\nWhen the flow data are very messy, and highly skewed like the one shown above, it is wiser to focus on selected flows. For example, flow greater than or equal to 5000 as shown below.\n\ntm_shape(mpsz)+\n  tm_polygons()+\n  flowLine %&gt;%\n  filter(MORNING_PEAK &gt;= 5000) %&gt;%\n  tm_shape()+\n  tm_lines(lwd = 'MORNING_PEAK',\n           style = 'quantile',\n           scale = c(0.1, 1, 3, 5, 7, 10),\n           n = 6,\n           alpha = 0.3)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex2/Hands-on_Ex2_Spatial_Weights.html",
    "href": "Hands-on_Ex/Hands-on_Ex2/Hands-on_Ex2_Spatial_Weights.html",
    "title": "Hands-on_Ex2_Spatial_Weights",
    "section": "",
    "text": "Two data sets will be used in this hands-on exercise:\n\nHunan County Boundary Layer: Geospatial data set in ESRI shapefile format\nHunan_2012.csv: Selected local development indicators in 2012\n\n\n\nWe will use the p_load() function of the pacman package to load the required packages: spdep (for spatial weights), sf, tmap, and tidyverse.\n\npacman::p_load(spdep, tmap, sf, knitr, tidyverse)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex2/Hands-on_Ex2_Spatial_Weights.html#the-study-area-and-data",
    "href": "Hands-on_Ex/Hands-on_Ex2/Hands-on_Ex2_Spatial_Weights.html#the-study-area-and-data",
    "title": "Hands-on_Ex2_Spatial_Weights",
    "section": "",
    "text": "Two data sets will be used in this hands-on exercise:\n\nHunan County Boundary Layer: Geospatial data set in ESRI shapefile format\nHunan_2012.csv: Selected local development indicators in 2012\n\n\n\nWe will use the p_load() function of the pacman package to load the required packages: spdep (for spatial weights), sf, tmap, and tidyverse.\n\npacman::p_load(spdep, tmap, sf, knitr, tidyverse)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex2/Hands-on_Ex2_Spatial_Weights.html#getting-the-data-into-the-r-environment",
    "href": "Hands-on_Ex/Hands-on_Ex2/Hands-on_Ex2_Spatial_Weights.html#getting-the-data-into-the-r-environment",
    "title": "Hands-on_Ex2_Spatial_Weights",
    "section": "Getting the Data into the R Environment",
    "text": "Getting the Data into the R Environment\n\nImport Shapefile into R Environment\nst_read() can be used to import the Hunan shapefile into a sf dataframe\n\nhunan &lt;- st_read(dsn = 'data/geospatial',\n                 layer = 'Hunan')\n\nReading layer `Hunan' from data source \n  `D:\\phlong2023\\ISSS624\\Hands-on_Ex\\Hands-on_Ex2\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 88 features and 7 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 108.7831 ymin: 24.6342 xmax: 114.2544 ymax: 30.12812\nGeodetic CRS:  WGS 84\n\n\n\n\nImport Attribute Data (aspatial) into R Environment\nread_csv() can be used to import the Hunan_2012.csv into R as a data frame\n\nhunan2012 &lt;- read_csv('data/aspatial/Hunan_2012.csv')\n\n\n\nPerforming Relational Join\nSince both data frames have 88 rows and share the ‘County’ column, we can use left_join() to update the hunan sf data frame with with attribute fields of hunan2012 data frame.\n\nhunan &lt;- left_join(hunan, hunan2012) %&gt;%\n  select(1:4, 7, 15)\n\nThe left_join() argument automatically seeks out the shared field for joining. In this case, it is ‘by = join_by(County)’."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex2/Hands-on_Ex2_Spatial_Weights.html#visualizing-regional-development-indicator",
    "href": "Hands-on_Ex/Hands-on_Ex2/Hands-on_Ex2_Spatial_Weights.html#visualizing-regional-development-indicator",
    "title": "Hands-on_Ex2_Spatial_Weights",
    "section": "Visualizing Regional Development Indicator",
    "text": "Visualizing Regional Development Indicator\nwe can prepare a basemap and a choropleth map showing the distribution of GDPPC 2012 by using qtm() of the tmap package.\n\nbasemap &lt;- tm_shape(hunan)+\n  tm_polygons() +\n  tm_text('NAME_3', size = 0.3) #A basemap is created showing the boundaries and names of counties in Hunan\n\ngdppc &lt;- qtm(hunan, 'GDPPC') # A choropleth map showing the distribution of GDPPC in Hunan\n\ntmap_arrange(basemap, gdppc, asp = 1, ncol = 2) # Arranging basemap and gdppc along two columns"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex2/Hands-on_Ex2_Spatial_Weights.html#computing-contiguity-spatial-weights",
    "href": "Hands-on_Ex/Hands-on_Ex2/Hands-on_Ex2_Spatial_Weights.html#computing-contiguity-spatial-weights",
    "title": "Hands-on_Ex2_Spatial_Weights",
    "section": "Computing Contiguity Spatial Weights",
    "text": "Computing Contiguity Spatial Weights\nWe can use poly2nb() of spep to compute contiguity weight matrices for the study area. This function builds a neighbours list based on regions with contiguous boundaries.\nIn poly2nb(), you can pass TRUE or FALSE to the queen argument in order to indicate whether to use the queen method. The default is TRUE.\n\nComputing (QUEEN) contiguity based neighbours\n\nwm_q &lt;- poly2nb(hunan, queen = TRUE)\nsummary(wm_q)\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 448 \nPercentage nonzero weights: 5.785124 \nAverage number of links: 5.090909 \nLink number distribution:\n\n 1  2  3  4  5  6  7  8  9 11 \n 2  2 12 16 24 14 11  4  2  1 \n2 least connected regions:\n30 65 with 1 link\n1 most connected region:\n85 with 11 links\n\n\nThe summary report above shows that there are 88 area units in Hunan. The most connected area unit has 11 neighbours (area 85). There are two area units with only 1 neighbour (30 and 65).\nFor each polygon in our polygon object, wm_q lists all neighboring polygons. To see the neighbors for the first polygon in the object, you can specify it similar to a list of list\n\nwm_q[[1]]\n\n[1]  2  3  4 57 85\n\n\nPolygon 1 has 5 neighbors. The numbers represent the polygon IDs as stored in hunan spdf class\nWe can retrieve the county name of Polygon ID = 1 by specifying it its position in the hunan data fram\n\nhunan$County[1]\n\n[1] \"Anxiang\"\n\n\nSimilarly, you can extract the name of its neighbors using their Polygon ID in wm_q\n\nhunan$County[c(2,3,4,57,85)]\n\n[1] \"Hanshou\" \"Jinshi\"  \"Li\"      \"Nan\"     \"Taoyuan\"\n\n\nWe can also retreive the GDPPC of these five counties\n\nnb1 &lt;- wm_q[[1]]\nnb1_gdppc &lt;- hunan$GDPPC[nb1]\nnb1_gdppc\n\n[1] 20981 34592 24473 21311 22879\n\n\nAdditionally, you can display the complete neighbor list using str() However, this output will cut across several pages.\n\n\nCreating (ROOK) Contiguity Based Neighbours\nWe can use Rook contiguity instead of Queen contiguity by passing FALSE to the queen argument of poly2nb\n\nwm_r &lt;- poly2nb(hunan, queen = FALSE)\nsummary(wm_r)\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 440 \nPercentage nonzero weights: 5.681818 \nAverage number of links: 5 \nLink number distribution:\n\n 1  2  3  4  5  6  7  8  9 10 \n 2  2 12 20 21 14 11  3  2  1 \n2 least connected regions:\n30 65 with 1 link\n1 most connected region:\n85 with 10 links\n\n\nSimilar to Queen, the number of area units in Hunan remains unchanged at 88. However, the most connected area unit has only 10 neighbours (area 85). The two area units with only one neighbour remain the same (area 30 and 65).\n\n\nVisualizing Contiguity Weights\nA connectivity graph takes a point and displays a line to each neighboring point.\nWe are working with polygons at the moment, so we will need to get points in order to make our connectivity graphs. The most typical method for this will be polygon centroids.\nWe can use the sf package to get Latitude and Longitude of polygon centroids before moving onto the graphs.\nWe will need points to associate with each polygon before we can make our connectivity graph. It will be a little more complicated than just running st_centroid on the sf object. We need the coordinates in a separate data frame. To do this we will use a mapping function.\nThe mapping function applies a given function to each element of a vector and returns a vector of the same length. Our input vector will be the geometry column of hunan. Our function will be st_centroid. We will be using map_dbl() variation of map from thhe purrr package.\nTo get our longitude values, we map the st_centroid function over the geometry column of hunan and access the longitude value through double bracket notation [[]] and 1 ([[1]]]). This allows us to get only the longitude, which is the first value in each centroid.\n\nlongitude &lt;- map_dbl(hunan$geometry, ~st_centroid(.x)[[1]])\n\nWe can do the same for latitude, only with [[2]] instead.\n\nlatitude &lt;- map_dbl(hunan$geometry, ~st_centroid(.x)[[2]])\n\nNow that we have longitude and latitude, we can use cbind to put them together into the same object (coords) as two separate columns\n\ncoords_hunan &lt;- cbind(longitude, latitude)\n\nWe can check the first few observation of this new object using head()\n\nhead(coords_hunan)\n\n     longitude latitude\n[1,]  112.1531 29.44362\n[2,]  112.0372 28.86489\n[3,]  111.8917 29.47107\n[4,]  111.7031 29.74499\n[5,]  111.6138 29.49258\n[6,]  111.0341 29.79863\n\n\n\nPlotting Queen Contiguity Based Neighbours Map\nThe contiguity map will draw a line between the centroid of each polygon and the centroids of its neighbors based on the Queen Contiguity.\n\nplot(hunan$geometry, border = 'lightgrey')\nplot(wm_q,coords_hunan, pch = 19, cex = 0.6, add = TRUE, col = 'red')\n\n\n\n\nIf we use plot(wm_q, coords_hunan) by itself, it will only provide a contiguity map with no border for each county.\n\n\nPlotting Rook Contiguity Based Neighbours Map\nWe can easily make a Rook contiguity map instead of a Queen contiguity map by using wm_r instead of wm_q\n\nplot(hunan$geometry, border = 'lightgrey')\nplot(wm_r, coords_hunan, pch = 19, cex = 0.6, add = TRUE, col = 'red')\n\n\n\n\n\n\nPlotting both Queen and Rook Contiguity Based Neighbours Maps\n\npar(mfrow = c(1,2))\nplot(hunan$geometry, main = 'Queen Contiguity', border = 'lightgrey')\nplot(wm_q, coords_hunan, pch = 19, cex = 0.6, add = TRUE, col = 'red')\nplot(hunan$geometry, main = 'Rook Contiguity', border = 'lightgrey')\nplot(wm_r, coords_hunan, pch = 19, cex = 0.6, add = TRUE, col = 'red')"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex2/Hands-on_Ex2_Spatial_Weights.html#computing-distance-based-neighbours",
    "href": "Hands-on_Ex/Hands-on_Ex2/Hands-on_Ex2_Spatial_Weights.html#computing-distance-based-neighbours",
    "title": "Hands-on_Ex2_Spatial_Weights",
    "section": "Computing Distance Based Neighbours",
    "text": "Computing Distance Based Neighbours\nDistance-based weight matrices can be derived using dnearneigh() of spdep.\nThe function identifies neighbours of region points by Euclidean distance with a distance band with lower d1= and upper d2= bounds controlled by the bounds= argument.\nIf unprojected coordinates are used and either specified in the coordinates object x or with x as a two column matrix and longlat = TRUE, great circle distances in km will be calculated assuming the WGS84 reference ellipsoid.\n\nDetermine the Cut-off Distance\nFirst, we need to determine the upper limit for distance band by using the steps below:\n\nReturn a matrix with the indices of points belonging to the set of the k nearest neighbours of each other by using knearneigh() of sdpep\nConvert the k nearest neighbour object returned by knearneigh() into a neighbours list of class nb with a list of integer vectors containing neighbour region number ids by using knn2nb()\nReturn the length of neighbour relationship edges by using nbdist() of spdep. The function returns in the units of the coordinates if the coordinates are projected, in km otherwise.\nRemove the list structure of the returned object by using unlist().\n\n\nk1 &lt;- knn2nb(knearneigh(coords_hunan))\nk1dists &lt;- unlist(nbdists(k1, coords_hunan, longlat = TRUE))\nsummary(k1dists)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  24.79   32.57   38.01   39.07   44.52   61.79 \n\n\nThe summary report shows that the largest first nearest neighbour distance is 61.79 km, so using this as the upper threshold gives certainty that all units will have at least one neighbour.\n\n\nComputing Fixed Distance Weight Watrix\ndnearneigh() can be used to compute the distance weight matrix. It will create an object containing the neighbors of a each id based on their distance in km with a lower bound d1=0 and upper bound d2=62\n\nwm_d62 &lt;- dnearneigh(coords_hunan, 0, 62, longlat = TRUE)\nwm_d62\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 324 \nPercentage nonzero weights: 4.183884 \nAverage number of links: 3.681818 \n\n\nstr() can be used to display the content of wm_d62\n\nstr(wm_d62)\n\nList of 88\n $ : int [1:5] 3 4 5 57 64\n $ : int [1:4] 57 58 78 85\n $ : int [1:4] 1 4 5 57\n $ : int [1:3] 1 3 5\n $ : int [1:4] 1 3 4 85\n $ : int 69\n $ : int [1:2] 67 84\n $ : int [1:4] 9 46 47 78\n $ : int [1:4] 8 46 68 84\n $ : int [1:4] 16 22 70 72\n $ : int [1:3] 14 17 72\n $ : int [1:5] 13 60 61 63 83\n $ : int [1:4] 12 15 60 83\n $ : int [1:2] 11 17\n $ : int 13\n $ : int [1:4] 10 17 22 83\n $ : int [1:3] 11 14 16\n $ : int [1:3] 20 22 63\n $ : int [1:5] 20 21 73 74 82\n $ : int [1:5] 18 19 21 22 82\n $ : int [1:6] 19 20 35 74 82 86\n $ : int [1:4] 10 16 18 20\n $ : int [1:3] 41 77 82\n $ : int [1:4] 25 28 31 54\n $ : int [1:4] 24 28 33 81\n $ : int [1:4] 27 33 42 81\n $ : int [1:2] 26 29\n $ : int [1:6] 24 25 33 49 52 54\n $ : int [1:2] 27 37\n $ : int 33\n $ : int [1:2] 24 36\n $ : int 50\n $ : int [1:5] 25 26 28 30 81\n $ : int [1:3] 36 45 80\n $ : int [1:6] 21 41 46 47 80 82\n $ : int [1:5] 31 34 45 56 80\n $ : int [1:2] 29 42\n $ : int [1:3] 44 77 79\n $ : int [1:4] 40 42 43 81\n $ : int [1:3] 39 45 79\n $ : int [1:5] 23 35 45 79 82\n $ : int [1:5] 26 37 39 43 81\n $ : int [1:3] 39 42 44\n $ : int [1:2] 38 43\n $ : int [1:6] 34 36 40 41 79 80\n $ : int [1:5] 8 9 35 47 86\n $ : int [1:5] 8 35 46 80 86\n $ : int [1:5] 50 51 52 53 55\n $ : int [1:4] 28 51 52 54\n $ : int [1:6] 32 48 51 52 54 55\n $ : int [1:4] 48 49 50 52\n $ : int [1:6] 28 48 49 50 51 54\n $ : int [1:2] 48 55\n $ : int [1:5] 24 28 49 50 52\n $ : int [1:4] 48 50 53 75\n $ : int 36\n $ : int [1:5] 1 2 3 58 64\n $ : int [1:5] 2 57 64 66 68\n $ : int [1:3] 60 87 88\n $ : int [1:4] 12 13 59 61\n $ : int [1:5] 12 60 62 63 87\n $ : int [1:4] 61 63 77 87\n $ : int [1:5] 12 18 61 62 83\n $ : int [1:4] 1 57 58 76\n $ : int 76\n $ : int [1:5] 58 67 68 76 84\n $ : int [1:2] 7 66\n $ : int [1:4] 9 58 66 84\n $ : int [1:2] 6 75\n $ : int [1:3] 10 72 73\n $ : int [1:2] 73 74\n $ : int [1:3] 10 11 70\n $ : int [1:4] 19 70 71 74\n $ : int [1:5] 19 21 71 73 86\n $ : int [1:2] 55 69\n $ : int [1:3] 64 65 66\n $ : int [1:3] 23 38 62\n $ : int [1:2] 2 8\n $ : int [1:4] 38 40 41 45\n $ : int [1:5] 34 35 36 45 47\n $ : int [1:5] 25 26 33 39 42\n $ : int [1:6] 19 20 21 23 35 41\n $ : int [1:4] 12 13 16 63\n $ : int [1:4] 7 9 66 68\n $ : int [1:2] 2 5\n $ : int [1:4] 21 46 47 74\n $ : int [1:4] 59 61 62 88\n $ : int [1:2] 59 87\n - attr(*, \"class\")= chr \"nb\"\n - attr(*, \"region.id\")= chr [1:88] \"1\" \"2\" \"3\" \"4\" ...\n - attr(*, \"call\")= language dnearneigh(x = coords_hunan, d1 = 0, d2 = 62, longlat = TRUE)\n - attr(*, \"dnn\")= num [1:2] 0 62\n - attr(*, \"bounds\")= chr [1:2] \"GE\" \"LE\"\n - attr(*, \"nbtype\")= chr \"distance\"\n - attr(*, \"sym\")= logi TRUE\n\n\nAnother way to display the structure of the weight matrix is to combine table() and card() of spdep\n\ntable(hunan$County, card(wm_d62))\n\n               \n                1 2 3 4 5 6\n  Anhua         1 0 0 0 0 0\n  Anren         0 0 0 1 0 0\n  Anxiang       0 0 0 0 1 0\n  Baojing       0 0 0 0 1 0\n  Chaling       0 0 1 0 0 0\n  Changning     0 0 1 0 0 0\n  Changsha      0 0 0 1 0 0\n  Chengbu       0 1 0 0 0 0\n  Chenxi        0 0 0 1 0 0\n  Cili          0 1 0 0 0 0\n  Dao           0 0 0 1 0 0\n  Dongan        0 0 1 0 0 0\n  Dongkou       0 0 0 1 0 0\n  Fenghuang     0 0 0 1 0 0\n  Guidong       0 0 1 0 0 0\n  Guiyang       0 0 0 1 0 0\n  Guzhang       0 0 0 0 0 1\n  Hanshou       0 0 0 1 0 0\n  Hengdong      0 0 0 0 1 0\n  Hengnan       0 0 0 0 1 0\n  Hengshan      0 0 0 0 0 1\n  Hengyang      0 0 0 0 0 1\n  Hongjiang     0 0 0 0 1 0\n  Huarong       0 0 0 1 0 0\n  Huayuan       0 0 0 1 0 0\n  Huitong       0 0 0 1 0 0\n  Jiahe         0 0 0 0 1 0\n  Jianghua      0 0 1 0 0 0\n  Jiangyong     0 1 0 0 0 0\n  Jingzhou      0 1 0 0 0 0\n  Jinshi        0 0 0 1 0 0\n  Jishou        0 0 0 0 0 1\n  Lanshan       0 0 0 1 0 0\n  Leiyang       0 0 0 1 0 0\n  Lengshuijiang 0 0 1 0 0 0\n  Li            0 0 1 0 0 0\n  Lianyuan      0 0 0 0 1 0\n  Liling        0 1 0 0 0 0\n  Linli         0 0 0 1 0 0\n  Linwu         0 0 0 1 0 0\n  Linxiang      1 0 0 0 0 0\n  Liuyang       0 1 0 0 0 0\n  Longhui       0 0 1 0 0 0\n  Longshan      0 1 0 0 0 0\n  Luxi          0 0 0 0 1 0\n  Mayang        0 0 0 0 0 1\n  Miluo         0 0 0 0 1 0\n  Nan           0 0 0 0 1 0\n  Ningxiang     0 0 0 1 0 0\n  Ningyuan      0 0 0 0 1 0\n  Pingjiang     0 1 0 0 0 0\n  Qidong        0 0 1 0 0 0\n  Qiyang        0 0 1 0 0 0\n  Rucheng       0 1 0 0 0 0\n  Sangzhi       0 1 0 0 0 0\n  Shaodong      0 0 0 0 1 0\n  Shaoshan      0 0 0 0 1 0\n  Shaoyang      0 0 0 1 0 0\n  Shimen        1 0 0 0 0 0\n  Shuangfeng    0 0 0 0 0 1\n  Shuangpai     0 0 0 1 0 0\n  Suining       0 0 0 0 1 0\n  Taojiang      0 1 0 0 0 0\n  Taoyuan       0 1 0 0 0 0\n  Tongdao       0 1 0 0 0 0\n  Wangcheng     0 0 0 1 0 0\n  Wugang        0 0 1 0 0 0\n  Xiangtan      0 0 0 1 0 0\n  Xiangxiang    0 0 0 0 1 0\n  Xiangyin      0 0 0 1 0 0\n  Xinhua        0 0 0 0 1 0\n  Xinhuang      1 0 0 0 0 0\n  Xinning       0 1 0 0 0 0\n  Xinshao       0 0 0 0 0 1\n  Xintian       0 0 0 0 1 0\n  Xupu          0 1 0 0 0 0\n  Yanling       0 0 1 0 0 0\n  Yizhang       1 0 0 0 0 0\n  Yongshun      0 0 0 1 0 0\n  Yongxing      0 0 0 1 0 0\n  You           0 0 0 1 0 0\n  Yuanjiang     0 0 0 0 1 0\n  Yuanling      1 0 0 0 0 0\n  Yueyang       0 0 1 0 0 0\n  Zhijiang      0 0 0 0 1 0\n  Zhongfang     0 0 0 1 0 0\n  Zhuzhou       0 0 0 0 1 0\n  Zixing        0 0 1 0 0 0\n\n\n\nn_comp &lt;- n.comp.nb(wm_d62)\nn_comp$nc\n\n[1] 1\n\n\n\ntable(n_comp$comp.id)\n\n\n 1 \n88 \n\n\n\n\nPlotting Fixed Distance Weight Matrix\nWe can plot the distance weight matrix using the code chunk below\n\nplot(hunan$geometry, border = 'lightgrey')\nplot(wm_d62, coords_hunan, add = TRUE) #add centroids and links between neighbors\nplot(k1, coords_hunan, add = TRUE, col = 'red', length = 0.08) #add red coloration to the nearest neighbor based on distance\n\n\n\n\nThe red lines show the links of 1st nearest neighbours and the black lines show the links of neighbours within the cut-off distance of 62km.\nAlternatively, we can plot two separate plots next to each other, one with centroid for each region and the links to all its neighbors, and one with centroid for each region and the link to its 1st nearest neighbor.\n\npar(mfrow = c(1,2))\nplot(hunan$geometry, border = 'lightgrey', main = '1st Nearest Neighbor')\nplot(k1, coords_hunan, add = TRUE, col = 'red', length = 0.08)\nplot(hunan$geometry, border = 'lightgrey', main = 'Distance Link')\nplot(wm_d62, coords_hunan, add = TRUE, pch = 19, cex = 0.6)\n\n\n\n\n\n\nComputing Adaptive Distance Weight Matrix\nOne of the characteristics of fixed distance weight matrix is that more densely settled areas (usually the urban areas) tend to have more neighbours and the less densely settled areas (usually the rural counties) tend to have fewwer neighbours. Having many neighbours smooths the neighbour relationship across more neighbours.\nIt is possible to control the numbers of neighbours directly by using k-nearest neighbours, either accepting asymmetric neighbours or imposing symmetry\n\nknn6 &lt;- knn2nb(knearneigh(coords_hunan, k = 6))\nknn6\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 528 \nPercentage nonzero weights: 6.818182 \nAverage number of links: 6 \nNon-symmetric neighbours list\n\n\nSimilarly we can display the content of the matrix by using str()\n\nstr(knn6)\n\nList of 88\n $ : int [1:6] 2 3 4 5 57 64\n $ : int [1:6] 1 3 57 58 78 85\n $ : int [1:6] 1 2 4 5 57 85\n $ : int [1:6] 1 3 5 6 69 85\n $ : int [1:6] 1 3 4 6 69 85\n $ : int [1:6] 3 4 5 69 75 85\n $ : int [1:6] 9 66 67 71 74 84\n $ : int [1:6] 9 46 47 78 80 86\n $ : int [1:6] 8 46 66 68 84 86\n $ : int [1:6] 16 19 22 70 72 73\n $ : int [1:6] 10 14 16 17 70 72\n $ : int [1:6] 13 15 60 61 63 83\n $ : int [1:6] 12 15 60 61 63 83\n $ : int [1:6] 11 15 16 17 72 83\n $ : int [1:6] 12 13 14 17 60 83\n $ : int [1:6] 10 11 17 22 72 83\n $ : int [1:6] 10 11 14 16 72 83\n $ : int [1:6] 20 22 23 63 77 83\n $ : int [1:6] 10 20 21 73 74 82\n $ : int [1:6] 18 19 21 22 23 82\n $ : int [1:6] 19 20 35 74 82 86\n $ : int [1:6] 10 16 18 19 20 83\n $ : int [1:6] 18 20 41 77 79 82\n $ : int [1:6] 25 28 31 52 54 81\n $ : int [1:6] 24 28 31 33 54 81\n $ : int [1:6] 25 27 29 33 42 81\n $ : int [1:6] 26 29 30 37 42 81\n $ : int [1:6] 24 25 33 49 52 54\n $ : int [1:6] 26 27 37 42 43 81\n $ : int [1:6] 26 27 28 33 49 81\n $ : int [1:6] 24 25 36 39 40 54\n $ : int [1:6] 24 31 50 54 55 56\n $ : int [1:6] 25 26 28 30 49 81\n $ : int [1:6] 36 40 41 45 56 80\n $ : int [1:6] 21 41 46 47 80 82\n $ : int [1:6] 31 34 40 45 56 80\n $ : int [1:6] 26 27 29 42 43 44\n $ : int [1:6] 23 43 44 62 77 79\n $ : int [1:6] 25 40 42 43 44 81\n $ : int [1:6] 31 36 39 43 45 79\n $ : int [1:6] 23 35 45 79 80 82\n $ : int [1:6] 26 27 37 39 43 81\n $ : int [1:6] 37 39 40 42 44 79\n $ : int [1:6] 37 38 39 42 43 79\n $ : int [1:6] 34 36 40 41 79 80\n $ : int [1:6] 8 9 35 47 78 86\n $ : int [1:6] 8 21 35 46 80 86\n $ : int [1:6] 49 50 51 52 53 55\n $ : int [1:6] 28 33 48 51 52 54\n $ : int [1:6] 32 48 51 52 54 55\n $ : int [1:6] 28 48 49 50 52 54\n $ : int [1:6] 28 48 49 50 51 54\n $ : int [1:6] 48 50 51 52 55 75\n $ : int [1:6] 24 28 49 50 51 52\n $ : int [1:6] 32 48 50 52 53 75\n $ : int [1:6] 32 34 36 78 80 85\n $ : int [1:6] 1 2 3 58 64 68\n $ : int [1:6] 2 57 64 66 68 78\n $ : int [1:6] 12 13 60 61 87 88\n $ : int [1:6] 12 13 59 61 63 87\n $ : int [1:6] 12 13 60 62 63 87\n $ : int [1:6] 12 38 61 63 77 87\n $ : int [1:6] 12 18 60 61 62 83\n $ : int [1:6] 1 3 57 58 68 76\n $ : int [1:6] 58 64 66 67 68 76\n $ : int [1:6] 9 58 67 68 76 84\n $ : int [1:6] 7 65 66 68 76 84\n $ : int [1:6] 9 57 58 66 78 84\n $ : int [1:6] 4 5 6 32 75 85\n $ : int [1:6] 10 16 19 22 72 73\n $ : int [1:6] 7 19 73 74 84 86\n $ : int [1:6] 10 11 14 16 17 70\n $ : int [1:6] 10 19 21 70 71 74\n $ : int [1:6] 19 21 71 73 84 86\n $ : int [1:6] 6 32 50 53 55 69\n $ : int [1:6] 58 64 65 66 67 68\n $ : int [1:6] 18 23 38 61 62 63\n $ : int [1:6] 2 8 9 46 58 68\n $ : int [1:6] 38 40 41 43 44 45\n $ : int [1:6] 34 35 36 41 45 47\n $ : int [1:6] 25 26 28 33 39 42\n $ : int [1:6] 19 20 21 23 35 41\n $ : int [1:6] 12 13 15 16 22 63\n $ : int [1:6] 7 9 66 68 71 74\n $ : int [1:6] 2 3 4 5 56 69\n $ : int [1:6] 8 9 21 46 47 74\n $ : int [1:6] 59 60 61 62 63 88\n $ : int [1:6] 59 60 61 62 63 87\n - attr(*, \"region.id\")= chr [1:88] \"1\" \"2\" \"3\" \"4\" ...\n - attr(*, \"call\")= language knearneigh(x = coords_hunan, k = 6)\n - attr(*, \"sym\")= logi FALSE\n - attr(*, \"type\")= chr \"knn\"\n - attr(*, \"knn-k\")= num 6\n - attr(*, \"class\")= chr \"nb\"\n\n\n\nPlotting Distance Based Neighbours\nWe can plot the weight matrix using the code below\n\nplot(hunan$geometry, border = 'lightgrey')\nplot(knn6, coords_hunan, pch = 19, cex = 0.6, add = TRUE, col = 'red')"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex2/Hands-on_Ex2_Spatial_Weights.html#weight-based-on-idw",
    "href": "Hands-on_Ex/Hands-on_Ex2/Hands-on_Ex2_Spatial_Weights.html#weight-based-on-idw",
    "title": "Hands-on_Ex2_Spatial_Weights",
    "section": "Weight Based on IDW",
    "text": "Weight Based on IDW\nWe can derive a spatial weight matrix based on the Inversed Distance method\nnbdists() can be used to compute the distances between areas\n\ndist &lt;- nbdists(wm_q, coords_hunan, longlat = TRUE)\nids &lt;- lapply(dist, function(x) 1/x)\nids\n\n[[1]]\n[1] 0.01535405 0.03916350 0.01820896 0.02807922 0.01145113\n\n[[2]]\n[1] 0.01535405 0.01764308 0.01925924 0.02323898 0.01719350\n\n[[3]]\n[1] 0.03916350 0.02822040 0.03695795 0.01395765\n\n[[4]]\n[1] 0.01820896 0.02822040 0.03414741 0.01539065\n\n[[5]]\n[1] 0.03695795 0.03414741 0.01524598 0.01618354\n\n[[6]]\n[1] 0.015390649 0.015245977 0.021748129 0.011883901 0.009810297\n\n[[7]]\n[1] 0.01708612 0.01473997 0.01150924 0.01872915\n\n[[8]]\n[1] 0.02022144 0.03453056 0.02529256 0.01036340 0.02284457 0.01500600 0.01515314\n\n[[9]]\n[1] 0.02022144 0.01574888 0.02109502 0.01508028 0.02902705 0.01502980\n\n[[10]]\n[1] 0.02281552 0.01387777 0.01538326 0.01346650 0.02100510 0.02631658 0.01874863\n[8] 0.01500046\n\n[[11]]\n[1] 0.01882869 0.02243492 0.02247473\n\n[[12]]\n[1] 0.02779227 0.02419652 0.02333385 0.02986130 0.02335429\n\n[[13]]\n[1] 0.02779227 0.02650020 0.02670323 0.01714243\n\n[[14]]\n[1] 0.01882869 0.01233868 0.02098555\n\n[[15]]\n[1] 0.02650020 0.01233868 0.01096284 0.01562226\n\n[[16]]\n[1] 0.02281552 0.02466962 0.02765018 0.01476814 0.01671430\n\n[[17]]\n[1] 0.01387777 0.02243492 0.02098555 0.01096284 0.02466962 0.01593341 0.01437996\n\n[[18]]\n[1] 0.02039779 0.02032767 0.01481665 0.01473691 0.01459380\n\n[[19]]\n[1] 0.01538326 0.01926323 0.02668415 0.02140253 0.01613589 0.01412874\n\n[[20]]\n[1] 0.01346650 0.02039779 0.01926323 0.01723025 0.02153130 0.01469240 0.02327034\n\n[[21]]\n[1] 0.02668415 0.01723025 0.01766299 0.02644986 0.02163800\n\n[[22]]\n[1] 0.02100510 0.02765018 0.02032767 0.02153130 0.01489296\n\n[[23]]\n[1] 0.01481665 0.01469240 0.01401432 0.02246233 0.01880425 0.01530458 0.01849605\n\n[[24]]\n[1] 0.02354598 0.01837201 0.02607264 0.01220154 0.02514180\n\n[[25]]\n[1] 0.02354598 0.02188032 0.01577283 0.01949232 0.02947957\n\n[[26]]\n[1] 0.02155798 0.01745522 0.02212108 0.02220532\n\n[[27]]\n[1] 0.02155798 0.02490625 0.01562326\n\n[[28]]\n[1] 0.01837201 0.02188032 0.02229549 0.03076171 0.02039506\n\n[[29]]\n[1] 0.02490625 0.01686587 0.01395022\n\n[[30]]\n[1] 0.02090587\n\n[[31]]\n[1] 0.02607264 0.01577283 0.01219005 0.01724850 0.01229012 0.01609781 0.01139438\n[8] 0.01150130\n\n[[32]]\n[1] 0.01220154 0.01219005 0.01712515 0.01340413 0.01280928 0.01198216 0.01053374\n[8] 0.01065655\n\n[[33]]\n[1] 0.01949232 0.01745522 0.02229549 0.02090587 0.01979045\n\n[[34]]\n[1] 0.03113041 0.03589551 0.02882915\n\n[[35]]\n[1] 0.01766299 0.02185795 0.02616766 0.02111721 0.02108253 0.01509020\n\n[[36]]\n[1] 0.01724850 0.03113041 0.01571707 0.01860991 0.02073549 0.01680129\n\n[[37]]\n[1] 0.01686587 0.02234793 0.01510990 0.01550676\n\n[[38]]\n[1] 0.01401432 0.02407426 0.02276151 0.01719415\n\n[[39]]\n[1] 0.01229012 0.02172543 0.01711924 0.02629732 0.01896385\n\n[[40]]\n[1] 0.01609781 0.01571707 0.02172543 0.01506473 0.01987922 0.01894207\n\n[[41]]\n[1] 0.02246233 0.02185795 0.02205991 0.01912542 0.01601083 0.01742892\n\n[[42]]\n[1] 0.02212108 0.01562326 0.01395022 0.02234793 0.01711924 0.01836831 0.01683518\n\n[[43]]\n[1] 0.01510990 0.02629732 0.01506473 0.01836831 0.03112027 0.01530782\n\n[[44]]\n[1] 0.01550676 0.02407426 0.03112027 0.01486508\n\n[[45]]\n[1] 0.03589551 0.01860991 0.01987922 0.02205991 0.02107101 0.01982700\n\n[[46]]\n[1] 0.03453056 0.04033752 0.02689769\n\n[[47]]\n[1] 0.02529256 0.02616766 0.04033752 0.01949145 0.02181458\n\n[[48]]\n[1] 0.02313819 0.03370576 0.02289485 0.01630057 0.01818085\n\n[[49]]\n[1] 0.03076171 0.02138091 0.02394529 0.01990000\n\n[[50]]\n[1] 0.01712515 0.02313819 0.02551427 0.02051530 0.02187179\n\n[[51]]\n[1] 0.03370576 0.02138091 0.02873854\n\n[[52]]\n[1] 0.02289485 0.02394529 0.02551427 0.02873854 0.03516672\n\n[[53]]\n[1] 0.01630057 0.01979945 0.01253977\n\n[[54]]\n[1] 0.02514180 0.02039506 0.01340413 0.01990000 0.02051530 0.03516672\n\n[[55]]\n[1] 0.01280928 0.01818085 0.02187179 0.01979945 0.01882298\n\n[[56]]\n[1] 0.01036340 0.01139438 0.01198216 0.02073549 0.01214479 0.01362855 0.01341697\n\n[[57]]\n[1] 0.028079221 0.017643082 0.031423501 0.029114131 0.013520292 0.009903702\n\n[[58]]\n[1] 0.01925924 0.03142350 0.02722997 0.01434859 0.01567192\n\n[[59]]\n[1] 0.01696711 0.01265572 0.01667105 0.01785036\n\n[[60]]\n[1] 0.02419652 0.02670323 0.01696711 0.02343040\n\n[[61]]\n[1] 0.02333385 0.01265572 0.02343040 0.02514093 0.02790764 0.01219751 0.02362452\n\n[[62]]\n[1] 0.02514093 0.02002219 0.02110260\n\n[[63]]\n[1] 0.02986130 0.02790764 0.01407043 0.01805987\n\n[[64]]\n[1] 0.02911413 0.01689892\n\n[[65]]\n[1] 0.02471705\n\n[[66]]\n[1] 0.01574888 0.01726461 0.03068853 0.01954805 0.01810569\n\n[[67]]\n[1] 0.01708612 0.01726461 0.01349843 0.01361172\n\n[[68]]\n[1] 0.02109502 0.02722997 0.03068853 0.01406357 0.01546511\n\n[[69]]\n[1] 0.02174813 0.01645838 0.01419926\n\n[[70]]\n[1] 0.02631658 0.01963168 0.02278487\n\n[[71]]\n[1] 0.01473997 0.01838483 0.03197403\n\n[[72]]\n[1] 0.01874863 0.02247473 0.01476814 0.01593341 0.01963168\n\n[[73]]\n[1] 0.01500046 0.02140253 0.02278487 0.01838483 0.01652709\n\n[[74]]\n[1] 0.01150924 0.01613589 0.03197403 0.01652709 0.01342099 0.02864567\n\n[[75]]\n[1] 0.011883901 0.010533736 0.012539774 0.018822977 0.016458383 0.008217581\n\n[[76]]\n[1] 0.01352029 0.01434859 0.01689892 0.02471705 0.01954805 0.01349843 0.01406357\n\n[[77]]\n[1] 0.014736909 0.018804247 0.022761507 0.012197506 0.020022195 0.014070428\n[7] 0.008440896\n\n[[78]]\n[1] 0.02323898 0.02284457 0.01508028 0.01214479 0.01567192 0.01546511 0.01140779\n\n[[79]]\n[1] 0.01530458 0.01719415 0.01894207 0.01912542 0.01530782 0.01486508 0.02107101\n\n[[80]]\n[1] 0.01500600 0.02882915 0.02111721 0.01680129 0.01601083 0.01982700 0.01949145\n[8] 0.01362855\n\n[[81]]\n[1] 0.02947957 0.02220532 0.01150130 0.01979045 0.01896385 0.01683518\n\n[[82]]\n[1] 0.02327034 0.02644986 0.01849605 0.02108253 0.01742892\n\n[[83]]\n[1] 0.023354289 0.017142433 0.015622258 0.016714303 0.014379961 0.014593799\n[7] 0.014892965 0.018059871 0.008440896\n\n[[84]]\n[1] 0.01872915 0.02902705 0.01810569 0.01361172 0.01342099 0.01297994\n\n[[85]]\n [1] 0.011451133 0.017193502 0.013957649 0.016183544 0.009810297 0.010656545\n [7] 0.013416965 0.009903702 0.014199260 0.008217581 0.011407794\n\n[[86]]\n[1] 0.01515314 0.01502980 0.01412874 0.02163800 0.01509020 0.02689769 0.02181458\n[8] 0.02864567 0.01297994\n\n[[87]]\n[1] 0.01667105 0.02362452 0.02110260 0.02058034\n\n[[88]]\n[1] 0.01785036 0.02058034\n\n\n\nRow-standardised Weights Matrix\nNext, we assign weights to each neighboring polygon.\nEach neighboring polygon will be assigned equal weight (style = “W”). This is accomplished by assigning the fraction of 1/(#ofneighbors) to each neighboring county then summing the weighted income values. While this is the most intuitive way to summarise the neighbors’ values, it has one drawback in that polygons along the edges of the study area will base their lagged values on fewer polygons thus potentially over- or under-estimating the true nature of the spatial autocorrelation in the data.\nFor this example, we’ll stick with the style=“W” option for simplicity’s sake but note that other more robust options are available, notably style=“B”.\n\nrswm_q &lt;- nb2listw(wm_q, style='W', zero.policy = TRUE)\nrswm_q\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 448 \nPercentage nonzero weights: 5.785124 \nAverage number of links: 5.090909 \n\nWeights style: W \nWeights constants summary:\n   n   nn S0       S1       S2\nW 88 7744 88 37.86334 365.9147\n\n\nThe zero.policy = TRUE option allows for lists of non-neighbors. This should be used with caution since the user may not be aware of missing neighbors in their dataset. However, a zero.policy of FALSE would return an error.\nTo see the weight of the first polygon’s eight neighbors:\n\nrswm_q$weights[10]\n\n[[1]]\n[1] 0.125 0.125 0.125 0.125 0.125 0.125 0.125 0.125\n\n\nEach neighbor is assigned a 0.125 of the total weight. This means that when R computes the average neighboring income values, each neighbor’s income will be multiplied by 0.2 before being tallied.\nWe can use the same method to derive a row standardised distance weight matrix.\n\nrswm_ids &lt;- nb2listw(wm_q, glist = ids, style = 'B', zero.policy = TRUE)\nrswm_ids\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 448 \nPercentage nonzero weights: 5.785124 \nAverage number of links: 5.090909 \n\nWeights style: B \nWeights constants summary:\n   n   nn       S0        S1     S2\nB 88 7744 8.786867 0.3776535 3.8137\n\n\nTo see the weight of the first polygon’s neighbours\n\nrswm_ids$weights[1]\n\n[[1]]\n[1] 0.01535405 0.03916350 0.01820896 0.02807922 0.01145113\n\n\nWe can see the summary of the weights by using summary()\n\nsummary(unlist(rswm_ids$weights))\n\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n0.008218 0.015088 0.018739 0.019614 0.022823 0.040338"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex2/Hands-on_Ex2_Spatial_Weights.html#application-of-spatial-weight-matrix",
    "href": "Hands-on_Ex/Hands-on_Ex2/Hands-on_Ex2_Spatial_Weights.html#application-of-spatial-weight-matrix",
    "title": "Hands-on_Ex2_Spatial_Weights",
    "section": "Application of Spatial Weight Matrix",
    "text": "Application of Spatial Weight Matrix\nIn this section we will create four different spatial lagged variables:\n\nspatial lag with row-standardized weights\nspatial lag as a sum of neighbouring values\nspatial window average\nspatial window sum\n\n\nSpatial Lag with Row-standardized Weights\nWe will compute the average neighbor GDPPC value for each polygon\n\nGDPPC.lag &lt;- lag.listw(rswm_q, hunan$GDPPC)\nGDPPC.lag\n\n [1] 24847.20 22724.80 24143.25 27737.50 27270.25 21248.80 43747.00 33582.71\n [9] 45651.17 32027.62 32671.00 20810.00 25711.50 30672.33 33457.75 31689.20\n[17] 20269.00 23901.60 25126.17 21903.43 22718.60 25918.80 20307.00 20023.80\n[25] 16576.80 18667.00 14394.67 19848.80 15516.33 20518.00 17572.00 15200.12\n[33] 18413.80 14419.33 24094.50 22019.83 12923.50 14756.00 13869.80 12296.67\n[41] 15775.17 14382.86 11566.33 13199.50 23412.00 39541.00 36186.60 16559.60\n[49] 20772.50 19471.20 19827.33 15466.80 12925.67 18577.17 14943.00 24913.00\n[57] 25093.00 24428.80 17003.00 21143.75 20435.00 17131.33 24569.75 23835.50\n[65] 26360.00 47383.40 55157.75 37058.00 21546.67 23348.67 42323.67 28938.60\n[73] 25880.80 47345.67 18711.33 29087.29 20748.29 35933.71 15439.71 29787.50\n[81] 18145.00 21617.00 29203.89 41363.67 22259.09 44939.56 16902.00 16930.00\n\n\nRecalled in the previous section, we retrieved the GDPPC of these five counties\n\nnb1 &lt;- wm_q[[1]]\nnb1_gdppc &lt;- hunan$GDPPC[nb1]\nnb1_gdppc\n\n[1] 20981 34592 24473 21311 22879\n\n\nQuestion: Can you see the meaning of Spatial lag with row-standardized weights?\nFor better comparison, we can try to print both series of values\n\nprint(GDPPC.lag[wm_q[[1]]])\n\n[1] 22724.80 24143.25 27737.50 25093.00 22259.09\n\nprint(nb1_gdppc)\n\n[1] 20981 34592 24473 21311 22879\n\n\nPossible Answer: Most neighbors were adjusted slightly based on their weights, particularly neighbor id 3. This accounts geographical distance into the GDPPC value of each neighbor, accounting for the influence on the value of GDPPC by the values of GDPPC of its neighbours.\nWe can append the spatially lag GDPPC values onto hunan sf data frame by using the code chunk below.\n\nlag.list &lt;- list(hunan$NAME_3, lag.listw(rswm_q, hunan$GDPPC))\nlag.res &lt;- as.data.frame(lag.list)\ncolnames(lag.res) &lt;- c('NAME_3', 'lag GDPPC')\nhunan &lt;- left_join(hunan,lag.res)\n\nWe can see the new spatial lag GDPPC using head()\n\nhead(hunan)\n\nSimple feature collection with 6 features and 7 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 110.4922 ymin: 28.61762 xmax: 112.3013 ymax: 30.12812\nGeodetic CRS:  WGS 84\n   NAME_2  ID_3  NAME_3   ENGTYPE_3  County GDPPC lag GDPPC\n1 Changde 21098 Anxiang      County Anxiang 23667  24847.20\n2 Changde 21100 Hanshou      County Hanshou 20981  22724.80\n3 Changde 21101  Jinshi County City  Jinshi 34592  24143.25\n4 Changde 21102      Li      County      Li 24473  27737.50\n5 Changde 21103   Linli      County   Linli 25554  27270.25\n6 Changde 21104  Shimen      County  Shimen 27137  21248.80\n                        geometry\n1 POLYGON ((112.0625 29.75523...\n2 POLYGON ((112.2288 29.11684...\n3 POLYGON ((111.8927 29.6013,...\n4 POLYGON ((111.3731 29.94649...\n5 POLYGON ((111.6324 29.76288...\n6 POLYGON ((110.8825 30.11675...\n\n\nNext, we can plot the GDPPC and spatial lag GDPPC for comparison\n\ngdppc &lt;- qtm(hunan, \"GDPPC\")\nlag_gdppc &lt;- qtm(hunan, \"lag GDPPC\")\ntmap_arrange(gdppc, lag_gdppc, asp = 1, ncol = 2)\n\n\n\n\n\n\nSpatial Lag as a Sum of Neighboring Values\nWe can calculate spatial lag a sum of neighboring values by assigning binary weights. This requires us to go back to our neighbors list, then apply a function that will assign binary weights, then we use glist= in the nb2listw function to explicitly assign these weights.\nWe start by applying a function that will assign a value of 1 per each neighbor. This is done with lapply, which we have been using to manipulate the neighbors structure throughout the past notebooks. Basically it applies a function across each value in the neighbor structure.\n\nb_weights &lt;- lapply(wm_q, function(x) 0*x + 1)\nb_weights2 &lt;- nb2listw(wm_q,\n                       glist = b_weights,\n                       style = 'B')\nb_weights2\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 448 \nPercentage nonzero weights: 5.785124 \nAverage number of links: 5.090909 \n\nWeights style: B \nWeights constants summary:\n   n   nn  S0  S1    S2\nB 88 7744 448 896 10224\n\n\nWith proper weights assigned, we can use lag.listw to compute a lag variable from our weight and GDPPC\n\nlag_sum &lt;- list(hunan$NAME_3, lag.listw(b_weights2, hunan$GDPPC))\nlag.res &lt;- as.data.frame(lag_sum)\ncolnames(lag.res) &lt;- c('NAME_3', 'lag_sum GDPPC')\n\nWe can take a glimpse of the newly created data frame\n\nlag.res\n\n          NAME_3 lag_sum GDPPC\n1        Anxiang        124236\n2        Hanshou        113624\n3         Jinshi         96573\n4             Li        110950\n5          Linli        109081\n6         Shimen        106244\n7        Liuyang        174988\n8      Ningxiang        235079\n9      Wangcheng        273907\n10         Anren        256221\n11       Guidong         98013\n12         Jiahe        104050\n13         Linwu        102846\n14       Rucheng         92017\n15       Yizhang        133831\n16      Yongxing        158446\n17        Zixing        141883\n18     Changning        119508\n19      Hengdong        150757\n20       Hengnan        153324\n21      Hengshan        113593\n22       Leiyang        129594\n23        Qidong        142149\n24        Chenxi        100119\n25     Zhongfang         82884\n26       Huitong         74668\n27      Jingzhou         43184\n28        Mayang         99244\n29       Tongdao         46549\n30      Xinhuang         20518\n31          Xupu        140576\n32      Yuanling        121601\n33      Zhijiang         92069\n34 Lengshuijiang         43258\n35    Shuangfeng        144567\n36        Xinhua        132119\n37       Chengbu         51694\n38        Dongan         59024\n39       Dongkou         69349\n40       Longhui         73780\n41      Shaodong         94651\n42       Suining        100680\n43        Wugang         69398\n44       Xinning         52798\n45       Xinshao        140472\n46      Shaoshan        118623\n47    Xiangxiang        180933\n48       Baojing         82798\n49     Fenghuang         83090\n50       Guzhang         97356\n51       Huayuan         59482\n52        Jishou         77334\n53      Longshan         38777\n54          Luxi        111463\n55      Yongshun         74715\n56         Anhua        174391\n57           Nan        150558\n58     Yuanjiang        122144\n59      Jianghua         68012\n60       Lanshan         84575\n61      Ningyuan        143045\n62     Shuangpai         51394\n63       Xintian         98279\n64       Huarong         47671\n65      Linxiang         26360\n66         Miluo        236917\n67     Pingjiang        220631\n68      Xiangyin        185290\n69          Cili         64640\n70       Chaling         70046\n71        Liling        126971\n72       Yanling        144693\n73           You        129404\n74       Zhuzhou        284074\n75       Sangzhi        112268\n76       Yueyang        203611\n77        Qiyang        145238\n78      Taojiang        251536\n79      Shaoyang        108078\n80      Lianyuan        238300\n81     Hongjiang        108870\n82      Hengyang        108085\n83       Guiyang        262835\n84      Changsha        248182\n85       Taoyuan        244850\n86      Xiangtan        404456\n87           Dao         67608\n88     Jiangyong         33860\n\n\nQuestion: Can you understand the meaning of Spatial Lag as a Sum of Neighboring Values?\nAnswer: Instead of using the GDPPC value of the polygon, this method sums the GDPPC values of all of its neighbors\nWe can append the lag_sum GDPPC field into the hunan sf data frame\n\nhunan &lt;- left_join(hunan, lag.res)\n\nNext, we can plot the GDPPC and spatial lag as sum of neighbors GDPPC for comparison\n\ngdppc &lt;- qtm(hunan, 'GDPPC')\nlag_sum_gdppc &lt;- qtm(hunan, 'lag_sum GDPPC')\ntmap_arrange(gdppc, lag_sum_gdppc, asp = 1, ncol = 2)\n\n\n\n\n\n\nSpatial Window Average\nThe spatial window average uses row-standardized weights and includes the diagonal element, or the self-weight. To do this in R, we need to go back to the neighbors structure and add the diagonal element before assigning weights.\nTo add the diagonal element to the neighbour list, we need to use include.self() from spdep.\n\nwm_qs &lt;- include.self(wm_q)\n\nNotice that the Number of nonzero links, Percentage nonzero weights and Average number of links are 536, 6.921488 and 6.090909 respectively as compared to wm_q of 448, 5.785124 and 5.090909\nWe can take a good look at the neighbour list of area [1]\n\nwm_qs[[1]]\n\n[1]  1  2  3  4 57 85\n\n\nNow, [1] has six neighbours instead of five, including itself.\nNow we can obtain the weights with nb2listw()\n\nwm_qs &lt;- nb2listw(wm_qs)\nwm_qs\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 536 \nPercentage nonzero weights: 6.921488 \nAverage number of links: 6.090909 \n\nWeights style: W \nWeights constants summary:\n   n   nn S0       S1       S2\nW 88 7744 88 30.90265 357.5308\n\n\nLastly, we can create the lag variable from our weight structure and GDPPC variable\n\nlag_w_avg_gdppc &lt;- lag.listw(wm_qs, hunan$GDPPC)\n\nlag_w_avg_gdppc\n\n [1] 24650.50 22434.17 26233.00 27084.60 26927.00 22230.17 47621.20 37160.12\n [9] 49224.71 29886.89 26627.50 22690.17 25366.40 25825.75 30329.00 32682.83\n[17] 25948.62 23987.67 25463.14 21904.38 23127.50 25949.83 20018.75 19524.17\n[25] 18955.00 17800.40 15883.00 18831.33 14832.50 17965.00 17159.89 16199.44\n[33] 18764.50 26878.75 23188.86 20788.14 12365.20 15985.00 13764.83 11907.43\n[41] 17128.14 14593.62 11644.29 12706.00 21712.29 43548.25 35049.00 16226.83\n[49] 19294.40 18156.00 19954.75 18145.17 12132.75 18419.29 14050.83 23619.75\n[57] 24552.71 24733.67 16762.60 20932.60 19467.75 18334.00 22541.00 26028.00\n[65] 29128.50 46569.00 47576.60 36545.50 20838.50 22531.00 42115.50 27619.00\n[73] 27611.33 44523.29 18127.43 28746.38 20734.50 33880.62 14716.38 28516.22\n[81] 18086.14 21244.50 29568.80 48119.71 22310.75 43151.60 17133.40 17009.33\n\n\nNext, we will convert the lag variable listw into a data frame similar to what we have done previously\n\nlag.list.wm_qs &lt;- list(hunan$NAME_3, lag.listw(wm_qs, hunan$GDPPC))\nlag_wm_qs.res &lt;- as.data.frame(lag.list.wm_qs)\ncolnames(lag_wm_qs.res) &lt;- c('NAME_3', 'lag_window_avg GDPPC')\n\nNow, we can append this data frame onto the original hunan sf data frame\n\nhunan &lt;- left_join(hunan, lag_wm_qs.res)\n\nTo compare the values of lag GDPPC and Spatial Window Average, kable() is used\n\nhunan %&gt;%\n  select('County', 'lag GDPPC', 'lag_window_avg GDPPC') %&gt;%\n  kable()\n\n\n\n\n\n\n\n\n\n\nCounty\nlag GDPPC\nlag_window_avg GDPPC\ngeometry\n\n\n\n\nAnxiang\n24847.20\n24650.50\nPOLYGON ((112.0625 29.75523…\n\n\nHanshou\n22724.80\n22434.17\nPOLYGON ((112.2288 29.11684…\n\n\nJinshi\n24143.25\n26233.00\nPOLYGON ((111.8927 29.6013,…\n\n\nLi\n27737.50\n27084.60\nPOLYGON ((111.3731 29.94649…\n\n\nLinli\n27270.25\n26927.00\nPOLYGON ((111.6324 29.76288…\n\n\nShimen\n21248.80\n22230.17\nPOLYGON ((110.8825 30.11675…\n\n\nLiuyang\n43747.00\n47621.20\nPOLYGON ((113.9905 28.5682,…\n\n\nNingxiang\n33582.71\n37160.12\nPOLYGON ((112.7181 28.38299…\n\n\nWangcheng\n45651.17\n49224.71\nPOLYGON ((112.7914 28.52688…\n\n\nAnren\n32027.62\n29886.89\nPOLYGON ((113.1757 26.82734…\n\n\nGuidong\n32671.00\n26627.50\nPOLYGON ((114.1799 26.20117…\n\n\nJiahe\n20810.00\n22690.17\nPOLYGON ((112.4425 25.74358…\n\n\nLinwu\n25711.50\n25366.40\nPOLYGON ((112.5914 25.55143…\n\n\nRucheng\n30672.33\n25825.75\nPOLYGON ((113.6759 25.87578…\n\n\nYizhang\n33457.75\n30329.00\nPOLYGON ((113.2621 25.68394…\n\n\nYongxing\n31689.20\n32682.83\nPOLYGON ((113.3169 26.41843…\n\n\nZixing\n20269.00\n25948.62\nPOLYGON ((113.7311 26.16259…\n\n\nChangning\n23901.60\n23987.67\nPOLYGON ((112.6144 26.60198…\n\n\nHengdong\n25126.17\n25463.14\nPOLYGON ((113.1056 27.21007…\n\n\nHengnan\n21903.43\n21904.38\nPOLYGON ((112.7599 26.98149…\n\n\nHengshan\n22718.60\n23127.50\nPOLYGON ((112.607 27.4689, …\n\n\nLeiyang\n25918.80\n25949.83\nPOLYGON ((112.9996 26.69276…\n\n\nQidong\n20307.00\n20018.75\nPOLYGON ((111.7818 27.0383,…\n\n\nChenxi\n20023.80\n19524.17\nPOLYGON ((110.2624 28.21778…\n\n\nZhongfang\n16576.80\n18955.00\nPOLYGON ((109.9431 27.72858…\n\n\nHuitong\n18667.00\n17800.40\nPOLYGON ((109.9419 27.10512…\n\n\nJingzhou\n14394.67\n15883.00\nPOLYGON ((109.8186 26.75842…\n\n\nMayang\n19848.80\n18831.33\nPOLYGON ((109.795 27.98008,…\n\n\nTongdao\n15516.33\n14832.50\nPOLYGON ((109.9294 26.46561…\n\n\nXinhuang\n20518.00\n17965.00\nPOLYGON ((109.227 27.43733,…\n\n\nXupu\n17572.00\n17159.89\nPOLYGON ((110.7189 28.30485…\n\n\nYuanling\n15200.12\n16199.44\nPOLYGON ((110.9652 28.99895…\n\n\nZhijiang\n18413.80\n18764.50\nPOLYGON ((109.8818 27.60661…\n\n\nLengshuijiang\n14419.33\n26878.75\nPOLYGON ((111.5307 27.81472…\n\n\nShuangfeng\n24094.50\n23188.86\nPOLYGON ((112.263 27.70421,…\n\n\nXinhua\n22019.83\n20788.14\nPOLYGON ((111.3345 28.19642…\n\n\nChengbu\n12923.50\n12365.20\nPOLYGON ((110.4455 26.69317…\n\n\nDongan\n14756.00\n15985.00\nPOLYGON ((111.4531 26.86812…\n\n\nDongkou\n13869.80\n13764.83\nPOLYGON ((110.6622 27.37305…\n\n\nLonghui\n12296.67\n11907.43\nPOLYGON ((110.985 27.65983,…\n\n\nShaodong\n15775.17\n17128.14\nPOLYGON ((111.9054 27.40254…\n\n\nSuining\n14382.86\n14593.62\nPOLYGON ((110.389 27.10006,…\n\n\nWugang\n11566.33\n11644.29\nPOLYGON ((110.9878 27.03345…\n\n\nXinning\n13199.50\n12706.00\nPOLYGON ((111.0736 26.84627…\n\n\nXinshao\n23412.00\n21712.29\nPOLYGON ((111.6013 27.58275…\n\n\nShaoshan\n39541.00\n43548.25\nPOLYGON ((112.5391 27.97742…\n\n\nXiangxiang\n36186.60\n35049.00\nPOLYGON ((112.4549 28.05783…\n\n\nBaojing\n16559.60\n16226.83\nPOLYGON ((109.7015 28.82844…\n\n\nFenghuang\n20772.50\n19294.40\nPOLYGON ((109.5239 28.19206…\n\n\nGuzhang\n19471.20\n18156.00\nPOLYGON ((109.8968 28.74034…\n\n\nHuayuan\n19827.33\n19954.75\nPOLYGON ((109.5647 28.61712…\n\n\nJishou\n15466.80\n18145.17\nPOLYGON ((109.8375 28.4696,…\n\n\nLongshan\n12925.67\n12132.75\nPOLYGON ((109.6337 29.62521…\n\n\nLuxi\n18577.17\n18419.29\nPOLYGON ((110.1067 28.41835…\n\n\nYongshun\n14943.00\n14050.83\nPOLYGON ((110.0003 29.29499…\n\n\nAnhua\n24913.00\n23619.75\nPOLYGON ((111.6034 28.63716…\n\n\nNan\n25093.00\n24552.71\nPOLYGON ((112.3232 29.46074…\n\n\nYuanjiang\n24428.80\n24733.67\nPOLYGON ((112.4391 29.1791,…\n\n\nJianghua\n17003.00\n16762.60\nPOLYGON ((111.6461 25.29661…\n\n\nLanshan\n21143.75\n20932.60\nPOLYGON ((112.2286 25.61123…\n\n\nNingyuan\n20435.00\n19467.75\nPOLYGON ((112.0715 26.09892…\n\n\nShuangpai\n17131.33\n18334.00\nPOLYGON ((111.8864 26.11957…\n\n\nXintian\n24569.75\n22541.00\nPOLYGON ((112.2578 26.0796,…\n\n\nHuarong\n23835.50\n26028.00\nPOLYGON ((112.9242 29.69134…\n\n\nLinxiang\n26360.00\n29128.50\nPOLYGON ((113.5502 29.67418…\n\n\nMiluo\n47383.40\n46569.00\nPOLYGON ((112.9902 29.02139…\n\n\nPingjiang\n55157.75\n47576.60\nPOLYGON ((113.8436 29.06152…\n\n\nXiangyin\n37058.00\n36545.50\nPOLYGON ((112.9173 28.98264…\n\n\nCili\n21546.67\n20838.50\nPOLYGON ((110.8822 29.69017…\n\n\nChaling\n23348.67\n22531.00\nPOLYGON ((113.7666 27.10573…\n\n\nLiling\n42323.67\n42115.50\nPOLYGON ((113.5673 27.94346…\n\n\nYanling\n28938.60\n27619.00\nPOLYGON ((113.9292 26.6154,…\n\n\nYou\n25880.80\n27611.33\nPOLYGON ((113.5879 27.41324…\n\n\nZhuzhou\n47345.67\n44523.29\nPOLYGON ((113.2493 28.02411…\n\n\nSangzhi\n18711.33\n18127.43\nPOLYGON ((110.556 29.40543,…\n\n\nYueyang\n29087.29\n28746.38\nPOLYGON ((113.343 29.61064,…\n\n\nQiyang\n20748.29\n20734.50\nPOLYGON ((111.5563 26.81318…\n\n\nTaojiang\n35933.71\n33880.62\nPOLYGON ((112.0508 28.67265…\n\n\nShaoyang\n15439.71\n14716.38\nPOLYGON ((111.5013 27.30207…\n\n\nLianyuan\n29787.50\n28516.22\nPOLYGON ((111.6789 28.02946…\n\n\nHongjiang\n18145.00\n18086.14\nPOLYGON ((110.1441 27.47513…\n\n\nHengyang\n21617.00\n21244.50\nPOLYGON ((112.7144 26.98613…\n\n\nGuiyang\n29203.89\n29568.80\nPOLYGON ((113.0811 26.04963…\n\n\nChangsha\n41363.67\n48119.71\nPOLYGON ((112.9421 28.03722…\n\n\nTaoyuan\n22259.09\n22310.75\nPOLYGON ((112.0612 29.32855…\n\n\nXiangtan\n44939.56\n43151.60\nPOLYGON ((113.0426 27.8942,…\n\n\nDao\n16902.00\n17133.40\nPOLYGON ((111.498 25.81679,…\n\n\nJiangyong\n16930.00\n17009.33\nPOLYGON ((111.3659 25.39472…\n\n\n\n\n\nFinally, we can create two plots in order to compare how lag GDPPC and lag_window_avg GDPPC are plotted\n\nw_avg_gdppc &lt;- qtm(hunan, 'lag_window_avg GDPPC')\ntmap_arrange(lag_gdppc, w_avg_gdppc, asp = 1, ncol = 2)\n\n\n\n\n\n\nSpatial Window Sum\nThe spatial window sum is the counterpart of the window average, but without using row-standardized weights.\nFirst, we create the neighbor list including self\n\nwm_qs &lt;- include.self(wm_q)\nwm_qs\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 536 \nPercentage nonzero weights: 6.921488 \nAverage number of links: 6.090909 \n\n\nNext, we can assign binary weights to the neighbour structure that includes the diagonal element similar to what was done in Spatial Lag as a Sum of Neighboring Values.\n\nb_weights &lt;- lapply(wm_qs, function(x) 0*x+1)\nb_weights[1]\n\n[[1]]\n[1] 1 1 1 1 1 1\n\n\nSimilar to Spatial Window Average, [1] now has six neighbours\nNow we can use nb2listw() to assign weight values, which is now binary\n\nb_weights2 &lt;- nb2listw(wm_qs,\n                       glist = b_weights,\n                       style = 'B')\nb_weights2\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 536 \nPercentage nonzero weights: 6.921488 \nAverage number of links: 6.090909 \n\nWeights style: B \nWeights constants summary:\n   n   nn  S0   S1    S2\nB 88 7744 536 1072 14160\n\n\nWith our new weight structure, we can compute the lag variable with lag.listw\n\nw_sum_gdppc &lt;- list(hunan$NAME_3, lag.listw(b_weights2, hunan$GDPPC))\nw_sum_gdppc\n\n[[1]]\n [1] \"Anxiang\"       \"Hanshou\"       \"Jinshi\"        \"Li\"           \n [5] \"Linli\"         \"Shimen\"        \"Liuyang\"       \"Ningxiang\"    \n [9] \"Wangcheng\"     \"Anren\"         \"Guidong\"       \"Jiahe\"        \n[13] \"Linwu\"         \"Rucheng\"       \"Yizhang\"       \"Yongxing\"     \n[17] \"Zixing\"        \"Changning\"     \"Hengdong\"      \"Hengnan\"      \n[21] \"Hengshan\"      \"Leiyang\"       \"Qidong\"        \"Chenxi\"       \n[25] \"Zhongfang\"     \"Huitong\"       \"Jingzhou\"      \"Mayang\"       \n[29] \"Tongdao\"       \"Xinhuang\"      \"Xupu\"          \"Yuanling\"     \n[33] \"Zhijiang\"      \"Lengshuijiang\" \"Shuangfeng\"    \"Xinhua\"       \n[37] \"Chengbu\"       \"Dongan\"        \"Dongkou\"       \"Longhui\"      \n[41] \"Shaodong\"      \"Suining\"       \"Wugang\"        \"Xinning\"      \n[45] \"Xinshao\"       \"Shaoshan\"      \"Xiangxiang\"    \"Baojing\"      \n[49] \"Fenghuang\"     \"Guzhang\"       \"Huayuan\"       \"Jishou\"       \n[53] \"Longshan\"      \"Luxi\"          \"Yongshun\"      \"Anhua\"        \n[57] \"Nan\"           \"Yuanjiang\"     \"Jianghua\"      \"Lanshan\"      \n[61] \"Ningyuan\"      \"Shuangpai\"     \"Xintian\"       \"Huarong\"      \n[65] \"Linxiang\"      \"Miluo\"         \"Pingjiang\"     \"Xiangyin\"     \n[69] \"Cili\"          \"Chaling\"       \"Liling\"        \"Yanling\"      \n[73] \"You\"           \"Zhuzhou\"       \"Sangzhi\"       \"Yueyang\"      \n[77] \"Qiyang\"        \"Taojiang\"      \"Shaoyang\"      \"Lianyuan\"     \n[81] \"Hongjiang\"     \"Hengyang\"      \"Guiyang\"       \"Changsha\"     \n[85] \"Taoyuan\"       \"Xiangtan\"      \"Dao\"           \"Jiangyong\"    \n\n[[2]]\n [1] 147903 134605 131165 135423 134635 133381 238106 297281 344573 268982\n[11] 106510 136141 126832 103303 151645 196097 207589 143926 178242 175235\n[21] 138765 155699 160150 117145 113730  89002  63532 112988  59330  35930\n[31] 154439 145795 112587 107515 162322 145517  61826  79925  82589  83352\n[41] 119897 116749  81510  63530 151986 174193 210294  97361  96472 108936\n[51]  79819 108871  48531 128935  84305 188958 171869 148402  83813 104663\n[61] 155742  73336 112705  78084  58257 279414 237883 219273  83354  90124\n[71] 168462 165714 165668 311663 126892 229971 165876 271045 117731 256646\n[81] 126603 127467 295688 336838 267729 431516  85667  51028\n\n\nNext, we will convert this object into a data frame\n\nw_sum_gdppc.res &lt;- as.data.frame(w_sum_gdppc)\ncolnames(w_sum_gdppc.res) &lt;- c('NAME_3', 'w_sum GDPPC')\n\nNext, we will join it with the original hunan data frame\n\nhunan &lt;- left_join(hunan, w_sum_gdppc.res)\n\nTo compare the values of lag GDPPC and Spatial window average, kable() of Knitr package is used to prepare a table using the code chunk below.\n\nhunan %&gt;%\n  select('County', 'lag_sum GDPPC', 'w_sum GDPPC') %&gt;%\n  kable()\n\n\n\n\n\n\n\n\n\n\nCounty\nlag_sum GDPPC\nw_sum GDPPC\ngeometry\n\n\n\n\nAnxiang\n124236\n147903\nPOLYGON ((112.0625 29.75523…\n\n\nHanshou\n113624\n134605\nPOLYGON ((112.2288 29.11684…\n\n\nJinshi\n96573\n131165\nPOLYGON ((111.8927 29.6013,…\n\n\nLi\n110950\n135423\nPOLYGON ((111.3731 29.94649…\n\n\nLinli\n109081\n134635\nPOLYGON ((111.6324 29.76288…\n\n\nShimen\n106244\n133381\nPOLYGON ((110.8825 30.11675…\n\n\nLiuyang\n174988\n238106\nPOLYGON ((113.9905 28.5682,…\n\n\nNingxiang\n235079\n297281\nPOLYGON ((112.7181 28.38299…\n\n\nWangcheng\n273907\n344573\nPOLYGON ((112.7914 28.52688…\n\n\nAnren\n256221\n268982\nPOLYGON ((113.1757 26.82734…\n\n\nGuidong\n98013\n106510\nPOLYGON ((114.1799 26.20117…\n\n\nJiahe\n104050\n136141\nPOLYGON ((112.4425 25.74358…\n\n\nLinwu\n102846\n126832\nPOLYGON ((112.5914 25.55143…\n\n\nRucheng\n92017\n103303\nPOLYGON ((113.6759 25.87578…\n\n\nYizhang\n133831\n151645\nPOLYGON ((113.2621 25.68394…\n\n\nYongxing\n158446\n196097\nPOLYGON ((113.3169 26.41843…\n\n\nZixing\n141883\n207589\nPOLYGON ((113.7311 26.16259…\n\n\nChangning\n119508\n143926\nPOLYGON ((112.6144 26.60198…\n\n\nHengdong\n150757\n178242\nPOLYGON ((113.1056 27.21007…\n\n\nHengnan\n153324\n175235\nPOLYGON ((112.7599 26.98149…\n\n\nHengshan\n113593\n138765\nPOLYGON ((112.607 27.4689, …\n\n\nLeiyang\n129594\n155699\nPOLYGON ((112.9996 26.69276…\n\n\nQidong\n142149\n160150\nPOLYGON ((111.7818 27.0383,…\n\n\nChenxi\n100119\n117145\nPOLYGON ((110.2624 28.21778…\n\n\nZhongfang\n82884\n113730\nPOLYGON ((109.9431 27.72858…\n\n\nHuitong\n74668\n89002\nPOLYGON ((109.9419 27.10512…\n\n\nJingzhou\n43184\n63532\nPOLYGON ((109.8186 26.75842…\n\n\nMayang\n99244\n112988\nPOLYGON ((109.795 27.98008,…\n\n\nTongdao\n46549\n59330\nPOLYGON ((109.9294 26.46561…\n\n\nXinhuang\n20518\n35930\nPOLYGON ((109.227 27.43733,…\n\n\nXupu\n140576\n154439\nPOLYGON ((110.7189 28.30485…\n\n\nYuanling\n121601\n145795\nPOLYGON ((110.9652 28.99895…\n\n\nZhijiang\n92069\n112587\nPOLYGON ((109.8818 27.60661…\n\n\nLengshuijiang\n43258\n107515\nPOLYGON ((111.5307 27.81472…\n\n\nShuangfeng\n144567\n162322\nPOLYGON ((112.263 27.70421,…\n\n\nXinhua\n132119\n145517\nPOLYGON ((111.3345 28.19642…\n\n\nChengbu\n51694\n61826\nPOLYGON ((110.4455 26.69317…\n\n\nDongan\n59024\n79925\nPOLYGON ((111.4531 26.86812…\n\n\nDongkou\n69349\n82589\nPOLYGON ((110.6622 27.37305…\n\n\nLonghui\n73780\n83352\nPOLYGON ((110.985 27.65983,…\n\n\nShaodong\n94651\n119897\nPOLYGON ((111.9054 27.40254…\n\n\nSuining\n100680\n116749\nPOLYGON ((110.389 27.10006,…\n\n\nWugang\n69398\n81510\nPOLYGON ((110.9878 27.03345…\n\n\nXinning\n52798\n63530\nPOLYGON ((111.0736 26.84627…\n\n\nXinshao\n140472\n151986\nPOLYGON ((111.6013 27.58275…\n\n\nShaoshan\n118623\n174193\nPOLYGON ((112.5391 27.97742…\n\n\nXiangxiang\n180933\n210294\nPOLYGON ((112.4549 28.05783…\n\n\nBaojing\n82798\n97361\nPOLYGON ((109.7015 28.82844…\n\n\nFenghuang\n83090\n96472\nPOLYGON ((109.5239 28.19206…\n\n\nGuzhang\n97356\n108936\nPOLYGON ((109.8968 28.74034…\n\n\nHuayuan\n59482\n79819\nPOLYGON ((109.5647 28.61712…\n\n\nJishou\n77334\n108871\nPOLYGON ((109.8375 28.4696,…\n\n\nLongshan\n38777\n48531\nPOLYGON ((109.6337 29.62521…\n\n\nLuxi\n111463\n128935\nPOLYGON ((110.1067 28.41835…\n\n\nYongshun\n74715\n84305\nPOLYGON ((110.0003 29.29499…\n\n\nAnhua\n174391\n188958\nPOLYGON ((111.6034 28.63716…\n\n\nNan\n150558\n171869\nPOLYGON ((112.3232 29.46074…\n\n\nYuanjiang\n122144\n148402\nPOLYGON ((112.4391 29.1791,…\n\n\nJianghua\n68012\n83813\nPOLYGON ((111.6461 25.29661…\n\n\nLanshan\n84575\n104663\nPOLYGON ((112.2286 25.61123…\n\n\nNingyuan\n143045\n155742\nPOLYGON ((112.0715 26.09892…\n\n\nShuangpai\n51394\n73336\nPOLYGON ((111.8864 26.11957…\n\n\nXintian\n98279\n112705\nPOLYGON ((112.2578 26.0796,…\n\n\nHuarong\n47671\n78084\nPOLYGON ((112.9242 29.69134…\n\n\nLinxiang\n26360\n58257\nPOLYGON ((113.5502 29.67418…\n\n\nMiluo\n236917\n279414\nPOLYGON ((112.9902 29.02139…\n\n\nPingjiang\n220631\n237883\nPOLYGON ((113.8436 29.06152…\n\n\nXiangyin\n185290\n219273\nPOLYGON ((112.9173 28.98264…\n\n\nCili\n64640\n83354\nPOLYGON ((110.8822 29.69017…\n\n\nChaling\n70046\n90124\nPOLYGON ((113.7666 27.10573…\n\n\nLiling\n126971\n168462\nPOLYGON ((113.5673 27.94346…\n\n\nYanling\n144693\n165714\nPOLYGON ((113.9292 26.6154,…\n\n\nYou\n129404\n165668\nPOLYGON ((113.5879 27.41324…\n\n\nZhuzhou\n284074\n311663\nPOLYGON ((113.2493 28.02411…\n\n\nSangzhi\n112268\n126892\nPOLYGON ((110.556 29.40543,…\n\n\nYueyang\n203611\n229971\nPOLYGON ((113.343 29.61064,…\n\n\nQiyang\n145238\n165876\nPOLYGON ((111.5563 26.81318…\n\n\nTaojiang\n251536\n271045\nPOLYGON ((112.0508 28.67265…\n\n\nShaoyang\n108078\n117731\nPOLYGON ((111.5013 27.30207…\n\n\nLianyuan\n238300\n256646\nPOLYGON ((111.6789 28.02946…\n\n\nHongjiang\n108870\n126603\nPOLYGON ((110.1441 27.47513…\n\n\nHengyang\n108085\n127467\nPOLYGON ((112.7144 26.98613…\n\n\nGuiyang\n262835\n295688\nPOLYGON ((113.0811 26.04963…\n\n\nChangsha\n248182\n336838\nPOLYGON ((112.9421 28.03722…\n\n\nTaoyuan\n244850\n267729\nPOLYGON ((112.0612 29.32855…\n\n\nXiangtan\n404456\n431516\nPOLYGON ((113.0426 27.8942,…\n\n\nDao\n67608\n85667\nPOLYGON ((111.498 25.81679,…\n\n\nJiangyong\n33860\n51028\nPOLYGON ((111.3659 25.39472…\n\n\n\n\n\nLastly, we can draw plots to compare the two methods: Spatial Lag as a Sum of Neighboring Values and Spatial Window Sum\n\nw_sum_gdppc &lt;- qtm(hunan, 'w_sum GDPPC')\ntmap_arrange(lag_sum_gdppc, w_sum_gdppc, asp = 1, ncol = 2)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex2/Hands-on_Ex2_Global_Measures.html",
    "href": "Hands-on_Ex/Hands-on_Ex2/Hands-on_Ex2_Global_Measures.html",
    "title": "Hands-on_Ex2_Global_Measures",
    "section": "",
    "text": "The goal of this hands-on exercise is to compute Global and Local Measures of Spatial Autocorrelation (GLSA)."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex2/Hands-on_Ex2_Global_Measures.html#overview",
    "href": "Hands-on_Ex/Hands-on_Ex2/Hands-on_Ex2_Global_Measures.html#overview",
    "title": "Hands-on_Ex2_Global_Measures",
    "section": "",
    "text": "The goal of this hands-on exercise is to compute Global and Local Measures of Spatial Autocorrelation (GLSA)."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex2/Hands-on_Ex2_Global_Measures.html#getting-started",
    "href": "Hands-on_Ex/Hands-on_Ex2/Hands-on_Ex2_Global_Measures.html#getting-started",
    "title": "Hands-on_Ex2_Global_Measures",
    "section": "Getting Started",
    "text": "Getting Started\n\nThe Analytical Question\nIn spatial policy, one of the main development objectives of the local government and planners is to ensure equal distribution of development in the province.\nOur task is to apply appropriate spatial statistical methods to discover if development are evenly distributed geographically.\n\nIf the answer is No, then our next question will be “is there a sign of spatial clustering?”.\n\nIf the answer is Yes, then our next question will be “Where are the clusters?”.\n\n\nIn this case study, we are interested to examine the spatial pattern of a selected development indicator (i.e. GDP per capita) of Hunan Province, People’s Republic of China (PRC).\n\n\nThe Study Area and Data\nTwo data sets will be used:\n\nHunan Province administrative boundary layer at county level. This is a geospatial dataset in ESRI shapefile format\nHunan_2012.csv: This csv file contains selected Hunan’s local development indicators in 2012\n\n\n\nLoading the Required Packages\nWe can use p_load() in the pacman package to load the required packages for data analysis: spdep (for spatial weights), sf, tmap, and tidyverse.\n\npacman::p_load(spdep, sf, tmap, tidyverse)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex2/Hands-on_Ex2_Global_Measures.html#getting-the-data-into-r-environment",
    "href": "Hands-on_Ex/Hands-on_Ex2/Hands-on_Ex2_Global_Measures.html#getting-the-data-into-r-environment",
    "title": "Hands-on_Ex2_Global_Measures",
    "section": "Getting the Data into R Environment",
    "text": "Getting the Data into R Environment\n\nImport shapefile into R\nst_read() can be used to import the Hunan shapefile into R as a simple features object.\n\nhunan &lt;- st_read(dsn = 'data/geospatial',\n                 layer = 'Hunan')\n\nReading layer `Hunan' from data source \n  `D:\\phlong2023\\ISSS624\\Hands-on_Ex\\Hands-on_Ex2\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 88 features and 7 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 108.7831 ymin: 24.6342 xmax: 114.2544 ymax: 30.12812\nGeodetic CRS:  WGS 84\n\n\n\n\nImport csv file into R\nread_csv() can be used to import the Hunan_2012.csv into R.\n\nhunan2012 &lt;- read_csv('data/aspatial/Hunan_2012.csv')\n\n\n\nPerforming Relational Join\nleft_join() can be used to join the attribute fields in hunan2012 with the hunan simple feature object.\n\n\n\n\n\n\nNote\n\n\n\nNote that left_join() automatically seeks out the shared column to join the data frames. However it can also by specified with the syntax: by = join_by(County)\n\n\n\nhunan &lt;- left_join(hunan, hunan2012) %&gt;%\n  select(1:4, 7, 15)\n\n\n\nVisualizing Regional Development Indicator\nThe tmap package can be used to prepare choropleth maps to show the distribution of GDP per capita (GDPPC) according to different breaks style (‘equal’, ‘quantile’).\n\nequal &lt;- tm_shape(hunan)+\n  tm_fill('GDPPC',\n          n = 5,\n          style = 'equal')+\n  tm_borders(alpha = 0.5)+\n  tm_layout(main.title = 'Equal Interval Classification')\n\nquantile &lt;- tm_shape(hunan)+\n  tm_fill('GDPPC',\n          n = 5,\n          style = 'quantile')+\n  tm_borders(alpha = 0.5)+\n  tm_layout(main.title = 'Equal Quantile Classification')\n\ntmap_arrange(equal, quantile, asp = 1, ncol = 2)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex2/Hands-on_Ex2_Global_Measures.html#global-spatial-autocorrelation",
    "href": "Hands-on_Ex/Hands-on_Ex2/Hands-on_Ex2_Global_Measures.html#global-spatial-autocorrelation",
    "title": "Hands-on_Ex2_Global_Measures",
    "section": "Global Spatial Autocorrelation",
    "text": "Global Spatial Autocorrelation\n\nComputing Contiguity Spatial Weights\nBefore we can compute the global spatial autocorrelation statistics, we need to construct a spatial weights of the study area. The spatial weights is used to define the neighbourhood relationships between the geographical units in the study area.\npoly2nb() is used to compute contiguity weight matrices for the study area. This function builds a neighbours list based on regions with contiguous boundaries. For this case study, we will use a Queen contiguity criteria, which look like below.\n\n\nwm_q &lt;- poly2nb(hunan, queen = TRUE)\nsummary(wm_q)\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 448 \nPercentage nonzero weights: 5.785124 \nAverage number of links: 5.090909 \nLink number distribution:\n\n 1  2  3  4  5  6  7  8  9 11 \n 2  2 12 16 24 14 11  4  2  1 \n2 least connected regions:\n30 65 with 1 link\n1 most connected region:\n85 with 11 links\n\n\nThe summary report above shows that there are 88 area units in Hunan. The most connected area unit has 11 neighbours (area 85). There are two area units with only 1 neighbour (30 and 65).\n\n\nRow-standardized Weights Matrix\nNext, we need to assign weights to each neighboring polygon.\nIn our case, each neighboring polygon will be assigned equal weight (style = ‘W’). This is accomplished by assigning 1/(#ofneighbors) to each neighboring county then summing the weighted income values.\nWhile this is the most intuitive way to summarize the neighbors’ values, it has one drawback in that polygons along the edges of the study area will base their lagged values on fewer polygons, thus potentially over- or under-estimating the true nature of the spatial autocorrelation in the data.\n\nrswm_q &lt;- nb2listw(wm_q,\n                   style = 'W',\n                   zero.policy = TRUE)\nrswm_q\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 448 \nPercentage nonzero weights: 5.785124 \nAverage number of links: 5.090909 \n\nWeights style: W \nWeights constants summary:\n   n   nn S0       S1       S2\nW 88 7744 88 37.86334 365.9147\n\n\nThe input of nb2listw() must be an object of class nb. The syntax of the function has two major arguments:\n\nstyle: can take values ‘W’, ‘B’, ‘C’, ‘U’, ‘minmax’ and ‘S’. B is the classic binary coding, W is row standardized (sums over all links to n), C is globally standardized (sums over all links to n), U is equal to C divided by the number of neighbours (sums over all links to unity), while S is the variance-stabilizing coding scheme proposed by Tiefelsdorf et al. 1999, p. 167-168 (sums over all links to n).\nzero policy: if set to TRUE, weights vectors of zero length are inserted for regions without neighbour in the neighbours list. These will in turn generate lag values of zero, equivalent to the sum of products of the zero row t(rep(0, length=length(neighbours))) %*% x, for arbitrary numerical vector x of length length(neighbours). The spatially lagged value of x for the zero-neighbour region will then be zero, which may (or may not) be a sensible choice.\n\n\n\nGlobal Spatial Autocorrelation: Moran’s I\n\nMoran’s I test\nmoran.test() in spdep can be used to perform Moran’s I statistical test\n\nmoran.test(hunan$GDPPC,\n           listw = rswm_q,\n           zero.policy = TRUE,\n           na.action = na.omit)\n\n\n    Moran I test under randomisation\n\ndata:  hunan$GDPPC  \nweights: rswm_q    \n\nMoran I statistic standard deviate = 4.7351, p-value = 1.095e-06\nalternative hypothesis: greater\nsample estimates:\nMoran I statistic       Expectation          Variance \n      0.300749970      -0.011494253       0.004348351 \n\n\nQuestion: What statistical conclusion can you draw from the output above?\nAnswer: As the p-value is below the alpha level of 5%, the result of the Moran’s I test is statistically significant and since the Moran I statistics is positive, we can conclude that there is positive spatial autocorrelation, or that similar values are spatially clustered.\n\n\nMonte Carlo Moran’s I\nmoran.mc() can be used to performs permutation test for Moran’s I statistic. A total of 1000 simulation will be performed.\n\nset.seed(1234)\nbperm &lt;- moran.mc(hunan$GDPPC,\n                 listw = rswm_q,\n                 nsim = 999,\n                 zero.policy = TRUE,\n                 na.action = na.omit)\nbperm\n\n\n    Monte-Carlo simulation of Moran I\n\ndata:  hunan$GDPPC \nweights: rswm_q  \nnumber of simulations + 1: 1000 \n\nstatistic = 0.30075, observed rank = 1000, p-value = 0.001\nalternative hypothesis: greater\n\n\nQuestion: What statistical conclusion can you draw from the output above?\nAnswer: The permutation test supports the result of the Moran’s I. As the p-value is 0.001, only 0.1% of the values equal or exceed it, the result of the Moran’s I test is statistically significant and since the Moran I statistics is positive, we can conclude that there is positive spatial autocorrelation, or that similar values are spatially clustered.\n\n\nVisualizing Monte Carlo Moran’s I\nIt is good practice to examine the simulated Moran’s I test statistics in greater detail. This can be achieved by plotting the distribution of the statistical values as a histogram.\nmean() can be used to get the mean of the simulated values of statistic.\n\nmean(bperm$res[1:999])\n\n[1] -0.01504572\n\n\nvar() can be used to get the variance of the simulated values of statistic.\n\nvar(bperm$res[1:999])\n\n[1] 0.004371574\n\n\nsummary() can be used to get the summary statistics of the simulated values of statistic.\n\nsummary(bperm$res[1:999])\n\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n-0.18339 -0.06168 -0.02125 -0.01505  0.02611  0.27593 \n\n\nhist() and abline() can be used to create a histogram of the simulated values of statistic of the Monte Carlo Moran’s I\n\nhist(bperm$res,\n     freq = TRUE,\n     breaks = 20,\n     xlab = \"Simulated Moran's I\")\nabline(v=0,\n       col='red')\n\n\n\n\nQuestion: What statistical observation can you draw from the output above?\nAnswer: It can be seen that that a very small number of values exceed or equal the value of I at 0.3, meaning that the autocorrelation is statistically significant. Additionally, since the simulated values of statistic is not normally distributed, it demonstrates the reliability of the permutation test to identify statistically significant autocorrelation.\n\n\n\nGlobal Spatial Autocorrelation: Geary’s\n\nGeary’s C test\ngeary.test() can be used to perform Geary’s C test for spatial autocorrelation.\n\ngeary.test(hunan$GDPPC, listw = rswm_q)\n\n\n    Geary C test under randomisation\n\ndata:  hunan$GDPPC \nweights: rswm_q \n\nGeary C statistic standard deviate = 3.6108, p-value = 0.0001526\nalternative hypothesis: Expectation greater than statistic\nsample estimates:\nGeary C statistic       Expectation          Variance \n        0.6907223         1.0000000         0.0073364 \n\n\nQuestion: What statistical conclusion can you draw from the output above?\nAnswer: Geary’s C value ranges from 0 to 2 where 1 is no spatial autocorrelation. Since the statistic is 0.69, it suggests that there is slight positive spatial correlation. Additionally since the p-value is very small, the result is statistically significant.\n\n\nComputing Monte Carlo Geary’s C\nA permutation test (Monte Carlo Geary’s C) can be performed using geary.mc()\n\nset.seed(1234)\nbperm &lt;- geary.mc(hunan$GDPPC,\n                  listw = rswm_q,\n                  nsim = 999)\nbperm\n\n\n    Monte-Carlo simulation of Geary C\n\ndata:  hunan$GDPPC \nweights: rswm_q \nnumber of simulations + 1: 1000 \n\nstatistic = 0.69072, observed rank = 1, p-value = 0.001\nalternative hypothesis: greater\n\n\nQuestion: What statistical conclusion can you draw from the output above?\nAnswer: The permutation test supports the result of the Geary’s C test. Since p-value is 0.001, the result is statistically significant. Furthermore, as the test statistic is 0.69, it can be concluded that there is positive spatial autocorrelation.\n\n\nVisualizing the Monte Carlo Geary’s C\nmean() can be used to get the mean of the simulated values of statistic.\n\nmean(bperm$res[1:999])\n\n[1] 1.004402\n\n\nvar() can be used to get the variance of the simulated values of statistic.\n\nvar(bperm$res[1:999])\n\n[1] 0.007436493\n\n\nsummary() can be used to get the summary statistic of the simulated values of statistic.\n\nsummary(bperm$res[1:999])\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n 0.7142  0.9502  1.0052  1.0044  1.0595  1.2722 \n\n\nhist() and abline() can be used to create a histogram of the simulated values of statistic of the Geary’s C.\n\nhist(bperm$res,\n     freq = TRUE,\n     breaks = 20,\n     xlab = 'Simulated Geary C')\nabline(v=1, col='red')\n\n\n\n\nQuestion: What statistical observation can you draw from the output?\nAnswer: The simulated values is normally distributed around 1, which is one of the implicit assumption of the Geary’s C test."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex2/Hands-on_Ex2_Global_Measures.html#spatial-correlogram",
    "href": "Hands-on_Ex/Hands-on_Ex2/Hands-on_Ex2_Global_Measures.html#spatial-correlogram",
    "title": "Hands-on_Ex2_Global_Measures",
    "section": "Spatial Correlogram",
    "text": "Spatial Correlogram\nSpatial correlograms are great to examine patterns of spatial autocorrelation in the data or model residuals.\nThey show how correlated are pairs of spatial observations when you increase the distance (lag) between them. They are plots of some index of autocorrelation (Moran’s I or Geary’s C) against distance.\nAlthough correlograms are not as fundamental as variograms (a keystone concept of geostatistic), they are very useful as an exploratory and descriptive tool. For this purpose, they actually provide richer information than variograms.\n\nCompute Moran’s I Correlogram\nsp.correlogram() can be used to compute a 6-lag spatial correlogram of GDPPC. The global spatial autocorrelation used is Moran’s I. plot() is then used to plot the output.\n\nMI_corr &lt;- sp.correlogram(wm_q,\n                          hunan$GDPPC,\n                          order = 6,\n                          method = 'I', style = 'W')\n\nplot(MI_corr)\n\n\n\n\nPlotting the output might not allow us to provide complete interpretation. This is because not all autocorrelation values are statistically significant. Hence, it is important for us to examine the full analysis report by printing out the analysis results as in the code chunk below.\n\nprint(MI_corr)\n\nSpatial correlogram for hunan$GDPPC \nmethod: Moran's I\n         estimate expectation   variance standard deviate Pr(I) two sided    \n1 (88)  0.3007500  -0.0114943  0.0043484           4.7351       2.189e-06 ***\n2 (88)  0.2060084  -0.0114943  0.0020962           4.7505       2.029e-06 ***\n3 (88)  0.0668273  -0.0114943  0.0014602           2.0496        0.040400 *  \n4 (88)  0.0299470  -0.0114943  0.0011717           1.2107        0.226015    \n5 (88) -0.1530471  -0.0114943  0.0012440          -4.0134       5.984e-05 ***\n6 (88) -0.1187070  -0.0114943  0.0016791          -2.6164        0.008886 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nQuestion: What statistical observation can you draw from the plot above?\nAnswer: All pairs of results are statistically significant, except for number 4 with a p-value larger than 0.05. This shows that the list of IDs in number 4 do not exhibit spatial autocorrelation with their neighbors.\n\n\nCompute Geary’s C correlogram and plot\nsp.correlogram() can be used to compute a 6-lag spatial correlogram of GDPPC. The global spatial autocorrelation used is Geary’s C. plot() is then used to plot the output.\n\nGC_corr &lt;- sp.correlogram(wm_q,\n                          hunan$GDPPC,\n                          order = 6,\n                          method = 'C', style = 'W')\nplot(GC_corr)\n\n\n\n\nWe will print out the analysis report using print().\n\nprint(GC_corr)\n\nSpatial correlogram for hunan$GDPPC \nmethod: Geary's C\n        estimate expectation  variance standard deviate Pr(I) two sided    \n1 (88) 0.6907223   1.0000000 0.0073364          -3.6108       0.0003052 ***\n2 (88) 0.7630197   1.0000000 0.0049126          -3.3811       0.0007220 ***\n3 (88) 0.9397299   1.0000000 0.0049005          -0.8610       0.3892612    \n4 (88) 1.0098462   1.0000000 0.0039631           0.1564       0.8757128    \n5 (88) 1.2008204   1.0000000 0.0035568           3.3673       0.0007592 ***\n6 (88) 1.0773386   1.0000000 0.0058042           1.0151       0.3100407    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex1/Hands-on_Ex1_Choropleth.html",
    "href": "Hands-on_Ex/Hands-on_Ex1/Hands-on_Ex1_Choropleth.html",
    "title": "Hands-on Exercise 1: Choropleth Mapping with R",
    "section": "",
    "text": "Choropleth mapping involves the symbolisation of enumeration units, such as countries, provinces, states, counties or census units, using area patterns or graduated colors. For example, a social scientist may need to use a choropleth map to portray the spatial distribution of aged population of Singapore by Master Plan 2014 Subzone Boundary.\nThis can be done using the tmap package. We can load this and other required packages (sf, tidyverse) using the code below.\n\npacman::p_load(sf, tidyverse, tmap)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex1/Hands-on_Ex1_Choropleth.html#overview",
    "href": "Hands-on_Ex/Hands-on_Ex1/Hands-on_Ex1_Choropleth.html#overview",
    "title": "Hands-on Exercise 1: Choropleth Mapping with R",
    "section": "",
    "text": "Choropleth mapping involves the symbolisation of enumeration units, such as countries, provinces, states, counties or census units, using area patterns or graduated colors. For example, a social scientist may need to use a choropleth map to portray the spatial distribution of aged population of Singapore by Master Plan 2014 Subzone Boundary.\nThis can be done using the tmap package. We can load this and other required packages (sf, tidyverse) using the code below.\n\npacman::p_load(sf, tidyverse, tmap)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex1/Hands-on_Ex1_Choropleth.html#importing-data-into-r",
    "href": "Hands-on_Ex/Hands-on_Ex1/Hands-on_Ex1_Choropleth.html#importing-data-into-r",
    "title": "Hands-on Exercise 1: Choropleth Mapping with R",
    "section": "Importing Data into R",
    "text": "Importing Data into R\n\nThe Data\nTwo datasets will be used:\n\nMaster Plan 2014 Subzone Boundary (Web) in ESRI shapefile format. It consists of geographical boundary of Singapore at the planning subzone level and is babsed on the URA Master Plan 2014.\nSingapore Residents by Planning Area/Subzone, Age Grouu, Sex, and Type of Dwelling, June 2011-2020 csv format. This is aspatial data. Its PA and SZ fields can be used to geocode to the Master Plan 2014 Subzone Boundary (Web) shapefile.\n\n\n\nImporting Geospatial Data into R\nst_read() can be used to read the Master Plan 2014 shapefile into an R dataframe.\n\nmpsz &lt;- st_read(dsn = 'data/geospatial',\n                layer = 'MP14_SUBZONE_WEB_PL')\n\nReading layer `MP14_SUBZONE_WEB_PL' from data source \n  `D:\\phlong2023\\ISSS624\\Hands-on_Ex\\Hands-on_Ex1\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\n\n\nglimpse() and head() can be used to look at the data types and first few rows of data\n\nglimpse(mpsz)\n\nRows: 323\nColumns: 16\n$ OBJECTID   &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, …\n$ SUBZONE_NO &lt;int&gt; 1, 1, 3, 8, 3, 7, 9, 2, 13, 7, 12, 6, 1, 5, 1, 1, 3, 2, 2, …\n$ SUBZONE_N  &lt;chr&gt; \"MARINA SOUTH\", \"PEARL'S HILL\", \"BOAT QUAY\", \"HENDERSON HIL…\n$ SUBZONE_C  &lt;chr&gt; \"MSSZ01\", \"OTSZ01\", \"SRSZ03\", \"BMSZ08\", \"BMSZ03\", \"BMSZ07\",…\n$ CA_IND     &lt;chr&gt; \"Y\", \"Y\", \"Y\", \"N\", \"N\", \"N\", \"N\", \"Y\", \"N\", \"N\", \"N\", \"N\",…\n$ PLN_AREA_N &lt;chr&gt; \"MARINA SOUTH\", \"OUTRAM\", \"SINGAPORE RIVER\", \"BUKIT MERAH\",…\n$ PLN_AREA_C &lt;chr&gt; \"MS\", \"OT\", \"SR\", \"BM\", \"BM\", \"BM\", \"BM\", \"SR\", \"QT\", \"QT\",…\n$ REGION_N   &lt;chr&gt; \"CENTRAL REGION\", \"CENTRAL REGION\", \"CENTRAL REGION\", \"CENT…\n$ REGION_C   &lt;chr&gt; \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\",…\n$ INC_CRC    &lt;chr&gt; \"5ED7EB253F99252E\", \"8C7149B9EB32EEFC\", \"C35FEFF02B13E0E5\",…\n$ FMEL_UPD_D &lt;date&gt; 2014-12-05, 2014-12-05, 2014-12-05, 2014-12-05, 2014-12-05…\n$ X_ADDR     &lt;dbl&gt; 31595.84, 28679.06, 29654.96, 26782.83, 26201.96, 25358.82,…\n$ Y_ADDR     &lt;dbl&gt; 29220.19, 29782.05, 29974.66, 29933.77, 30005.70, 29991.38,…\n$ SHAPE_Leng &lt;dbl&gt; 5267.381, 3506.107, 1740.926, 3313.625, 2825.594, 4428.913,…\n$ SHAPE_Area &lt;dbl&gt; 1630379.27, 559816.25, 160807.50, 595428.89, 387429.44, 103…\n$ geometry   &lt;MULTIPOLYGON [m]&gt; MULTIPOLYGON (((31495.56 30..., MULTIPOLYGON (…\n\n\n\nhead(mpsz, 5)\n\nSimple feature collection with 5 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 25867.68 ymin: 28369.47 xmax: 32362.39 ymax: 30435.54\nProjected CRS: SVY21\n  OBJECTID SUBZONE_NO      SUBZONE_N SUBZONE_C CA_IND      PLN_AREA_N\n1        1          1   MARINA SOUTH    MSSZ01      Y    MARINA SOUTH\n2        2          1   PEARL'S HILL    OTSZ01      Y          OUTRAM\n3        3          3      BOAT QUAY    SRSZ03      Y SINGAPORE RIVER\n4        4          8 HENDERSON HILL    BMSZ08      N     BUKIT MERAH\n5        5          3        REDHILL    BMSZ03      N     BUKIT MERAH\n  PLN_AREA_C       REGION_N REGION_C          INC_CRC FMEL_UPD_D   X_ADDR\n1         MS CENTRAL REGION       CR 5ED7EB253F99252E 2014-12-05 31595.84\n2         OT CENTRAL REGION       CR 8C7149B9EB32EEFC 2014-12-05 28679.06\n3         SR CENTRAL REGION       CR C35FEFF02B13E0E5 2014-12-05 29654.96\n4         BM CENTRAL REGION       CR 3775D82C5DDBEFBD 2014-12-05 26782.83\n5         BM CENTRAL REGION       CR 85D9ABEF0A40678F 2014-12-05 26201.96\n    Y_ADDR SHAPE_Leng SHAPE_Area                       geometry\n1 29220.19   5267.381  1630379.3 MULTIPOLYGON (((31495.56 30...\n2 29782.05   3506.107   559816.2 MULTIPOLYGON (((29092.28 30...\n3 29974.66   1740.926   160807.5 MULTIPOLYGON (((29932.33 29...\n4 29933.77   3313.625   595428.9 MULTIPOLYGON (((27131.28 30...\n5 30005.70   2825.594   387429.4 MULTIPOLYGON (((26451.03 30...\n\n\n\n\nImporting Attribute Data into R\nFor the resident population data, read_csv() will be used as it is stored as a csv\n\npopdata &lt;- read_csv('data/aspatial/respopagesexfa2011to2020.csv')"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex1/Hands-on_Ex1_Choropleth.html#data-preparation",
    "href": "Hands-on_Ex/Hands-on_Ex1/Hands-on_Ex1_Choropleth.html#data-preparation",
    "title": "Hands-on Exercise 1: Choropleth Mapping with R",
    "section": "Data Preparation",
    "text": "Data Preparation\nBefore a thematic map can be prepared, you are required to prepare a data table with year 2020 values. The data table should include the variables PA, SZ, YOUNG, ECONOMY ACTIVE, AGED, TOTAL, DEPENDENCY.\nThis table would have the rows be each unique PA and SZ and with the following new columns:\n\nYOUNG: number of people from age group 0-4 to age group 20-24\nECONOMY ACTIVE: number of people from age group 25-29 to age group 60-64\nAGED: number of people age group 65 +\nTOTAL: number of people in all age groups\nDEPENDENCY: the ratio between YOUNG + AGED against ECONOMY ACTIVE\n\n\nData Wrangling\nThe following data wrangling and transformation functions will be used:\n\npivot_wider(): To pivot the dataframe from long to wide format with rows becoming new columns\nmutate(), filter(), and group_by(): Creating new columns, filtering, and group columns based on value of some columns\n\n\npopdata2020 &lt;- popdata %&gt;%\n  filter(Time == 2020) %&gt;% #Getting only 2020 data\n  group_by(PA, SZ, AG) %&gt;%\n  summarise(`POP` = sum(`Pop`)) %&gt;% #Summarizing by population based on the group_by\n  ungroup() %&gt;%\n  pivot_wider(names_from = AG,\n              values_from = POP)%&gt;% #pivot wider based on names in AG and values from POP\n  mutate(YOUNG = rowSums(.[3:6])+rowSums(.[14])) %&gt;%\n  mutate(`ECONOMY ACTIVE` = rowSums(.[7:13])+rowSums(.[15]))%&gt;%\n  mutate(`AGED` = rowSums(.[16:21])) %&gt;%\n  mutate(`TOTAL` = rowSums(.[3:21])) %&gt;%\n  mutate(`DEPENDENCY` = (`YOUNG` + `AGED`)/`ECONOMY ACTIVE`) %&gt;%\n  select(`PA`, `SZ`, `YOUNG`, `ECONOMY ACTIVE`, `AGED`, `TOTAL`, `DEPENDENCY`)\n\n\n\nJoining the attribute data and geospatial data\nCurrently, the values of the PA and SZ fields are a mix of lower and uppercase characters while the values in SUBZONE_N and PLN_AREA_N are all uppercase.\nWe need to convert the values in PA and SZ fields to uppercase.\n\npopdata2020 &lt;- popdata2020 %&gt;%\n  mutate_at(.vars = vars(PA, SZ), #Apply the toupper function to multiple columns\n            .funs = list(toupper)) %&gt;% \n  filter(`ECONOMY ACTIVE` &gt; 0)\n\nleft_join() can then be used to join the geographical data and attribute table based on SZ being the same as SUBZONE_N. left_join() is used with the simple feature dataframe (mpsz) as the left data table to ensure the output will be a simple features dataframe; it will also keep all observations in mpsz.\n\nmpsz_pop2020 &lt;- left_join(mpsz, popdata2020,\n                          by = c('SUBZONE_N' = 'SZ'))\n\nNow, we can use write_rds to create a new rds (R Data Serialization) file with the new dataframe\n\nwrite_rds(mpsz_pop2020, 'data/rds/mpszpop2020.rds')"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex1/Hands-on_Ex1_Choropleth.html#choropleth-mapping-geospatial-data-using-tmap",
    "href": "Hands-on_Ex/Hands-on_Ex1/Hands-on_Ex1_Choropleth.html#choropleth-mapping-geospatial-data-using-tmap",
    "title": "Hands-on Exercise 1: Choropleth Mapping with R",
    "section": "Choropleth Mapping Geospatial Data using tmap",
    "text": "Choropleth Mapping Geospatial Data using tmap\n\nPlotting a choropleth map quickly by using qtm()\nDefault visualization using qtm(). Note that tmap_mode() with “plot” is used to produce a static map. For interactive mode, “view” should be used.\n\ntmap_mode('plot')\nqtm(mpsz_pop2020,\n    fill = 'DEPENDENCY') #the DEPENDENCY column will be used for the color variation\n\n\n\n\n\n\nCreating a choropleth map by using tmap’s elements\nDespite its usefulness of drawing a choropleth map quickly and easily, the disadvantge of qtm() is that it makes aesthetics of individual layers harder to control. To draw a high quality cartographic choropleth map as shown in the figure below, tmap’s drawing elements should be used.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill('DEPENDENCY',\n          style='quantile',\n          palette = 'Blues',\n          title = 'Dependency ration')+\n  tm_layout(main.title = 'Distribution of Dependency Ratio by planning subzone',\n            main.title.position = 'center',\n            main.title.size = 1.2,\n            legend.height = 0.45,\n            legend.width = 0.35,\n            frame = TRUE)+\n  tm_borders(alpha = 0.5)+\n  tm_compass(type='8star',size=2)+\n  tm_scale_bar()+\n  tm_grid(alpha = 0.2)+\n  tm_credits('Sourrce: Planning Sub-zone boundary from Urban Redevelopment Authority \\n and Population data from Department of Statistics (DOS)',\n             position = c('left','bottom'))\n\n\n\n\nThe following sections will explain each step of the process executed in the code chunk above\n\n\nDrawing a base map\nThe basic building block of tmap is tm_shape() which is used to define the input data and tm_polygons() which is used to draw the planning subzone polygons.\n\ntm_shape(mpsz_pop2020)+\n  tm_polygons()\n\n\n\n\n\n\nDrawing a choropleth map using tm_polygons()\ntm_polygons() can be modified with the target variable in order to draw the choropleth map showing the geographical distribution of the selected variable.\n\ntm_shape(mpsz_pop2020)+\n  tm_polygons('DEPENDENCY')\n\n\n\n\nThings to learn from tm_polygons():\n\nThe default interval binning used to draw the choropleth map is called “pretty”.\nThe default colour scheme used is YlOrRd of ColorBrewer.\nBy default, Missing value will be shaded in grey.\n\n\n\nDrawing a choropleth map using tm_fill() and tm_border()\ntm_polygons() is a wrapper of tm_fill() and tm_border():\n\ntm_fill() shades the polygons by using the default colour scheme\ntm_borders() adds the borders of the shapefile onto the choropleth map\n\nIf you use tm_fill() alone, there will be no border between the subzones. The planning subzones are shared according to the respective dependency values.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill('DEPENDENCY')\n\n\n\n\ntm_borders() can be used to add the boundary of the planning subzones. tm_borders() has three arguments:\n\nalpha: transparency of the line\ncol: border colour\nlwd: line width\nlty: line type\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill('DEPENDENCY')+\n  tm_borders(lwd = 0.1, alpha = 1)\n\n\n\n\n\n\nData classification methods of tmap\nMost choropleth maps employ some methods of data classification. The point of classification is to take a large number of observations and group them into data ranges or classes.\ntmap provides a total ten data classification methods, namely: fixed, sd, equal, pretty (default), quantile, kmeans, hclust, bclust, fisher, and jenks.\nTo define a data classification method, the style argument of tm_fill() or tm_polygons() will be used.\n\nPlotting choropleth maps with built-in classification methods\njenks data classification method\n\ntm_shape(mpsz_pop2020)+\n  tm_fill('DEPENDENCY',\n          n = 5, #number of classes\n          style = 'jenks')+\n  tm_borders(alpha = 0.5)\n\n\n\n\nequal data classification method\n\ntm_shape(mpsz_pop2020)+\n  tm_fill('DEPENDENCY',\n          n = 5,\n          style = 'equal')+\n  tm_borders(alpha = 0.5)\n\n\n\n\nquantile data classification method\n\ntm_shape(mpsz_pop2020)+\n  tm_fill('DEPENDENCY',\n          n = 5,\n          style = 'quantile')+\n  tm_borders(alpha = 0.5)\n\n\n\n\nThe distribution of quantile data classification method are more evenly distributed then equal data classification method.\nUsing the quantile style with different numbers of classes\n2 classes\n\ntm_shape(mpsz_pop2020)+\n  tm_fill('DEPENDENCY',\n          n = 2,\n          style = 'quantile')+\n  tm_borders(alpha = 0.5)\n\n\n\n\n6 classes\n\ntm_shape(mpsz_pop2020)+\n  tm_fill('DEPENDENCY',\n          n = 6,\n          style = 'quantile')+\n  tm_borders(alpha = 0.5)\n\n\n\n\n10 classes\n\ntm_shape(mpsz_pop2020)+\n  tm_fill('DEPENDENCY',\n          n = 10,\n          style = 'quantile')+\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\nPlotting choropleth map with custom break\nFor all the built-in styles, the category breaks are computed internally. In order to override these defaults, the breakpoints can be set explicitly by means of the breaks argument to the tm_fill(). It is important to note that, in tmap the breaks include a minimum and maximum. As a result, in order to end up with n categories, n+1 elements must be specified in the breaks option (the values must be in increasing order).\nsummary() can be used to get some descriptive statistics on the variable ‘DEPENDENCY’ before setting break points.\n\nsummary(mpsz_pop2020$DEPENDENCY)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n 0.0000  0.6540  0.7063  0.7712  0.7657 19.0000      92 \n\n\nWith reference to the results above and the need to include a minimum and maximum (0 and 100), we can set our breaks with the vector c(0, 0.5, 0.6, 0.7, 0.8, 1.00)\nNow we can plot the choropleth map with custom breaks\n\ntm_shape(mpsz_pop2020)+\n  tm_fill('DEPENDENCY',\n          breaks = c(0, 0.5, 0.6, 0.7, 0.8, 1.00))+\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\nColour Scheme\ntmap supports colour ramps either defined by the user or a set of predefined colour ramps from the RColorBrewer package\n\nUsing ColourBrewer palette\nTo change the colour, we assigned the preferred colour to the palette argument of tm_fill()\n\ntm_shape(mpsz_pop2020)+\n  tm_fill('DEPENDENCY',\n          n = 6,\n          style = 'quantile',\n          palette = 'Blues')+\n  tm_borders(alpha = 0.5)\n\n\n\n\nWe can also reverse the color scheme (darker for lower values) by adding a ‘-’ prefix to the palette argument\n\ntm_shape(mpsz_pop2020)+\n  tm_fill('DEPENDENCY',\n          n = 6,\n          style = 'quantile',\n          palette = '-Blues')+\n  tm_borders(alpha = 0.5)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex1/Hands-on_Ex1_Choropleth.html#map-layouts",
    "href": "Hands-on_Ex/Hands-on_Ex1/Hands-on_Ex1_Choropleth.html#map-layouts",
    "title": "Hands-on Exercise 1: Choropleth Mapping with R",
    "section": "Map Layouts",
    "text": "Map Layouts\nMap layout refers to the combination of all map elements into a cohensive map. Map elements include among others the objects to be mapped, the title, the scale bar, the compass, margins and aspects ratios. Colour settings and data classification methods covered in the previous section relate to the palette and break-points are used to affect how the map looks.\n\nMap Legend\nIn tmap, several legend options are provided to change the placement, format and appearance of the legend.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill('DEPENDENCY',\n          style = 'jenks',\n          palette = 'Blues',\n          legend.hist = TRUE,\n          legend.is.portrait = TRUE,\n          legend.hist.z = 0.1)+\n  tm_layout(main.title = 'Distribution of Dependency Ratio by planning subzone \\n (Jenks Classification)',\n            main.title.position = 'center',\n            main.title.size = 1,\n            legend.height = 0.45,\n            legend.width = 0.35,\n            legend.outside = FALSE,\n            legend.position = c('right','bottom'),\n            frame = FALSE)+\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\nMap Style\ntmap allows a wide variety of layout settings to be changes. They can be called by using tmap_style()\n\ntm_shape(mpsz_pop2020)+\n  tm_fill('DEPENDENCY',\n          style = 'quantile',\n          palette = '-Greens')+\n  tm_borders(alpha = 0.5)+\n  tmap_style('classic')\n\n\n\n\n\n\nCartographic Furniture\nBeside map style, tmap also also provides arguments to draw other map furniture such as compass, scale bar and grid lines.\ntm_compass() can be used to add a compass.\ntm_scale_bar() can be used to add a scale bar.\ntm_grid() can be used to add grid lines.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill('DEPENDENCY',\n          style = 'quantile',\n          palette = 'Blues',\n          title = 'No. of persons')+\n  tm_layout(main.title = 'Distribution of Dependency Ratio by planning subzone \\n (Jenks Classification)',\n            main.title.position = 'center',\n            main.title.size = 1.2,\n            legend.height = 0.45,\n            legend.width = 0.35,\n            frame = TRUE)+\n  tm_borders(alpha = 0.5)+\n  tm_compass(type = '8star', size = 2)+\n  tm_scale_bar(width = 0.15)+\n  tm_grid(lwd = 0.1, alpha = 0.2)+\n  tm_credits('Source: Planning Sub-zone boundary from Urban Redevelopment Authority (URA) \\n and Population data from Department of Statistic (DOS)',\n             position = c('left','bottom'))\n\n\n\n\nTo reset to the default style, use tmap_style(‘white’)\n\ntmap_style('white')"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex1/Hands-on_Ex1_Choropleth.html#drawing-small-multiple-choropleth-maps",
    "href": "Hands-on_Ex/Hands-on_Ex1/Hands-on_Ex1_Choropleth.html#drawing-small-multiple-choropleth-maps",
    "title": "Hands-on Exercise 1: Choropleth Mapping with R",
    "section": "Drawing Small Multiple Choropleth Maps",
    "text": "Drawing Small Multiple Choropleth Maps\nSmall multiple maps, also referred to as facet maps, are composed of many maps arrange side-by-side, and sometimes stacked vertically. Small multiple maps enable the visualisation of how spatial relationships change with respect to another variable, such as time.\nIn tmap, small multiple maps can be plotted in three ways:\n\nby assigning multiple values to at least one of the aesthetic arguments\nby defining a group-by variable in tm_facets()\nby creating multiple standalone maps with tmap_arrange()\n\n\nBy assigning multiple values to at least one of the aesthetic arguments\nThe ncols argument in tm_fill() can be used to make multiple choropleth maps\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(c('YOUNG','AGED'),\n          style = 'equal',\n          palette = 'Blues')+\n  tm_layout(legend.position = c('right','bottom'))+\n  tm_borders(alpha = 0.5)+\n  tmap_style('white')\n\n\n\n\n\ntm_shape(mpsz_pop2020)+\n  tm_polygons(c('DEPENDENCY','AGED'),\n              style = c('equal','quantile'),\n              palette = list('Blues','Greens'))+\n  tm_layout(legend.position = c('right','bottom'))\n\n\n\n\n\n\nBy defining a group-by variable in tm_facets()\n\ntm_shape(mpsz_pop2020)+\n  tm_fill('DEPENDENCY',\n          style = 'quantile',\n          palette = 'Blues',\n          thres.poly = 0)+\n  tm_facets(by = 'REGION_N',\n            free.coords = TRUE,\n            drop.shapes = TRUE)+\n  tm_layout(legend.show = FALSE,\n            title.position = c('center','center'),\n            title.size = 20)+\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\nBy creating multiple stand-alone maps with tmap_arrange()\n\nyoungmap &lt;- tm_shape(mpsz_pop2020)+\n  tm_polygons('YOUNG',\n              style = 'quantile',\n              palette = 'Blues')\n\nagedmap &lt;- tm_shape(mpsz_pop2020)+\n  tm_polygons('AGED',\n              style = 'quantile',\n              palette = 'Blues')\n\ntmap_arrange(youngmap, agedmap, asp = 1, ncol = 2)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex1/Hands-on_Ex1_Choropleth.html#mapping-spatial-object-meeting-a-selection-criterion",
    "href": "Hands-on_Ex/Hands-on_Ex1/Hands-on_Ex1_Choropleth.html#mapping-spatial-object-meeting-a-selection-criterion",
    "title": "Hands-on Exercise 1: Choropleth Mapping with R",
    "section": "Mapping Spatial Object Meeting a Selection Criterion",
    "text": "Mapping Spatial Object Meeting a Selection Criterion\nSelection function can be used to map spatial objects meeting the selection criterion\n\ntm_shape(mpsz_pop2020[mpsz_pop2020$REGION_N=='CENTRAL REGION',])+\n  tm_fill('DEPENDENCY',\n          style = 'quantile',\n          palette = 'Blues',\n          legend.hist = TRUE,\n          legend.is.portrait = TRUE,\n          legend.hist.z = 0.1)+\n  tm_layout(legend.outside = TRUE,\n            legend.height = 0.45,\n            legend.width = 5.0,\n            legend.position = c('right','bottom'),\n            frame = FALSE)+\n  tm_borders(alpha = 0.5)"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\nThis was made by Phan Hoang Long for ISSS624."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex1/Hands-on_Ex1.html",
    "href": "Hands-on_Ex/Hands-on_Ex1/Hands-on_Ex1.html",
    "title": "Hands-on Exercise 1: Geospatial Data Wrangling with R",
    "section": "",
    "text": "In this hands-on exercise, I learn how to import and wrangle geospatial data using appropriate R packages."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex1/Hands-on_Ex1.html#overview",
    "href": "Hands-on_Ex/Hands-on_Ex1/Hands-on_Ex1.html#overview",
    "title": "Hands-on Exercise 1: Geospatial Data Wrangling with R",
    "section": "",
    "text": "In this hands-on exercise, I learn how to import and wrangle geospatial data using appropriate R packages."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex1/Hands-on_Ex1.html#getting-started",
    "href": "Hands-on_Ex/Hands-on_Ex1/Hands-on_Ex1.html#getting-started",
    "title": "Hands-on Exercise 1: Geospatial Data Wrangling with R",
    "section": "Getting Started",
    "text": "Getting Started\nThe code chunk below installs and loads sf and tidyverse packages into R environment.\n\npacman::p_load(sf, tidyverse)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex1/Hands-on_Ex1.html#importing-geospatial-data",
    "href": "Hands-on_Ex/Hands-on_Ex1/Hands-on_Ex1.html#importing-geospatial-data",
    "title": "Hands-on Exercise 1: Geospatial Data Wrangling with R",
    "section": "Importing Geospatial Data",
    "text": "Importing Geospatial Data\n\nImporting polygon features data\nReading the Master Planning 2014 Subzone shapefile into a dataframe\n\nmpsz &lt;- st_read(dsn = \"data/geospatial\", layer = \"MP14_SUBZONE_WEB_PL\")\n\nReading layer `MP14_SUBZONE_WEB_PL' from data source \n  `D:\\phlong2023\\ISSS624\\Hands-on_Ex\\Hands-on_Ex1\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\n\n\nReading the CyclingPath shapefile into a dataframe\n\ncyclingpath &lt;- st_read(dsn = \"data/geospatial\",layer = 'CyclingPathGazette')\n\nReading layer `CyclingPathGazette' from data source \n  `D:\\phlong2023\\ISSS624\\Hands-on_Ex\\Hands-on_Ex1\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 2558 features and 2 fields\nGeometry type: MULTILINESTRING\nDimension:     XY\nBounding box:  xmin: 11854.32 ymin: 28347.98 xmax: 42626.09 ymax: 48948.15\nProjected CRS: SVY21\n\n\nRead the Pre-School Locations kml file into a dataframe using a complete path\n\npreschool &lt;- st_read('data/geospatial/PreSchoolsLocation.kml')\n\nReading layer `PRESCHOOLS_LOCATION' from data source \n  `D:\\phlong2023\\ISSS624\\Hands-on_Ex\\Hands-on_Ex1\\data\\geospatial\\PreSchoolsLocation.kml' \n  using driver `KML'\nSimple feature collection with 2290 features and 2 fields\nGeometry type: POINT\nDimension:     XYZ\nBounding box:  xmin: 103.6878 ymin: 1.247759 xmax: 103.9897 ymax: 1.462134\nz_range:       zmin: 0 zmax: 0\nGeodetic CRS:  WGS 84"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex1/Hands-on_Ex1.html#checking-the-content-of-a-simple-feature-dataframe",
    "href": "Hands-on_Ex/Hands-on_Ex1/Hands-on_Ex1.html#checking-the-content-of-a-simple-feature-dataframe",
    "title": "Hands-on Exercise 1: Geospatial Data Wrangling with R",
    "section": "Checking the Content of a Simple Feature DataFrame",
    "text": "Checking the Content of a Simple Feature DataFrame\n\nWorking with st_geometry()\nUsing st_geometry() to retrieve basic information of the dataframe\n\nst_geometry(mpsz)\n\nGeometry set for 323 features \nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\nFirst 5 geometries:\n\n\nMULTIPOLYGON (((31495.56 30140.01, 31980.96 296...\n\n\nMULTIPOLYGON (((29092.28 30021.89, 29119.64 300...\n\n\nMULTIPOLYGON (((29932.33 29879.12, 29947.32 298...\n\n\nMULTIPOLYGON (((27131.28 30059.73, 27088.33 297...\n\n\nMULTIPOLYGON (((26451.03 30396.46, 26440.47 303...\n\n\n\n\nWorking with glimpse()\nUse glimpse() to get the data types of each column and some of their values\n\nglimpse(mpsz)\n\nRows: 323\nColumns: 16\n$ OBJECTID   &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, …\n$ SUBZONE_NO &lt;int&gt; 1, 1, 3, 8, 3, 7, 9, 2, 13, 7, 12, 6, 1, 5, 1, 1, 3, 2, 2, …\n$ SUBZONE_N  &lt;chr&gt; \"MARINA SOUTH\", \"PEARL'S HILL\", \"BOAT QUAY\", \"HENDERSON HIL…\n$ SUBZONE_C  &lt;chr&gt; \"MSSZ01\", \"OTSZ01\", \"SRSZ03\", \"BMSZ08\", \"BMSZ03\", \"BMSZ07\",…\n$ CA_IND     &lt;chr&gt; \"Y\", \"Y\", \"Y\", \"N\", \"N\", \"N\", \"N\", \"Y\", \"N\", \"N\", \"N\", \"N\",…\n$ PLN_AREA_N &lt;chr&gt; \"MARINA SOUTH\", \"OUTRAM\", \"SINGAPORE RIVER\", \"BUKIT MERAH\",…\n$ PLN_AREA_C &lt;chr&gt; \"MS\", \"OT\", \"SR\", \"BM\", \"BM\", \"BM\", \"BM\", \"SR\", \"QT\", \"QT\",…\n$ REGION_N   &lt;chr&gt; \"CENTRAL REGION\", \"CENTRAL REGION\", \"CENTRAL REGION\", \"CENT…\n$ REGION_C   &lt;chr&gt; \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\",…\n$ INC_CRC    &lt;chr&gt; \"5ED7EB253F99252E\", \"8C7149B9EB32EEFC\", \"C35FEFF02B13E0E5\",…\n$ FMEL_UPD_D &lt;date&gt; 2014-12-05, 2014-12-05, 2014-12-05, 2014-12-05, 2014-12-05…\n$ X_ADDR     &lt;dbl&gt; 31595.84, 28679.06, 29654.96, 26782.83, 26201.96, 25358.82,…\n$ Y_ADDR     &lt;dbl&gt; 29220.19, 29782.05, 29974.66, 29933.77, 30005.70, 29991.38,…\n$ SHAPE_Leng &lt;dbl&gt; 5267.381, 3506.107, 1740.926, 3313.625, 2825.594, 4428.913,…\n$ SHAPE_Area &lt;dbl&gt; 1630379.27, 559816.25, 160807.50, 595428.89, 387429.44, 103…\n$ geometry   &lt;MULTIPOLYGON [m]&gt; MULTIPOLYGON (((31495.56 30..., MULTIPOLYGON (…\n\n\n\n\nWorking with head()\nhead() lets us inspect the top n rows of the dataframe\n\nhead(mpsz, n= 5)\n\nSimple feature collection with 5 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 25867.68 ymin: 28369.47 xmax: 32362.39 ymax: 30435.54\nProjected CRS: SVY21\n  OBJECTID SUBZONE_NO      SUBZONE_N SUBZONE_C CA_IND      PLN_AREA_N\n1        1          1   MARINA SOUTH    MSSZ01      Y    MARINA SOUTH\n2        2          1   PEARL'S HILL    OTSZ01      Y          OUTRAM\n3        3          3      BOAT QUAY    SRSZ03      Y SINGAPORE RIVER\n4        4          8 HENDERSON HILL    BMSZ08      N     BUKIT MERAH\n5        5          3        REDHILL    BMSZ03      N     BUKIT MERAH\n  PLN_AREA_C       REGION_N REGION_C          INC_CRC FMEL_UPD_D   X_ADDR\n1         MS CENTRAL REGION       CR 5ED7EB253F99252E 2014-12-05 31595.84\n2         OT CENTRAL REGION       CR 8C7149B9EB32EEFC 2014-12-05 28679.06\n3         SR CENTRAL REGION       CR C35FEFF02B13E0E5 2014-12-05 29654.96\n4         BM CENTRAL REGION       CR 3775D82C5DDBEFBD 2014-12-05 26782.83\n5         BM CENTRAL REGION       CR 85D9ABEF0A40678F 2014-12-05 26201.96\n    Y_ADDR SHAPE_Leng SHAPE_Area                       geometry\n1 29220.19   5267.381  1630379.3 MULTIPOLYGON (((31495.56 30...\n2 29782.05   3506.107   559816.2 MULTIPOLYGON (((29092.28 30...\n3 29974.66   1740.926   160807.5 MULTIPOLYGON (((29932.33 29...\n4 29933.77   3313.625   595428.9 MULTIPOLYGON (((27131.28 30...\n5 30005.70   2825.594   387429.4 MULTIPOLYGON (((26451.03 30..."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex1/Hands-on_Ex1.html#plotting-geospatial-data",
    "href": "Hands-on_Ex/Hands-on_Ex1/Hands-on_Ex1.html#plotting-geospatial-data",
    "title": "Hands-on Exercise 1: Geospatial Data Wrangling with R",
    "section": "Plotting Geospatial Data",
    "text": "Plotting Geospatial Data\nThe default plot of an sf object is a multi-plot of all attributes, up to a reasonable maximum. This can be seen using the plot() function.\n\nplot(mpsz)\n\nWarning: plotting the first 9 out of 15 attributes; use max.plot = 15 to plot\nall\n\n\n\n\n\nWe can choose to plot only the geometry (outline) by using st_geometry()\n\nplot(st_geometry(mpsz))\n\n\n\n\nWe can also choose the specific attribute of the dataframe we would like to plot by addressing it in the R dataframe\n\nplot(mpsz['PLN_AREA_N'])"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex1/Hands-on_Ex1.html#working-with-projection",
    "href": "Hands-on_Ex/Hands-on_Ex1/Hands-on_Ex1.html#working-with-projection",
    "title": "Hands-on Exercise 1: Geospatial Data Wrangling with R",
    "section": "Working with Projection",
    "text": "Working with Projection\nMap projection is an important property of a geospatial data. In order to perform geoprocessing using two geospatial data, we need to ensure that both geospatial data are projected using similar coordinate system.\nThe process of projecting one dataframe from one coordinate system to another is called projection transformation.\n\nAssigning EPSG code to a simple feature data frame\nIdentifying the coordinate system of a dataframe using st_crs()\n\nst_crs(mpsz)\n\nCoordinate Reference System:\n  User input: SVY21 \n  wkt:\nPROJCRS[\"SVY21\",\n    BASEGEOGCRS[\"SVY21[WGS84]\",\n        DATUM[\"World Geodetic System 1984\",\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]],\n            ID[\"EPSG\",6326]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"Degree\",0.0174532925199433]]],\n    CONVERSION[\"unnamed\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",1.36666666666667,\n            ANGLEUNIT[\"Degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",103.833333333333,\n            ANGLEUNIT[\"Degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",1,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",28001.642,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",38744.572,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"(E)\",east,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1,\n                ID[\"EPSG\",9001]]],\n        AXIS[\"(N)\",north,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1,\n                ID[\"EPSG\",9001]]]]\n\n\nIn order to assign the correct EPSG code, use st_set_crs()\n\nmpsz3414 &lt;- st_set_crs(mpsz,3414)\n\nWarning: st_crs&lt;- : replacing crs does not reproject data; use st_transform for\nthat\n\n\nDouble check the new ESPG using st_crs()\n\nst_crs(mpsz3414)\n\nCoordinate Reference System:\n  User input: EPSG:3414 \n  wkt:\nPROJCRS[\"SVY21 / Singapore TM\",\n    BASEGEOGCRS[\"SVY21\",\n        DATUM[\"SVY21\",\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4757]],\n    CONVERSION[\"Singapore Transverse Mercator\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",1.36666666666667,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",103.833333333333,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",1,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",28001.642,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",38744.572,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"northing (N)\",north,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"easting (E)\",east,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Cadastre, engineering survey, topographic mapping.\"],\n        AREA[\"Singapore - onshore and offshore.\"],\n        BBOX[1.13,103.59,1.47,104.07]],\n    ID[\"EPSG\",3414]]\n\n\n\n\nTransforming the projection of preschool from WGS84 to SVY21\nIn geospatial analytics, it is very common for us to transform the original data from geographic coordinate system to projected coordinate system. This is because geographic coordinate system is not appropriate if the analysis need to use distance or/and area measurements.\nCheck the coordinate system for the preschool dataframe\n\nst_geometry(preschool)\n\nGeometry set for 2290 features \nGeometry type: POINT\nDimension:     XYZ\nBounding box:  xmin: 103.6878 ymin: 1.247759 xmax: 103.9897 ymax: 1.462134\nz_range:       zmin: 0 zmax: 0\nGeodetic CRS:  WGS 84\nFirst 5 geometries:\n\n\nPOINT Z (103.8072 1.299333 0)\n\n\nPOINT Z (103.826 1.312839 0)\n\n\nPOINT Z (103.8409 1.348843 0)\n\n\nPOINT Z (103.8048 1.435024 0)\n\n\nPOINT Z (103.839 1.33315 0)\n\n\nst_set_crs() is not appropriate here because we need to reproject the dataframe from one coordinate system to another coordinate system mathematically.\nThis can be performed using st_transform()\n\npreschool3414 &lt;- st_transform(preschool, crs = 3414)\n\nDouble-check the coordinate system for preschool3414\n\nst_geometry(preschool3414)\n\nGeometry set for 2290 features \nGeometry type: POINT\nDimension:     XYZ\nBounding box:  xmin: 11810.03 ymin: 25596.33 xmax: 45404.24 ymax: 49300.88\nz_range:       zmin: 0 zmax: 0\nProjected CRS: SVY21 / Singapore TM\nFirst 5 geometries:\n\n\nPOINT Z (25089.46 31299.16 0)\n\n\nPOINT Z (27189.07 32792.54 0)\n\n\nPOINT Z (28844.56 36773.76 0)\n\n\nPOINT Z (24821.92 46303.16 0)\n\n\nPOINT Z (28637.82 35038.49 0)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex1/Hands-on_Ex1.html#importing-and-converting-aspatial-data",
    "href": "Hands-on_Ex/Hands-on_Ex1/Hands-on_Ex1.html#importing-and-converting-aspatial-data",
    "title": "Hands-on Exercise 1: Geospatial Data Wrangling with R",
    "section": "Importing and Converting Aspatial Data",
    "text": "Importing and Converting Aspatial Data\n\nImporting the Aspatial Data\nWe can read the listings csv into an R tibble dataframe using read_csv() of readr\n\nlistings &lt;- read_csv('data/aspatial/listings.csv')\n\nRows: 3483 Columns: 75\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (26): listing_url, source, name, description, neighborhood_overview, pi...\ndbl  (37): id, scrape_id, host_id, host_listings_count, host_total_listings_...\nlgl   (7): host_is_superhost, host_has_profile_pic, host_identity_verified, ...\ndate  (5): last_scraped, host_since, calendar_last_scraped, first_review, la...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nWe can use list(), instead of glimpse() in order to see the columns, data types, and some rows of the new dataframe\n\nlist(listings)\n\n[[1]]\n# A tibble: 3,483 × 75\n       id listing_url            scrape_id last_scraped source name  description\n    &lt;dbl&gt; &lt;chr&gt;                      &lt;dbl&gt; &lt;date&gt;       &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt;      \n 1  71609 https://www.airbnb.co…   2.02e13 2023-09-23   previ… Vill… For 3 room…\n 2  71896 https://www.airbnb.co…   2.02e13 2023-09-23   previ… Home… &lt;b&gt;The spa…\n 3  71903 https://www.airbnb.co…   2.02e13 2023-09-23   previ… Home… Like your …\n 4 275343 https://www.airbnb.co…   2.02e13 2023-09-23   city … Rent… **IMPORTAN…\n 5 275344 https://www.airbnb.co…   2.02e13 2023-09-23   city … Rent… Lovely hom…\n 6 289234 https://www.airbnb.co…   2.02e13 2023-09-23   previ… Home… This whole…\n 7 294281 https://www.airbnb.co…   2.02e13 2023-09-23   city … Rent… I have 3 b…\n 8 324945 https://www.airbnb.co…   2.02e13 2023-09-23   city … Rent… **IMPORTAN…\n 9 330095 https://www.airbnb.co…   2.02e13 2023-09-23   city … Rent… **IMPORTAN…\n10 369141 https://www.airbnb.co…   2.02e13 2023-09-23   city … Plac… A room in …\n# ℹ 3,473 more rows\n# ℹ 68 more variables: neighborhood_overview &lt;chr&gt;, picture_url &lt;chr&gt;,\n#   host_id &lt;dbl&gt;, host_url &lt;chr&gt;, host_name &lt;chr&gt;, host_since &lt;date&gt;,\n#   host_location &lt;chr&gt;, host_about &lt;chr&gt;, host_response_time &lt;chr&gt;,\n#   host_response_rate &lt;chr&gt;, host_acceptance_rate &lt;chr&gt;,\n#   host_is_superhost &lt;lgl&gt;, host_thumbnail_url &lt;chr&gt;, host_picture_url &lt;chr&gt;,\n#   host_neighbourhood &lt;chr&gt;, host_listings_count &lt;dbl&gt;, …\n\n\n\n\nCreating a simple feature dataframe from an aspatial dataframe\nst_as_sf() can be used to convert the listing dataframe into a simple feature dataframe. Note that:\n\ncoords argument requires the column name of the x-coordinates first (longitude) then the column name of the y-coordinates (latitude)\ncrs argument requires the specific coordinates system. As we suspect the coordinate system of listings to be WGS84, this would be crs = 4326 . Singapore’s EPSG code is 3414 as we have used before.\nWe use %&gt;% in dplyr to nest st_transform() to reproject the new simple feature dataframe into SVY21 (EPSG: 3414) coordinates system.\n\n\nlistings_sf &lt;- st_as_sf(listings,\n                        coords = c('longitude','latitude'),\n                        crs=4326)%&gt;%\n  st_transform(crs=3414)\n\nglimpse() can be used to view the new simple feature dataframe, its data types, and some row values. Notice that a new column called geometry has been added and longitude and latitude have been dropped.\n\nglimpse(listings_sf)\n\nRows: 3,483\nColumns: 74\n$ id                                           &lt;dbl&gt; 71609, 71896, 71903, 2753…\n$ listing_url                                  &lt;chr&gt; \"https://www.airbnb.com/r…\n$ scrape_id                                    &lt;dbl&gt; 2.023092e+13, 2.023092e+1…\n$ last_scraped                                 &lt;date&gt; 2023-09-23, 2023-09-23, …\n$ source                                       &lt;chr&gt; \"previous scrape\", \"previ…\n$ name                                         &lt;chr&gt; \"Villa in Singapore · ★4.…\n$ description                                  &lt;chr&gt; \"For 3 rooms.Book room 1&…\n$ neighborhood_overview                        &lt;chr&gt; NA, NA, \"Quiet and view o…\n$ picture_url                                  &lt;chr&gt; \"https://a0.muscache.com/…\n$ host_id                                      &lt;dbl&gt; 367042, 367042, 367042, 1…\n$ host_url                                     &lt;chr&gt; \"https://www.airbnb.com/u…\n$ host_name                                    &lt;chr&gt; \"Belinda\", \"Belinda\", \"Be…\n$ host_since                                   &lt;date&gt; 2011-01-29, 2011-01-29, …\n$ host_location                                &lt;chr&gt; \"Singapore\", \"Singapore\",…\n$ host_about                                   &lt;chr&gt; \"Hi My name is Belinda -H…\n$ host_response_time                           &lt;chr&gt; \"within a few hours\", \"wi…\n$ host_response_rate                           &lt;chr&gt; \"100%\", \"100%\", \"100%\", \"…\n$ host_acceptance_rate                         &lt;chr&gt; \"100%\", \"100%\", \"100%\", \"…\n$ host_is_superhost                            &lt;lgl&gt; FALSE, FALSE, FALSE, FALS…\n$ host_thumbnail_url                           &lt;chr&gt; \"https://a0.muscache.com/…\n$ host_picture_url                             &lt;chr&gt; \"https://a0.muscache.com/…\n$ host_neighbourhood                           &lt;chr&gt; \"Tampines\", \"Tampines\", \"…\n$ host_listings_count                          &lt;dbl&gt; 5, 5, 5, 52, 52, 5, 7, 52…\n$ host_total_listings_count                    &lt;dbl&gt; 15, 15, 15, 65, 65, 15, 8…\n$ host_verifications                           &lt;chr&gt; \"['email', 'phone']\", \"['…\n$ host_has_profile_pic                         &lt;lgl&gt; TRUE, TRUE, TRUE, TRUE, T…\n$ host_identity_verified                       &lt;lgl&gt; TRUE, TRUE, TRUE, TRUE, T…\n$ neighbourhood                                &lt;chr&gt; NA, NA, \"Singapore, Singa…\n$ neighbourhood_cleansed                       &lt;chr&gt; \"Tampines\", \"Tampines\", \"…\n$ neighbourhood_group_cleansed                 &lt;chr&gt; \"East Region\", \"East Regi…\n$ property_type                                &lt;chr&gt; \"Private room in villa\", …\n$ room_type                                    &lt;chr&gt; \"Private room\", \"Private …\n$ accommodates                                 &lt;dbl&gt; 3, 1, 2, 1, 1, 4, 2, 1, 1…\n$ bathrooms                                    &lt;lgl&gt; NA, NA, NA, NA, NA, NA, N…\n$ bathrooms_text                               &lt;chr&gt; \"1 private bath\", \"Shared…\n$ bedrooms                                     &lt;dbl&gt; NA, NA, NA, NA, NA, 3, NA…\n$ beds                                         &lt;dbl&gt; 3, 1, 2, 1, 1, 5, 1, 1, 1…\n$ amenities                                    &lt;chr&gt; \"[\\\"Private backyard \\\\u2…\n$ price                                        &lt;chr&gt; \"$150.00\", \"$80.00\", \"$80…\n$ minimum_nights                               &lt;dbl&gt; 92, 92, 92, 60, 60, 92, 9…\n$ maximum_nights                               &lt;dbl&gt; 365, 365, 365, 999, 999, …\n$ minimum_minimum_nights                       &lt;dbl&gt; 92, 92, 92, 60, 60, 92, 9…\n$ maximum_minimum_nights                       &lt;dbl&gt; 92, 92, 92, 60, 60, 92, 9…\n$ minimum_maximum_nights                       &lt;dbl&gt; 1125, 1125, 1125, 1125, 1…\n$ maximum_maximum_nights                       &lt;dbl&gt; 1125, 1125, 1125, 1125, 1…\n$ minimum_nights_avg_ntm                       &lt;dbl&gt; 92, 92, 92, 60, 60, 92, 9…\n$ maximum_nights_avg_ntm                       &lt;dbl&gt; 1125, 1125, 1125, 1125, 1…\n$ calendar_updated                             &lt;lgl&gt; NA, NA, NA, NA, NA, NA, N…\n$ has_availability                             &lt;lgl&gt; TRUE, TRUE, TRUE, TRUE, T…\n$ availability_30                              &lt;dbl&gt; 28, 28, 28, 1, 30, 28, 30…\n$ availability_60                              &lt;dbl&gt; 58, 58, 58, 1, 60, 58, 60…\n$ availability_90                              &lt;dbl&gt; 88, 88, 88, 1, 90, 88, 90…\n$ availability_365                             &lt;dbl&gt; 89, 89, 89, 275, 274, 89,…\n$ calendar_last_scraped                        &lt;date&gt; 2023-09-23, 2023-09-23, …\n$ number_of_reviews                            &lt;dbl&gt; 20, 24, 47, 22, 17, 12, 1…\n$ number_of_reviews_ltm                        &lt;dbl&gt; 0, 0, 0, 0, 3, 0, 0, 1, 3…\n$ number_of_reviews_l30d                       &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 1, 1…\n$ first_review                                 &lt;date&gt; 2011-12-19, 2011-07-30, …\n$ last_review                                  &lt;date&gt; 2020-01-17, 2019-10-13, …\n$ review_scores_rating                         &lt;dbl&gt; 4.44, 4.16, 4.41, 4.40, 4…\n$ review_scores_accuracy                       &lt;dbl&gt; 4.37, 4.22, 4.39, 4.16, 4…\n$ review_scores_cleanliness                    &lt;dbl&gt; 4.00, 4.09, 4.52, 4.26, 4…\n$ review_scores_checkin                        &lt;dbl&gt; 4.63, 4.43, 4.63, 4.47, 4…\n$ review_scores_communication                  &lt;dbl&gt; 4.78, 4.43, 4.64, 4.42, 4…\n$ review_scores_location                       &lt;dbl&gt; 4.26, 4.17, 4.50, 4.53, 4…\n$ review_scores_value                          &lt;dbl&gt; 4.32, 4.04, 4.36, 4.63, 4…\n$ license                                      &lt;chr&gt; NA, NA, NA, \"S0399\", \"S03…\n$ instant_bookable                             &lt;lgl&gt; FALSE, FALSE, FALSE, TRUE…\n$ calculated_host_listings_count               &lt;dbl&gt; 5, 5, 5, 52, 52, 5, 7, 52…\n$ calculated_host_listings_count_entire_homes  &lt;dbl&gt; 0, 0, 0, 1, 1, 0, 1, 1, 1…\n$ calculated_host_listings_count_private_rooms &lt;dbl&gt; 5, 5, 5, 51, 51, 5, 6, 51…\n$ calculated_host_listings_count_shared_rooms  &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ reviews_per_month                            &lt;dbl&gt; 0.14, 0.16, 0.31, 0.17, 0…\n$ geometry                                     &lt;POINT [m]&gt; POINT (41972.5 3639…"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex1/Hands-on_Ex1.html#geoprocessing-with-sf-package",
    "href": "Hands-on_Ex/Hands-on_Ex1/Hands-on_Ex1.html#geoprocessing-with-sf-package",
    "title": "Hands-on Exercise 1: Geospatial Data Wrangling with R",
    "section": "Geoprocessing with sf package",
    "text": "Geoprocessing with sf package\nThe sf package offers a wide range of geoprocessing (GIS) functions.\nIn this section, you will learn how to perform two commonly used geoprocessing functions, namely buffering and point in polygon count.\n\nBuffering\nThe scenario:\nThe authority is planning to upgrade the exiting cycling path. To do so, they need to acquire 5 metres of reserved land on the both sides of the current cycling path. You are tasked to determine the extend of the land need to be acquired and their total area.\nThe solution\nWe can use st_buffer() to compute the 5-meter buffers around cycling paths\n\nbuffer_cycling &lt;- st_buffer(cyclingpath, dist =5,\n                            nQuadSegs = 30)\n\nWe can then calculate the area of each of the buffers using st_area()\n\nbuffer_cycling$AREA &lt;- st_area(buffer_cycling)\n\nLastly, we can sum up all the areas of the buffers to derive the total land involved\n\nsum(buffer_cycling$AREA)\n\n1774367 [m^2]\n\n\n\n\nPoint-in-polygon count\nThe scenario:\nA pre-school service group want to find out the numbers of pre-schools in each Planning Subzone.\nThe solution:\nWe can: first, identify pre-schools located inside each Planning Subzone by using st_intersects(), second, length() can be used to calculate number of pre-schools that falls inside each planning subzone.\n\nmpsz3414$`PreSch Count` &lt;- lengths(st_intersects(mpsz3414, preschool3414))\n\nsummary() can be used to check the summary statistics of the newly created PreSch Count column in mpsz3414\n\nsummary(mpsz3414$`PreSch Count`)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   0.00    0.00    4.00    7.09   10.00   72.00 \n\n\ntop_n() can be used to list the top n planning subzone with the highest number of pre-school\n\ntop_n(mpsz3414,1,`PreSch Count`)\n\nSimple feature collection with 1 feature and 16 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 39655.33 ymin: 35966 xmax: 42940.57 ymax: 38622.37\nProjected CRS: SVY21 / Singapore TM\n  OBJECTID SUBZONE_NO     SUBZONE_N SUBZONE_C CA_IND PLN_AREA_N PLN_AREA_C\n1      189          2 TAMPINES EAST    TMSZ02      N   TAMPINES         TM\n     REGION_N REGION_C          INC_CRC FMEL_UPD_D   X_ADDR   Y_ADDR SHAPE_Leng\n1 EAST REGION       ER 21658EAAF84F4D8D 2014-12-05 41122.55 37392.39   10180.62\n  SHAPE_Area                       geometry PreSch Count\n1    4339824 MULTIPOLYGON (((42196.76 38...           72\n\n\nWe can also calculate the density of preschool by planning subzone:\nFirst, st_area() can be used to derive the area of each planning subzone.\n\nmpsz3414$AREA &lt;- mpsz3414%&gt;%\n  st_area()\n\nNext, mutate() can be used to compute the density by using the previously created ‘PreSch Count’ and ‘AREA’ columns\n\nmpsz3414 &lt;- mpsz3414 %&gt;%\n  mutate(`PreSch Density` = (`PreSch Count`/AREA)*1000000)\n\nWe can extract the planning subzone with the highest preschool density using top_n()\n\ntop_n(mpsz3414,1,`PreSch Density`)\n\nSimple feature collection with 1 feature and 18 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 29501.64 ymin: 28623.75 xmax: 29976.93 ymax: 29362.03\nProjected CRS: SVY21 / Singapore TM\n  OBJECTID SUBZONE_NO SUBZONE_N SUBZONE_C CA_IND    PLN_AREA_N PLN_AREA_C\n1       27          8     CECIL    DTSZ08      Y DOWNTOWN CORE         DT\n        REGION_N REGION_C          INC_CRC FMEL_UPD_D  X_ADDR   Y_ADDR\n1 CENTRAL REGION       CR 65AA82AF6F4D925D 2014-12-05 29730.2 29011.33\n  SHAPE_Leng SHAPE_Area                       geometry PreSch Count\n1   2116.095   196619.9 MULTIPOLYGON (((29808.18 28...            7\n            AREA   PreSch Density\n1 196619.9 [m^2] 35.60169 [1/m^2]"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex1/Hands-on_Ex1_SFEDA.html",
    "href": "Hands-on_Ex/Hands-on_Ex1/Hands-on_Ex1_SFEDA.html",
    "title": "Hands-on Exercise 1: Geospatial Data Wrangling with R",
    "section": "",
    "text": "In this hands-on exercise, I learn how to import and wrangle geospatial data using appropriate R packages."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex1/Hands-on_Ex1_SFEDA.html#overview",
    "href": "Hands-on_Ex/Hands-on_Ex1/Hands-on_Ex1_SFEDA.html#overview",
    "title": "Hands-on Exercise 1: Geospatial Data Wrangling with R",
    "section": "",
    "text": "In this hands-on exercise, I learn how to import and wrangle geospatial data using appropriate R packages."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex1/Hands-on_Ex1_SFEDA.html#getting-started",
    "href": "Hands-on_Ex/Hands-on_Ex1/Hands-on_Ex1_SFEDA.html#getting-started",
    "title": "Hands-on Exercise 1: Geospatial Data Wrangling with R",
    "section": "Getting Started",
    "text": "Getting Started\nThe code chunk below installs and loads sf and tidyverse packages into R environment.\n\npacman::p_load(sf, tidyverse)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex1/Hands-on_Ex1_SFEDA.html#importing-geospatial-data",
    "href": "Hands-on_Ex/Hands-on_Ex1/Hands-on_Ex1_SFEDA.html#importing-geospatial-data",
    "title": "Hands-on Exercise 1: Geospatial Data Wrangling with R",
    "section": "Importing Geospatial Data",
    "text": "Importing Geospatial Data\n\nImporting polygon features data\nReading the Master Planning 2014 Subzone shapefile into a dataframe\n\nmpsz &lt;- st_read(dsn = \"data/geospatial\", layer = \"MP14_SUBZONE_WEB_PL\")\n\nReading layer `MP14_SUBZONE_WEB_PL' from data source \n  `D:\\phlong2023\\ISSS624\\Hands-on_Ex\\Hands-on_Ex1\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\n\n\nReading the CyclingPath shapefile into a dataframe\n\ncyclingpath &lt;- st_read(dsn = \"data/geospatial\",layer = 'CyclingPathGazette')\n\nReading layer `CyclingPathGazette' from data source \n  `D:\\phlong2023\\ISSS624\\Hands-on_Ex\\Hands-on_Ex1\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 2558 features and 2 fields\nGeometry type: MULTILINESTRING\nDimension:     XY\nBounding box:  xmin: 11854.32 ymin: 28347.98 xmax: 42626.09 ymax: 48948.15\nProjected CRS: SVY21\n\n\nRead the Pre-School Locations kml file into a dataframe using a complete path\n\npreschool &lt;- st_read('data/geospatial/PreSchoolsLocation.kml')\n\nReading layer `PRESCHOOLS_LOCATION' from data source \n  `D:\\phlong2023\\ISSS624\\Hands-on_Ex\\Hands-on_Ex1\\data\\geospatial\\PreSchoolsLocation.kml' \n  using driver `KML'\nSimple feature collection with 2290 features and 2 fields\nGeometry type: POINT\nDimension:     XYZ\nBounding box:  xmin: 103.6878 ymin: 1.247759 xmax: 103.9897 ymax: 1.462134\nz_range:       zmin: 0 zmax: 0\nGeodetic CRS:  WGS 84"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex1/Hands-on_Ex1_SFEDA.html#checking-the-content-of-a-simple-feature-dataframe",
    "href": "Hands-on_Ex/Hands-on_Ex1/Hands-on_Ex1_SFEDA.html#checking-the-content-of-a-simple-feature-dataframe",
    "title": "Hands-on Exercise 1: Geospatial Data Wrangling with R",
    "section": "Checking the Content of a Simple Feature DataFrame",
    "text": "Checking the Content of a Simple Feature DataFrame\n\nWorking with st_geometry()\nUsing st_geometry() to retrieve basic information of the dataframe\n\nst_geometry(mpsz)\n\nGeometry set for 323 features \nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\nFirst 5 geometries:\n\n\n\n\nWorking with glimpse()\nUse glimpse() to get the data types of each column and some of their values\n\nglimpse(mpsz)\n\nRows: 323\nColumns: 16\n$ OBJECTID   &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, …\n$ SUBZONE_NO &lt;int&gt; 1, 1, 3, 8, 3, 7, 9, 2, 13, 7, 12, 6, 1, 5, 1, 1, 3, 2, 2, …\n$ SUBZONE_N  &lt;chr&gt; \"MARINA SOUTH\", \"PEARL'S HILL\", \"BOAT QUAY\", \"HENDERSON HIL…\n$ SUBZONE_C  &lt;chr&gt; \"MSSZ01\", \"OTSZ01\", \"SRSZ03\", \"BMSZ08\", \"BMSZ03\", \"BMSZ07\",…\n$ CA_IND     &lt;chr&gt; \"Y\", \"Y\", \"Y\", \"N\", \"N\", \"N\", \"N\", \"Y\", \"N\", \"N\", \"N\", \"N\",…\n$ PLN_AREA_N &lt;chr&gt; \"MARINA SOUTH\", \"OUTRAM\", \"SINGAPORE RIVER\", \"BUKIT MERAH\",…\n$ PLN_AREA_C &lt;chr&gt; \"MS\", \"OT\", \"SR\", \"BM\", \"BM\", \"BM\", \"BM\", \"SR\", \"QT\", \"QT\",…\n$ REGION_N   &lt;chr&gt; \"CENTRAL REGION\", \"CENTRAL REGION\", \"CENTRAL REGION\", \"CENT…\n$ REGION_C   &lt;chr&gt; \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\",…\n$ INC_CRC    &lt;chr&gt; \"5ED7EB253F99252E\", \"8C7149B9EB32EEFC\", \"C35FEFF02B13E0E5\",…\n$ FMEL_UPD_D &lt;date&gt; 2014-12-05, 2014-12-05, 2014-12-05, 2014-12-05, 2014-12-05…\n$ X_ADDR     &lt;dbl&gt; 31595.84, 28679.06, 29654.96, 26782.83, 26201.96, 25358.82,…\n$ Y_ADDR     &lt;dbl&gt; 29220.19, 29782.05, 29974.66, 29933.77, 30005.70, 29991.38,…\n$ SHAPE_Leng &lt;dbl&gt; 5267.381, 3506.107, 1740.926, 3313.625, 2825.594, 4428.913,…\n$ SHAPE_Area &lt;dbl&gt; 1630379.27, 559816.25, 160807.50, 595428.89, 387429.44, 103…\n$ geometry   &lt;MULTIPOLYGON [m]&gt; MULTIPOLYGON (((31495.56 30..., MULTIPOLYGON (…\n\n\n\n\nWorking with head()\nhead() lets us inspect the top n rows of the dataframe\n\nhead(mpsz, n= 5)\n\nSimple feature collection with 5 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 25867.68 ymin: 28369.47 xmax: 32362.39 ymax: 30435.54\nProjected CRS: SVY21\n  OBJECTID SUBZONE_NO      SUBZONE_N SUBZONE_C CA_IND      PLN_AREA_N\n1        1          1   MARINA SOUTH    MSSZ01      Y    MARINA SOUTH\n2        2          1   PEARL'S HILL    OTSZ01      Y          OUTRAM\n3        3          3      BOAT QUAY    SRSZ03      Y SINGAPORE RIVER\n4        4          8 HENDERSON HILL    BMSZ08      N     BUKIT MERAH\n5        5          3        REDHILL    BMSZ03      N     BUKIT MERAH\n  PLN_AREA_C       REGION_N REGION_C          INC_CRC FMEL_UPD_D   X_ADDR\n1         MS CENTRAL REGION       CR 5ED7EB253F99252E 2014-12-05 31595.84\n2         OT CENTRAL REGION       CR 8C7149B9EB32EEFC 2014-12-05 28679.06\n3         SR CENTRAL REGION       CR C35FEFF02B13E0E5 2014-12-05 29654.96\n4         BM CENTRAL REGION       CR 3775D82C5DDBEFBD 2014-12-05 26782.83\n5         BM CENTRAL REGION       CR 85D9ABEF0A40678F 2014-12-05 26201.96\n    Y_ADDR SHAPE_Leng SHAPE_Area                       geometry\n1 29220.19   5267.381  1630379.3 MULTIPOLYGON (((31495.56 30...\n2 29782.05   3506.107   559816.2 MULTIPOLYGON (((29092.28 30...\n3 29974.66   1740.926   160807.5 MULTIPOLYGON (((29932.33 29...\n4 29933.77   3313.625   595428.9 MULTIPOLYGON (((27131.28 30...\n5 30005.70   2825.594   387429.4 MULTIPOLYGON (((26451.03 30..."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex1/Hands-on_Ex1_SFEDA.html#plotting-geospatial-data",
    "href": "Hands-on_Ex/Hands-on_Ex1/Hands-on_Ex1_SFEDA.html#plotting-geospatial-data",
    "title": "Hands-on Exercise 1: Geospatial Data Wrangling with R",
    "section": "Plotting Geospatial Data",
    "text": "Plotting Geospatial Data\nThe default plot of an sf object is a multi-plot of all attributes, up to a reasonable maximum. This can be seen using the plot() function.\n\nplot(mpsz)\n\n\n\n\nWe can choose to plot only the geometry (outline) by using st_geometry()\n\nplot(st_geometry(mpsz))\n\n\n\n\nWe can also choose the specific attribute of the dataframe we would like to plot by addressing it in the R dataframe\n\nplot(mpsz['PLN_AREA_N'])"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex1/Hands-on_Ex1_SFEDA.html#working-with-projection",
    "href": "Hands-on_Ex/Hands-on_Ex1/Hands-on_Ex1_SFEDA.html#working-with-projection",
    "title": "Hands-on Exercise 1: Geospatial Data Wrangling with R",
    "section": "Working with Projection",
    "text": "Working with Projection\nMap projection is an important property of a geospatial data. In order to perform geoprocessing using two geospatial data, we need to ensure that both geospatial data are projected using similar coordinate system.\nThe process of projecting one dataframe from one coordinate system to another is called projection transformation.\n\nAssigning EPSG code to a simple feature data frame\nIdentifying the coordinate system of a dataframe using st_crs()\n\nst_crs(mpsz)\n\nCoordinate Reference System:\n  User input: SVY21 \n  wkt:\nPROJCRS[\"SVY21\",\n    BASEGEOGCRS[\"SVY21[WGS84]\",\n        DATUM[\"World Geodetic System 1984\",\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]],\n            ID[\"EPSG\",6326]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"Degree\",0.0174532925199433]]],\n    CONVERSION[\"unnamed\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",1.36666666666667,\n            ANGLEUNIT[\"Degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",103.833333333333,\n            ANGLEUNIT[\"Degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",1,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",28001.642,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",38744.572,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"(E)\",east,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1,\n                ID[\"EPSG\",9001]]],\n        AXIS[\"(N)\",north,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1,\n                ID[\"EPSG\",9001]]]]\n\n\nIn order to assign the correct EPSG code, use st_set_crs()\n\nmpsz3414 &lt;- st_set_crs(mpsz,3414)\n\nDouble check the new ESPG using st_crs()\n\nst_crs(mpsz3414)\n\nCoordinate Reference System:\n  User input: EPSG:3414 \n  wkt:\nPROJCRS[\"SVY21 / Singapore TM\",\n    BASEGEOGCRS[\"SVY21\",\n        DATUM[\"SVY21\",\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4757]],\n    CONVERSION[\"Singapore Transverse Mercator\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",1.36666666666667,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",103.833333333333,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",1,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",28001.642,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",38744.572,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"northing (N)\",north,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"easting (E)\",east,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Cadastre, engineering survey, topographic mapping.\"],\n        AREA[\"Singapore - onshore and offshore.\"],\n        BBOX[1.13,103.59,1.47,104.07]],\n    ID[\"EPSG\",3414]]\n\n\n\n\nTransforming the projection of preschool from WGS84 to SVY21\nIn geospatial analytics, it is very common for us to transform the original data from geographic coordinate system to projected coordinate system. This is because geographic coordinate system is not appropriate if the analysis need to use distance or/and area measurements.\nCheck the coordinate system for the preschool dataframe\n\nst_geometry(preschool)\n\nGeometry set for 2290 features \nGeometry type: POINT\nDimension:     XYZ\nBounding box:  xmin: 103.6878 ymin: 1.247759 xmax: 103.9897 ymax: 1.462134\nz_range:       zmin: 0 zmax: 0\nGeodetic CRS:  WGS 84\nFirst 5 geometries:\n\n\nst_set_crs() is not appropriate here because we need to reproject the dataframe from one coordinate system to another coordinate system mathematically.\nThis can be performed using st_transform()\n\npreschool3414 &lt;- st_transform(preschool, crs = 3414)\n\nDouble-check the coordinate system for preschool3414\n\nst_geometry(preschool3414)\n\nGeometry set for 2290 features \nGeometry type: POINT\nDimension:     XYZ\nBounding box:  xmin: 11810.03 ymin: 25596.33 xmax: 45404.24 ymax: 49300.88\nz_range:       zmin: 0 zmax: 0\nProjected CRS: SVY21 / Singapore TM\nFirst 5 geometries:"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex1/Hands-on_Ex1_SFEDA.html#importing-and-converting-aspatial-data",
    "href": "Hands-on_Ex/Hands-on_Ex1/Hands-on_Ex1_SFEDA.html#importing-and-converting-aspatial-data",
    "title": "Hands-on Exercise 1: Geospatial Data Wrangling with R",
    "section": "Importing and Converting Aspatial Data",
    "text": "Importing and Converting Aspatial Data\n\nImporting the Aspatial Data\nWe can read the listings csv into an R tibble dataframe using read_csv() of readr\n\nlistings &lt;- read_csv('data/aspatial/listings.csv')\n\nWe can use list(), instead of glimpse() in order to see the columns, data types, and some rows of the new dataframe\n\nlist(listings)\n\n[[1]]\n# A tibble: 3,483 × 75\n       id listing_url            scrape_id last_scraped source name  description\n    &lt;dbl&gt; &lt;chr&gt;                      &lt;dbl&gt; &lt;date&gt;       &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt;      \n 1  71609 https://www.airbnb.co…   2.02e13 2023-09-23   previ… Vill… For 3 room…\n 2  71896 https://www.airbnb.co…   2.02e13 2023-09-23   previ… Home… &lt;b&gt;The spa…\n 3  71903 https://www.airbnb.co…   2.02e13 2023-09-23   previ… Home… Like your …\n 4 275343 https://www.airbnb.co…   2.02e13 2023-09-23   city … Rent… **IMPORTAN…\n 5 275344 https://www.airbnb.co…   2.02e13 2023-09-23   city … Rent… Lovely hom…\n 6 289234 https://www.airbnb.co…   2.02e13 2023-09-23   previ… Home… This whole…\n 7 294281 https://www.airbnb.co…   2.02e13 2023-09-23   city … Rent… I have 3 b…\n 8 324945 https://www.airbnb.co…   2.02e13 2023-09-23   city … Rent… **IMPORTAN…\n 9 330095 https://www.airbnb.co…   2.02e13 2023-09-23   city … Rent… **IMPORTAN…\n10 369141 https://www.airbnb.co…   2.02e13 2023-09-23   city … Plac… A room in …\n# ℹ 3,473 more rows\n# ℹ 68 more variables: neighborhood_overview &lt;chr&gt;, picture_url &lt;chr&gt;,\n#   host_id &lt;dbl&gt;, host_url &lt;chr&gt;, host_name &lt;chr&gt;, host_since &lt;date&gt;,\n#   host_location &lt;chr&gt;, host_about &lt;chr&gt;, host_response_time &lt;chr&gt;,\n#   host_response_rate &lt;chr&gt;, host_acceptance_rate &lt;chr&gt;,\n#   host_is_superhost &lt;lgl&gt;, host_thumbnail_url &lt;chr&gt;, host_picture_url &lt;chr&gt;,\n#   host_neighbourhood &lt;chr&gt;, host_listings_count &lt;dbl&gt;, …\n\n\n\n\nCreating a simple feature dataframe from an aspatial dataframe\nst_as_sf() can be used to convert the listing dataframe into a simple feature dataframe. Note that:\n\ncoords argument requires the column name of the x-coordinates first (longitude) then the column name of the y-coordinates (latitude)\ncrs argument requires the specific coordinates system. As we suspect the coordinate system of listings to be WGS84, this would be crs = 4326 . Singapore’s EPSG code is 3414 as we have used before.\nWe use %&gt;% in dplyr to nest st_transform() to reproject the new simple feature dataframe into SVY21 (EPSG: 3414) coordinates system.\n\n\nlistings_sf &lt;- st_as_sf(listings,\n                        coords = c('longitude','latitude'),\n                        crs=4326)%&gt;%\n  st_transform(crs=3414)\n\nglimpse() can be used to view the new simple feature dataframe, its data types, and some row values. Notice that a new column called geometry has been added and longitude and latitude have been dropped.\n\nglimpse(listings_sf)\n\nRows: 3,483\nColumns: 74\n$ id                                           &lt;dbl&gt; 71609, 71896, 71903, 2753…\n$ listing_url                                  &lt;chr&gt; \"https://www.airbnb.com/r…\n$ scrape_id                                    &lt;dbl&gt; 2.023092e+13, 2.023092e+1…\n$ last_scraped                                 &lt;date&gt; 2023-09-23, 2023-09-23, …\n$ source                                       &lt;chr&gt; \"previous scrape\", \"previ…\n$ name                                         &lt;chr&gt; \"Villa in Singapore · ★4.…\n$ description                                  &lt;chr&gt; \"For 3 rooms.Book room 1&…\n$ neighborhood_overview                        &lt;chr&gt; NA, NA, \"Quiet and view o…\n$ picture_url                                  &lt;chr&gt; \"https://a0.muscache.com/…\n$ host_id                                      &lt;dbl&gt; 367042, 367042, 367042, 1…\n$ host_url                                     &lt;chr&gt; \"https://www.airbnb.com/u…\n$ host_name                                    &lt;chr&gt; \"Belinda\", \"Belinda\", \"Be…\n$ host_since                                   &lt;date&gt; 2011-01-29, 2011-01-29, …\n$ host_location                                &lt;chr&gt; \"Singapore\", \"Singapore\",…\n$ host_about                                   &lt;chr&gt; \"Hi My name is Belinda -H…\n$ host_response_time                           &lt;chr&gt; \"within a few hours\", \"wi…\n$ host_response_rate                           &lt;chr&gt; \"100%\", \"100%\", \"100%\", \"…\n$ host_acceptance_rate                         &lt;chr&gt; \"100%\", \"100%\", \"100%\", \"…\n$ host_is_superhost                            &lt;lgl&gt; FALSE, FALSE, FALSE, FALS…\n$ host_thumbnail_url                           &lt;chr&gt; \"https://a0.muscache.com/…\n$ host_picture_url                             &lt;chr&gt; \"https://a0.muscache.com/…\n$ host_neighbourhood                           &lt;chr&gt; \"Tampines\", \"Tampines\", \"…\n$ host_listings_count                          &lt;dbl&gt; 5, 5, 5, 52, 52, 5, 7, 52…\n$ host_total_listings_count                    &lt;dbl&gt; 15, 15, 15, 65, 65, 15, 8…\n$ host_verifications                           &lt;chr&gt; \"['email', 'phone']\", \"['…\n$ host_has_profile_pic                         &lt;lgl&gt; TRUE, TRUE, TRUE, TRUE, T…\n$ host_identity_verified                       &lt;lgl&gt; TRUE, TRUE, TRUE, TRUE, T…\n$ neighbourhood                                &lt;chr&gt; NA, NA, \"Singapore, Singa…\n$ neighbourhood_cleansed                       &lt;chr&gt; \"Tampines\", \"Tampines\", \"…\n$ neighbourhood_group_cleansed                 &lt;chr&gt; \"East Region\", \"East Regi…\n$ property_type                                &lt;chr&gt; \"Private room in villa\", …\n$ room_type                                    &lt;chr&gt; \"Private room\", \"Private …\n$ accommodates                                 &lt;dbl&gt; 3, 1, 2, 1, 1, 4, 2, 1, 1…\n$ bathrooms                                    &lt;lgl&gt; NA, NA, NA, NA, NA, NA, N…\n$ bathrooms_text                               &lt;chr&gt; \"1 private bath\", \"Shared…\n$ bedrooms                                     &lt;dbl&gt; NA, NA, NA, NA, NA, 3, NA…\n$ beds                                         &lt;dbl&gt; 3, 1, 2, 1, 1, 5, 1, 1, 1…\n$ amenities                                    &lt;chr&gt; \"[\\\"Private backyard \\\\u2…\n$ price                                        &lt;chr&gt; \"$150.00\", \"$80.00\", \"$80…\n$ minimum_nights                               &lt;dbl&gt; 92, 92, 92, 60, 60, 92, 9…\n$ maximum_nights                               &lt;dbl&gt; 365, 365, 365, 999, 999, …\n$ minimum_minimum_nights                       &lt;dbl&gt; 92, 92, 92, 60, 60, 92, 9…\n$ maximum_minimum_nights                       &lt;dbl&gt; 92, 92, 92, 60, 60, 92, 9…\n$ minimum_maximum_nights                       &lt;dbl&gt; 1125, 1125, 1125, 1125, 1…\n$ maximum_maximum_nights                       &lt;dbl&gt; 1125, 1125, 1125, 1125, 1…\n$ minimum_nights_avg_ntm                       &lt;dbl&gt; 92, 92, 92, 60, 60, 92, 9…\n$ maximum_nights_avg_ntm                       &lt;dbl&gt; 1125, 1125, 1125, 1125, 1…\n$ calendar_updated                             &lt;lgl&gt; NA, NA, NA, NA, NA, NA, N…\n$ has_availability                             &lt;lgl&gt; TRUE, TRUE, TRUE, TRUE, T…\n$ availability_30                              &lt;dbl&gt; 28, 28, 28, 1, 30, 28, 30…\n$ availability_60                              &lt;dbl&gt; 58, 58, 58, 1, 60, 58, 60…\n$ availability_90                              &lt;dbl&gt; 88, 88, 88, 1, 90, 88, 90…\n$ availability_365                             &lt;dbl&gt; 89, 89, 89, 275, 274, 89,…\n$ calendar_last_scraped                        &lt;date&gt; 2023-09-23, 2023-09-23, …\n$ number_of_reviews                            &lt;dbl&gt; 20, 24, 47, 22, 17, 12, 1…\n$ number_of_reviews_ltm                        &lt;dbl&gt; 0, 0, 0, 0, 3, 0, 0, 1, 3…\n$ number_of_reviews_l30d                       &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 1, 1…\n$ first_review                                 &lt;date&gt; 2011-12-19, 2011-07-30, …\n$ last_review                                  &lt;date&gt; 2020-01-17, 2019-10-13, …\n$ review_scores_rating                         &lt;dbl&gt; 4.44, 4.16, 4.41, 4.40, 4…\n$ review_scores_accuracy                       &lt;dbl&gt; 4.37, 4.22, 4.39, 4.16, 4…\n$ review_scores_cleanliness                    &lt;dbl&gt; 4.00, 4.09, 4.52, 4.26, 4…\n$ review_scores_checkin                        &lt;dbl&gt; 4.63, 4.43, 4.63, 4.47, 4…\n$ review_scores_communication                  &lt;dbl&gt; 4.78, 4.43, 4.64, 4.42, 4…\n$ review_scores_location                       &lt;dbl&gt; 4.26, 4.17, 4.50, 4.53, 4…\n$ review_scores_value                          &lt;dbl&gt; 4.32, 4.04, 4.36, 4.63, 4…\n$ license                                      &lt;chr&gt; NA, NA, NA, \"S0399\", \"S03…\n$ instant_bookable                             &lt;lgl&gt; FALSE, FALSE, FALSE, TRUE…\n$ calculated_host_listings_count               &lt;dbl&gt; 5, 5, 5, 52, 52, 5, 7, 52…\n$ calculated_host_listings_count_entire_homes  &lt;dbl&gt; 0, 0, 0, 1, 1, 0, 1, 1, 1…\n$ calculated_host_listings_count_private_rooms &lt;dbl&gt; 5, 5, 5, 51, 51, 5, 6, 51…\n$ calculated_host_listings_count_shared_rooms  &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ reviews_per_month                            &lt;dbl&gt; 0.14, 0.16, 0.31, 0.17, 0…\n$ geometry                                     &lt;POINT [m]&gt; POINT (41972.5 3639…"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex1/Hands-on_Ex1_SFEDA.html#geoprocessing-with-sf-package",
    "href": "Hands-on_Ex/Hands-on_Ex1/Hands-on_Ex1_SFEDA.html#geoprocessing-with-sf-package",
    "title": "Hands-on Exercise 1: Geospatial Data Wrangling with R",
    "section": "Geoprocessing with sf package",
    "text": "Geoprocessing with sf package\nThe sf package offers a wide range of geoprocessing (GIS) functions.\nIn this section, you will learn how to perform two commonly used geoprocessing functions, namely buffering and point in polygon count.\n\nBuffering\nThe scenario:\nThe authority is planning to upgrade the exiting cycling path. To do so, they need to acquire 5 metres of reserved land on the both sides of the current cycling path. You are tasked to determine the extend of the land need to be acquired and their total area.\nThe solution\nWe can use st_buffer() to compute the 5-meter buffers around cycling paths\n\nbuffer_cycling &lt;- st_buffer(cyclingpath, dist =5,\n                            nQuadSegs = 30)\n\nWe can then calculate the area of each of the buffers using st_area()\n\nbuffer_cycling$AREA &lt;- st_area(buffer_cycling)\n\nLastly, we can sum up all the areas of the buffers to derive the total land involved\n\nsum(buffer_cycling$AREA)\n\n1774367 [m^2]\n\n\n\n\nPoint-in-polygon count\nThe scenario:\nA pre-school service group want to find out the numbers of pre-schools in each Planning Subzone.\nThe solution:\nWe can: first, identify pre-schools located inside each Planning Subzone by using st_intersects(), second, length() can be used to calculate number of pre-schools that falls inside each planning subzone.\n\nmpsz3414$`PreSch Count` &lt;- lengths(st_intersects(mpsz3414, preschool3414))\n\nsummary() can be used to check the summary statistics of the newly created PreSch Count column in mpsz3414\n\nsummary(mpsz3414$`PreSch Count`)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   0.00    0.00    4.00    7.09   10.00   72.00 \n\n\ntop_n() can be used to list the top n planning subzone with the highest number of pre-school\n\ntop_n(mpsz3414,1,`PreSch Count`)\n\nSimple feature collection with 1 feature and 16 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 39655.33 ymin: 35966 xmax: 42940.57 ymax: 38622.37\nProjected CRS: SVY21 / Singapore TM\n  OBJECTID SUBZONE_NO     SUBZONE_N SUBZONE_C CA_IND PLN_AREA_N PLN_AREA_C\n1      189          2 TAMPINES EAST    TMSZ02      N   TAMPINES         TM\n     REGION_N REGION_C          INC_CRC FMEL_UPD_D   X_ADDR   Y_ADDR SHAPE_Leng\n1 EAST REGION       ER 21658EAAF84F4D8D 2014-12-05 41122.55 37392.39   10180.62\n  SHAPE_Area                       geometry PreSch Count\n1    4339824 MULTIPOLYGON (((42196.76 38...           72\n\n\nWe can also calculate the density of preschool by planning subzone:\nFirst, st_area() can be used to derive the area of each planning subzone.\n\nmpsz3414$AREA &lt;- mpsz3414%&gt;%\n  st_area()\n\nNext, mutate() can be used to compute the density by using the previously created ‘PreSch Count’ and ‘AREA’ columns\n\nmpsz3414 &lt;- mpsz3414 %&gt;%\n  mutate(`PreSch Density` = (`PreSch Count`/AREA)*1000000)\n\nWe can extract the planning subzone with the highest preschool density using top_n()\n\ntop_n(mpsz3414,1,`PreSch Density`)\n\nSimple feature collection with 1 feature and 18 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 29501.64 ymin: 28623.75 xmax: 29976.93 ymax: 29362.03\nProjected CRS: SVY21 / Singapore TM\n  OBJECTID SUBZONE_NO SUBZONE_N SUBZONE_C CA_IND    PLN_AREA_N PLN_AREA_C\n1       27          8     CECIL    DTSZ08      Y DOWNTOWN CORE         DT\n        REGION_N REGION_C          INC_CRC FMEL_UPD_D  X_ADDR   Y_ADDR\n1 CENTRAL REGION       CR 65AA82AF6F4D925D 2014-12-05 29730.2 29011.33\n  SHAPE_Leng SHAPE_Area                       geometry PreSch Count\n1   2116.095   196619.9 MULTIPOLYGON (((29808.18 28...            7\n            AREA   PreSch Density\n1 196619.9 [m^2] 35.60169 [1/m^2]"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex2/Hands-on_Ex2_Local_Measures.html",
    "href": "Hands-on_Ex/Hands-on_Ex2/Hands-on_Ex2_Local_Measures.html",
    "title": "Hands-on_Ex2_Local_Measures",
    "section": "",
    "text": "The goal of this hands-on exercise is to compute Global and Local Measures of Spatial Autocorrelation (GLSA)."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex2/Hands-on_Ex2_Local_Measures.html#overview",
    "href": "Hands-on_Ex/Hands-on_Ex2/Hands-on_Ex2_Local_Measures.html#overview",
    "title": "Hands-on_Ex2_Local_Measures",
    "section": "",
    "text": "The goal of this hands-on exercise is to compute Global and Local Measures of Spatial Autocorrelation (GLSA)."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex2/Hands-on_Ex2_Local_Measures.html#getting-started",
    "href": "Hands-on_Ex/Hands-on_Ex2/Hands-on_Ex2_Local_Measures.html#getting-started",
    "title": "Hands-on_Ex2_Local_Measures",
    "section": "Getting Started",
    "text": "Getting Started\n\nThe Analytical Question\nIn spatial policy, one of the main development objectives of the local government and planners is to ensure equal distribution of development in the province.\nOur task is to apply appropriate spatial statistical methods to discover if development are evenly distributed geographically.\n\nIf the answer is No, then our next question will be “is there a sign of spatial clustering?”.\n\nIf the answer is Yes, then our next question will be “Where are the clusters?”.\n\n\nIn this case study, we are interested to examine the spatial pattern of a selected development indicator (i.e. GDP per capita) of Hunan Province, People’s Republic of China (PRC).\n\n\nThe Study Area and Data\nTwo data sets will be used:\n\nHunan Province administrative boundary layer at county level. This is a geospatial dataset in ESRI shapefile format\nHunan_2012.csv: This csv file contains selected Hunan’s local development indicators in 2012\n\n\n\nLoading the Required Packages\nWe can use p_load() in the pacman package to load the required packages for data analysis: spdep (for spatial weights), sf, tmap, and tidyverse.\n\npacman::p_load(spdep, sf, tmap, tidyverse)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex2/Hands-on_Ex2_Local_Measures.html#getting-the-data-into-r-environment",
    "href": "Hands-on_Ex/Hands-on_Ex2/Hands-on_Ex2_Local_Measures.html#getting-the-data-into-r-environment",
    "title": "Hands-on_Ex2_Local_Measures",
    "section": "Getting the Data into R Environment",
    "text": "Getting the Data into R Environment\n\nImport shapefile into R\nst_read() can be used to import the Hunan shapefile into R as a simple features object.\n\nhunan &lt;- st_read(dsn = 'data/geospatial',\n                 layer = 'Hunan')\n\nReading layer `Hunan' from data source \n  `D:\\phlong2023\\ISSS624\\Hands-on_Ex\\Hands-on_Ex2\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 88 features and 7 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 108.7831 ymin: 24.6342 xmax: 114.2544 ymax: 30.12812\nGeodetic CRS:  WGS 84\n\n\n\n\nImport csv file into R\nread_csv() can be used to import the Hunan_2012.csv into R.\n\nhunan2012 &lt;- read_csv('data/aspatial/Hunan_2012.csv')\n\n\n\nPerforming Relational Join\nleft_join() can be used to join the attribute fields in hunan2012 with the hunan simple feature object.\n\n\n\n\n\n\nNote\n\n\n\nNote that left_join() automatically seeks out the shared column to join the data frames. However it can also by specified with the syntax: by = join_by(County)\n\n\n\nhunan &lt;- left_join(hunan, hunan2012) %&gt;%\n  select(1:4, 7, 15)\n\n\n\nVisualizing Regional Development Indicator\nThe tmap package can be used to prepare choropleth maps to show the distribution of GDP per capita (GDPPC) according to different breaks style (‘equal’, ‘quantile’).\n\nequal &lt;- tm_shape(hunan)+\n  tm_fill('GDPPC',\n          n = 5,\n          style = 'equal')+\n  tm_borders(alpha = 0.5)+\n  tm_layout(main.title = 'Equal Interval Classification')\n\nquantile &lt;- tm_shape(hunan)+\n  tm_fill('GDPPC',\n          n = 5,\n          style = 'quantile')+\n  tm_borders(alpha = 0.5)+\n  tm_layout(main.title = 'Equal Quantile Classification')\n\ntmap_arrange(equal, quantile, asp = 1, ncol = 2)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex2/Hands-on_Ex2_Local_Measures.html#global-spatial-autocorrelation",
    "href": "Hands-on_Ex/Hands-on_Ex2/Hands-on_Ex2_Local_Measures.html#global-spatial-autocorrelation",
    "title": "Hands-on_Ex2_Local_Measures",
    "section": "Global Spatial Autocorrelation",
    "text": "Global Spatial Autocorrelation\n\nComputing Contiguity Spatial Weights\nBefore we can compute the global spatial autocorrelation statistics, we need to construct a spatial weights of the study area. The spatial weights is used to define the neighbourhood relationships between the geographical units in the study area.\npoly2nb() is used to compute contiguity weight matrices for the study area. This function builds a neighbours list based on regions with contiguous boundaries. For this case study, we will use a Queen contiguity criteria, which look like below.\n\n\nwm_q &lt;- poly2nb(hunan, queen = TRUE)\nsummary(wm_q)\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 448 \nPercentage nonzero weights: 5.785124 \nAverage number of links: 5.090909 \nLink number distribution:\n\n 1  2  3  4  5  6  7  8  9 11 \n 2  2 12 16 24 14 11  4  2  1 \n2 least connected regions:\n30 65 with 1 link\n1 most connected region:\n85 with 11 links\n\n\nThe summary report above shows that there are 88 area units in Hunan. The most connected area unit has 11 neighbours (area 85). There are two area units with only 1 neighbour (30 and 65).\n\n\nRow-standardized Weights Matrix\nNext, we need to assign weights to each neighboring polygon.\nIn our case, each neighboring polygon will be assigned equal weight (style = ‘W’). This is accomplished by assigning 1/(#ofneighbors) to each neighboring county then summing the weighted income values.\nWhile this is the most intuitive way to summarize the neighbors’ values, it has one drawback in that polygons along the edges of the study area will base their lagged values on fewer polygons, thus potentially over- or under-estimating the true nature of the spatial autocorrelation in the data.\n\nrswm_q &lt;- nb2listw(wm_q,\n                   style = 'W',\n                   zero.policy = TRUE)\nrswm_q\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 448 \nPercentage nonzero weights: 5.785124 \nAverage number of links: 5.090909 \n\nWeights style: W \nWeights constants summary:\n   n   nn S0       S1       S2\nW 88 7744 88 37.86334 365.9147\n\n\nThe input of nb2listw() must be an object of class nb. The syntax of the function has two major arguments:\n\nstyle: can take values ‘W’, ‘B’, ‘C’, ‘U’, ‘minmax’ and ‘S’. B is the classic binary coding, W is row standardized (sums over all links to n), C is globally standardized (sums over all links to n), U is equal to C divided by the number of neighbours (sums over all links to unity), while S is the variance-stabilizing coding scheme proposed by Tiefelsdorf et al. 1999, p. 167-168 (sums over all links to n).\nzero policy: if set to TRUE, weights vectors of zero length are inserted for regions without neighbour in the neighbours list. These will in turn generate lag values of zero, equivalent to the sum of products of the zero row t(rep(0, length=length(neighbours))) %*% x, for arbitrary numerical vector x of length length(neighbours). The spatially lagged value of x for the zero-neighbour region will then be zero, which may (or may not) be a sensible choice.\n\n\n\nGlobal Spatial Autocorrelation: Moran’s I\n\nMoran’s I test\nmoran.test() in spdep can be used to perform Moran’s I statistical test\n\nmoran.test(hunan$GDPPC,\n           listw = rswm_q,\n           zero.policy = TRUE,\n           na.action = na.omit)\n\n\n    Moran I test under randomisation\n\ndata:  hunan$GDPPC  \nweights: rswm_q    \n\nMoran I statistic standard deviate = 4.7351, p-value = 1.095e-06\nalternative hypothesis: greater\nsample estimates:\nMoran I statistic       Expectation          Variance \n      0.300749970      -0.011494253       0.004348351 \n\n\nQuestion: What statistical conclusion can you draw from the output above?\nAnswer: As the p-value is below the alpha level of 5%, the result of the Moran’s I test is statistically significant and since the Moran I statistics is positive, we can conclude that there is positive spatial autocorrelation, or that similar values are spatially clustered.\n\n\nMonte Carlo Moran’s I\nmoran.mc() can be used to performs permutation test for Moran’s I statistic. A total of 1000 simulation will be performed.\n\nset.seed(1234)\nbperm &lt;- moran.mc(hunan$GDPPC,\n                 listw = rswm_q,\n                 nsim = 999,\n                 zero.policy = TRUE,\n                 na.action = na.omit)\nbperm\n\n\n    Monte-Carlo simulation of Moran I\n\ndata:  hunan$GDPPC \nweights: rswm_q  \nnumber of simulations + 1: 1000 \n\nstatistic = 0.30075, observed rank = 1000, p-value = 0.001\nalternative hypothesis: greater\n\n\nQuestion: What statistical conclusion can you draw from the output above?\nAnswer: The permutation test supports the result of the Moran’s I. As the p-value is 0.001, only 0.1% of the values equal or exceed it, the result of the Moran’s I test is statistically significant and since the Moran I statistics is positive, we can conclude that there is positive spatial autocorrelation, or that similar values are spatially clustered.\n\n\nVisualizing Monte Carlo Moran’s I\nIt is good practice to examine the simulated Moran’s I test statistics in greater detail. This can be achieved by plotting the distribution of the statistical values as a histogram.\nmean() can be used to get the mean of the simulated values of statistic.\n\nmean(bperm$res[1:999])\n\n[1] -0.01504572\n\n\nvar() can be used to get the variance of the simulated values of statistic.\n\nvar(bperm$res[1:999])\n\n[1] 0.004371574\n\n\nsummary() can be used to get the summary statistics of the simulated values of statistic.\n\nsummary(bperm$res[1:999])\n\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n-0.18339 -0.06168 -0.02125 -0.01505  0.02611  0.27593 \n\n\nhist() and abline() can be used to create a histogram of the simulated values of statistic of the Monte Carlo Moran’s I\n\nhist(bperm$res,\n     freq = TRUE,\n     breaks = 20,\n     xlab = \"Simulated Moran's I\")\nabline(v=0,\n       col='red')\n\n\n\n\nQuestion: What statistical observation can you draw from the output above?\nAnswer: It can be seen that that a very small number of values exceed or equal the value of I at 0.3, meaning that the autocorrelation is statistically significant. Additionally, since the simulated values of statistic is not normally distributed, it demonstrates the reliability of the permutation test to identify statistically significant autocorrelation.\nQuestion: Recreate the graph using ggplot2\n\nbperm_df &lt;- data.frame(bperm$res)\n\nggplot(bperm_df, aes(x=bperm.res))+\n  geom_histogram(col = 'black', size = 0.3, fill = 'lightgrey', boundary = 0, bins = 27)+\n  theme_classic()+\n  labs(title = \"Histogram of Simulated Statistics\",\n       x = \"Simulated Moran's I\",\n       y = \"Frequency\")+\n  theme(plot.title = element_text(hjust = 0.5))+\n  scale_y_continuous(breaks=c(0,20,40,60,80,100))+\n  geom_vline(xintercept = 0, col = 'red')\n\n\n\n\n\n\n\nGlobal Spatial Autocorrelation: Geary’s\n\nGeary’s C test\ngeary.test() can be used to perform Geary’s C test for spatial autocorrelation.\n\ngeary.test(hunan$GDPPC, listw = rswm_q)\n\n\n    Geary C test under randomisation\n\ndata:  hunan$GDPPC \nweights: rswm_q \n\nGeary C statistic standard deviate = 3.6108, p-value = 0.0001526\nalternative hypothesis: Expectation greater than statistic\nsample estimates:\nGeary C statistic       Expectation          Variance \n        0.6907223         1.0000000         0.0073364 \n\n\nQuestion: What statistical conclusion can you draw from the output above?\nAnswer: Geary’s C value ranges from 0 to 2 where 1 is no spatial autocorrelation. Since the statistic is 0.69, it suggests that there is slight positive spatial correlation. Additionally since the p-value is very small, the result is statistically significant.\n\n\nComputing Monte Carlo Geary’s C\nA permutation test (Monte Carlo Geary’s C) can be performed using geary.mc()\n\nset.seed(1234)\nbperm &lt;- geary.mc(hunan$GDPPC,\n                  listw = rswm_q,\n                  nsim = 999)\nbperm\n\n\n    Monte-Carlo simulation of Geary C\n\ndata:  hunan$GDPPC \nweights: rswm_q \nnumber of simulations + 1: 1000 \n\nstatistic = 0.69072, observed rank = 1, p-value = 0.001\nalternative hypothesis: greater\n\n\nQuestion: What statistical conclusion can you draw from the output above?\nAnswer: The permutation test supports the result of the Geary’s C test. Since p-value is 0.001, the result is statistically significant. Furthermore, as the test statistic is 0.69, it can be concluded that there is positive spatial autocorrelation.\n\n\nVisualizing the Monte Carlo Geary’s C\nmean() can be used to get the mean of the simulated values of statistic.\n\nmean(bperm$res[1:999])\n\n[1] 1.004402\n\n\nvar() can be used to get the variance of the simulated values of statistic.\n\nvar(bperm$res[1:999])\n\n[1] 0.007436493\n\n\nsummary() can be used to get the summary statistic of the simulated values of statistic.\n\nsummary(bperm$res[1:999])\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n 0.7142  0.9502  1.0052  1.0044  1.0595  1.2722 \n\n\nhist() and abline() can be used to create a histogram of the simulated values of statistic of the Geary’s C.\n\nhist(bperm$res,\n     freq = TRUE,\n     breaks = 20,\n     xlab = 'Simulated Geary C')\nabline(v=1, col='red')\n\n\n\n\nQuestion: What statistical observation can you draw from the output?\nAnswer: The simulated values is normally distributed around 1, which is one of the implicit assumption of the Geary’s C test."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex2/Hands-on_Ex2_Local_Measures.html#spatial-correlogram",
    "href": "Hands-on_Ex/Hands-on_Ex2/Hands-on_Ex2_Local_Measures.html#spatial-correlogram",
    "title": "Hands-on_Ex2_Local_Measures",
    "section": "Spatial Correlogram",
    "text": "Spatial Correlogram\nSpatial correlograms are great to examine patterns of spatial autocorrelation in the data or model residuals.\nThey show how correlated are pairs of spatial observations when you increase the distance (lag) between them. They are plots of some index of autocorrelation (Moran’s I or Geary’s C) against distance.\nAlthough correlograms are not as fundamental as variograms (a keystone concept of geostatistic), they are very useful as an exploratory and descriptive tool. For this purpose, they actually provide richer information than variograms.\n\nCompute Moran’s I Correlogram\nsp.correlogram() can be used to compute a 6-lag spatial correlogram of GDPPC. The global spatial autocorrelation used is Moran’s I. plot() is then used to plot the output.\n\nMI_corr &lt;- sp.correlogram(wm_q,\n                          hunan$GDPPC,\n                          order = 6,\n                          method = 'I', style = 'W')\n\nplot(MI_corr)\n\n\n\n\nPlotting the output might not allow us to provide complete interpretation. This is because not all autocorrelation values are statistically significant. Hence, it is important for us to examine the full analysis report by printing out the analysis results as in the code chunk below.\n\nprint(MI_corr)\n\nSpatial correlogram for hunan$GDPPC \nmethod: Moran's I\n         estimate expectation   variance standard deviate Pr(I) two sided    \n1 (88)  0.3007500  -0.0114943  0.0043484           4.7351       2.189e-06 ***\n2 (88)  0.2060084  -0.0114943  0.0020962           4.7505       2.029e-06 ***\n3 (88)  0.0668273  -0.0114943  0.0014602           2.0496        0.040400 *  \n4 (88)  0.0299470  -0.0114943  0.0011717           1.2107        0.226015    \n5 (88) -0.1530471  -0.0114943  0.0012440          -4.0134       5.984e-05 ***\n6 (88) -0.1187070  -0.0114943  0.0016791          -2.6164        0.008886 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nQuestion: What statistical observation can you draw from the plot above?\nAnswer: All pairs of results are statistically significant, except for number 4 with a p-value larger than 0.05. This shows that the list of IDs in number 4 do not exhibit spatial autocorrelation with their neighbors.\n\n\nCompute Geary’s C correlogram and plot\nsp.correlogram() can be used to compute a 6-lag spatial correlogram of GDPPC. The global spatial autocorrelation used is Geary’s C. plot() is then used to plot the output.\n\nGC_corr &lt;- sp.correlogram(wm_q,\n                          hunan$GDPPC,\n                          order = 6,\n                          method = 'C', style = 'W')\nplot(GC_corr)\n\n\n\n\nWe will print out the analysis report using print().\n\nprint(GC_corr)\n\nSpatial correlogram for hunan$GDPPC \nmethod: Geary's C\n        estimate expectation  variance standard deviate Pr(I) two sided    \n1 (88) 0.6907223   1.0000000 0.0073364          -3.6108       0.0003052 ***\n2 (88) 0.7630197   1.0000000 0.0049126          -3.3811       0.0007220 ***\n3 (88) 0.9397299   1.0000000 0.0049005          -0.8610       0.3892612    \n4 (88) 1.0098462   1.0000000 0.0039631           0.1564       0.8757128    \n5 (88) 1.2008204   1.0000000 0.0035568           3.3673       0.0007592 ***\n6 (88) 1.0773386   1.0000000 0.0058042           1.0151       0.3100407    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex2/Hands-on_Ex2_Local_Measures.html#cluster-and-outlier-analysis",
    "href": "Hands-on_Ex/Hands-on_Ex2/Hands-on_Ex2_Local_Measures.html#cluster-and-outlier-analysis",
    "title": "Hands-on_Ex2_Local_Measures",
    "section": "Cluster and Outlier Analysis",
    "text": "Cluster and Outlier Analysis\nLocal indicators of Spatial Association (LISA) are statistics that evaluate the existence of clusters in the spatial arrangement of a given variable. For instance, if we are sutdying cancer rates among census tracts in a given city, local clusters in the rates mean that there are areas that have higher or lower rates than is to be expected by chance alone; that is, the values occuring are above or below those of a random distribution in space.\n\nComputing local Moran’s I\nlocalmoran() can be used to compute local Moran’s I. It computes li values, given a set of zi values and a listw object providing neighbour weighting information for the polygon associated with the zi values.\n\nflips &lt;- order(hunan$County) #This code arrange the county column index of hunan in alphabetical order according to the county names\nlocalMI &lt;- localmoran(hunan$GDPPC, rswm_q)\nhead(localMI)\n\n            Ii          E.Ii       Var.Ii        Z.Ii Pr(z != E(Ii))\n1 -0.001468468 -2.815006e-05 4.723841e-04 -0.06626904      0.9471636\n2  0.025878173 -6.061953e-04 1.016664e-02  0.26266425      0.7928094\n3 -0.011987646 -5.366648e-03 1.133362e-01 -0.01966705      0.9843090\n4  0.001022468 -2.404783e-07 5.105969e-06  0.45259801      0.6508382\n5  0.014814881 -6.829362e-05 1.449949e-03  0.39085814      0.6959021\n6 -0.038793829 -3.860263e-04 6.475559e-03 -0.47728835      0.6331568\n\n\nlocalmoran() returns a matrix of values whose columns are:\n\nli: the local Moran’s I statistics\nE.li: the expectation of local Moran statistic under the randomization hypothesis\nVar.li: the variance of local Moran statistic under the randomization hypothesis\nZ.li: the standard deviation of the local Moran statistic\nPr(): the p-value of local Moran statistics\n\nprintCoefmat() can be used to see these statistics for each of the county in our study.\n\nprintCoefmat(data.frame(\n  localMI[flips,],\n  row.names = hunan$County[flips]),\n  check.names = FALSE)\n\n                       Ii        E.Ii      Var.Ii        Z.Ii Pr.z....E.Ii..\nAnhua         -2.2493e-02 -5.0048e-03  5.8235e-02 -7.2467e-02         0.9422\nAnren         -3.9932e-01 -7.0111e-03  7.0348e-02 -1.4791e+00         0.1391\nAnxiang       -1.4685e-03 -2.8150e-05  4.7238e-04 -6.6269e-02         0.9472\nBaojing        3.4737e-01 -5.0089e-03  8.3636e-02  1.2185e+00         0.2230\nChaling        2.0559e-02 -9.6812e-04  2.7711e-02  1.2932e-01         0.8971\nChangning     -2.9868e-05 -9.0010e-09  1.5105e-07 -7.6828e-02         0.9388\nChangsha       4.9022e+00 -2.1348e-01  2.3194e+00  3.3590e+00         0.0008\nChengbu        7.3725e-01 -1.0534e-02  2.2132e-01  1.5895e+00         0.1119\nChenxi         1.4544e-01 -2.8156e-03  4.7116e-02  6.8299e-01         0.4946\nCili           7.3176e-02 -1.6747e-03  4.7902e-02  3.4200e-01         0.7324\nDao            2.1420e-01 -2.0824e-03  4.4123e-02  1.0297e+00         0.3032\nDongan         1.5210e-01 -6.3485e-04  1.3471e-02  1.3159e+00         0.1882\nDongkou        5.2918e-01 -6.4461e-03  1.0748e-01  1.6338e+00         0.1023\nFenghuang      1.8013e-01 -6.2832e-03  1.3257e-01  5.1198e-01         0.6087\nGuidong       -5.9160e-01 -1.3086e-02  3.7003e-01 -9.5104e-01         0.3416\nGuiyang        1.8240e-01 -3.6908e-03  3.2610e-02  1.0305e+00         0.3028\nGuzhang        2.8466e-01 -8.5054e-03  1.4152e-01  7.7931e-01         0.4358\nHanshou        2.5878e-02 -6.0620e-04  1.0167e-02  2.6266e-01         0.7928\nHengdong       9.9964e-03 -4.9063e-04  6.7742e-03  1.2742e-01         0.8986\nHengnan        2.8064e-02 -3.2160e-04  3.7597e-03  4.6294e-01         0.6434\nHengshan      -5.8201e-03 -3.0437e-05  5.1076e-04 -2.5618e-01         0.7978\nHengyang       6.2997e-02 -1.3046e-03  2.1865e-02  4.3486e-01         0.6637\nHongjiang      1.8790e-01 -2.3019e-03  3.1725e-02  1.0678e+00         0.2856\nHuarong       -1.5389e-02 -1.8667e-03  8.1030e-02 -4.7503e-02         0.9621\nHuayuan        8.3772e-02 -8.5569e-04  2.4495e-02  5.4072e-01         0.5887\nHuitong        2.5997e-01 -5.2447e-03  1.1077e-01  7.9685e-01         0.4255\nJiahe         -1.2431e-01 -3.0550e-03  5.1111e-02 -5.3633e-01         0.5917\nJianghua       2.8651e-01 -3.8280e-03  8.0968e-02  1.0204e+00         0.3076\nJiangyong      2.4337e-01 -2.7082e-03  1.1746e-01  7.1800e-01         0.4728\nJingzhou       1.8270e-01 -8.5106e-04  2.4363e-02  1.1759e+00         0.2396\nJinshi        -1.1988e-02 -5.3666e-03  1.1334e-01 -1.9667e-02         0.9843\nJishou        -2.8680e-01 -2.6305e-03  4.4028e-02 -1.3543e+00         0.1756\nLanshan        6.3334e-02 -9.6365e-04  2.0441e-02  4.4972e-01         0.6529\nLeiyang        1.1581e-02 -1.4948e-04  2.5082e-03  2.3422e-01         0.8148\nLengshuijiang -1.7903e+00 -8.2129e-02  2.1598e+00 -1.1623e+00         0.2451\nLi             1.0225e-03 -2.4048e-07  5.1060e-06  4.5260e-01         0.6508\nLianyuan      -1.4672e-01 -1.8983e-03  1.9145e-02 -1.0467e+00         0.2952\nLiling         1.3774e+00 -1.5097e-02  4.2601e-01  2.1335e+00         0.0329\nLinli          1.4815e-02 -6.8294e-05  1.4499e-03  3.9086e-01         0.6959\nLinwu         -2.4621e-03 -9.0703e-06  1.9258e-04 -1.7676e-01         0.8597\nLinxiang       6.5904e-02 -2.9028e-03  2.5470e-01  1.3634e-01         0.8916\nLiuyang        3.3688e+00 -7.7502e-02  1.5180e+00  2.7972e+00         0.0052\nLonghui        8.0801e-01 -1.1377e-02  1.5538e-01  2.0787e+00         0.0376\nLongshan       7.5663e-01 -1.1100e-02  3.1449e-01  1.3690e+00         0.1710\nLuxi           1.8177e-01 -2.4855e-03  3.4249e-02  9.9561e-01         0.3194\nMayang         2.1852e-01 -5.8773e-03  9.8049e-02  7.1663e-01         0.4736\nMiluo          1.8704e+00 -1.6927e-02  2.7925e-01  3.5715e+00         0.0004\nNan           -9.5789e-03 -4.9497e-04  6.8341e-03 -1.0988e-01         0.9125\nNingxiang      1.5607e+00 -7.3878e-02  8.0012e-01  1.8274e+00         0.0676\nNingyuan       2.0910e-01 -7.0884e-03  8.2306e-02  7.5356e-01         0.4511\nPingjiang     -9.8964e-01 -2.6457e-03  5.6027e-02 -4.1698e+00         0.0000\nQidong         1.1806e-01 -2.1207e-03  2.4747e-02  7.6396e-01         0.4449\nQiyang         6.1966e-02 -7.3374e-04  8.5743e-03  6.7712e-01         0.4983\nRucheng       -3.6992e-01 -8.8999e-03  2.5272e-01 -7.1814e-01         0.4727\nSangzhi        2.5053e-01 -4.9470e-03  6.8000e-02  9.7972e-01         0.3272\nShaodong      -3.2659e-02 -3.6592e-05  5.0546e-04 -1.4510e+00         0.1468\nShaoshan       2.1223e+00 -5.0227e-02  1.3668e+00  1.8583e+00         0.0631\nShaoyang       5.9499e-01 -1.1253e-02  1.3012e-01  1.6807e+00         0.0928\nShimen        -3.8794e-02 -3.8603e-04  6.4756e-03 -4.7729e-01         0.6332\nShuangfeng     9.2835e-03 -2.2867e-03  3.1516e-02  6.5174e-02         0.9480\nShuangpai      8.0591e-02 -3.1366e-04  8.9838e-03  8.5358e-01         0.3933\nSuining        3.7585e-01 -3.5933e-03  4.1870e-02  1.8544e+00         0.0637\nTaojiang      -2.5394e-01 -1.2395e-03  1.4477e-02 -2.1002e+00         0.0357\nTaoyuan        1.4729e-02 -1.2039e-04  8.5103e-04  5.0903e-01         0.6107\nTongdao        4.6482e-01 -6.9870e-03  1.9879e-01  1.0582e+00         0.2900\nWangcheng      4.4220e+00 -1.1067e-01  1.3596e+00  3.8873e+00         0.0001\nWugang         7.1003e-01 -7.8144e-03  1.0710e-01  2.1935e+00         0.0283\nXiangtan       2.4530e-01 -3.6457e-04  3.2319e-03  4.3213e+00         0.0000\nXiangxiang     2.6271e-01 -1.2703e-03  2.1290e-02  1.8092e+00         0.0704\nXiangyin       5.4525e-01 -4.7442e-03  7.9236e-02  1.9539e+00         0.0507\nXinhua         1.1810e-01 -6.2649e-03  8.6001e-02  4.2409e-01         0.6715\nXinhuang       1.5725e-01 -4.1820e-03  3.6648e-01  2.6667e-01         0.7897\nXinning        6.8928e-01 -9.6674e-03  2.0328e-01  1.5502e+00         0.1211\nXinshao        5.7578e-02 -8.5932e-03  1.1769e-01  1.9289e-01         0.8470\nXintian       -7.4050e-03 -5.1493e-03  1.0877e-01 -6.8395e-03         0.9945\nXupu           3.2406e-01 -5.7468e-03  5.7735e-02  1.3726e+00         0.1699\nYanling       -6.9021e-02 -5.9211e-04  9.9306e-03 -6.8667e-01         0.4923\nYizhang       -2.6844e-01 -2.2463e-03  4.7588e-02 -1.2202e+00         0.2224\nYongshun       6.3064e-01 -1.1350e-02  1.8830e-01  1.4795e+00         0.1390\nYongxing       4.3411e-01 -9.0735e-03  1.5088e-01  1.1409e+00         0.2539\nYou            7.8750e-02 -7.2728e-03  1.2116e-01  2.4714e-01         0.8048\nYuanjiang      2.0004e-04 -1.7760e-04  2.9798e-03  6.9181e-03         0.9945\nYuanling       8.7298e-03 -2.2981e-06  2.3221e-05  1.8121e+00         0.0700\nYueyang        4.1189e-02 -1.9768e-04  2.3113e-03  8.6085e-01         0.3893\nZhijiang       1.0476e-01 -7.8123e-04  1.3100e-02  9.2214e-01         0.3565\nZhongfang     -2.2685e-01 -2.1455e-03  3.5927e-02 -1.1855e+00         0.2358\nZhuzhou        3.2864e-01 -5.2432e-04  7.2391e-03  3.8688e+00         0.0001\nZixing        -7.6849e-01 -8.8210e-02  9.4057e-01 -7.0144e-01         0.4830\n\n\n\n\nMapping the local Moran’s I\nBefore mapping the local Moran’s I, we can append the local Moran’s I data frame (localMI) onto hunan sf data frame.\n\n\n\n\n\n\nNote\n\n\n\nBy using cbind(), all columns of local MI will be added to hunan. The orders of the counties in both data frame are the same so the statistics will match.\n\n\n\nhunan.localMI &lt;- cbind(hunan, localMI)%&gt;%\n  rename(Pr.Ii = Pr.z....E.Ii..)\n\nWe can then create a choropleth map of the local Moran’s I values\n\ntm_shape(hunan.localMI)+\n  tm_fill(col='Ii',\n          style='pretty',\n          palette = 'RdBu',\n          title = 'Local Moran Statistics')+\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\nMapping the local Moran’s I p-values\nWe can also map the local Moran’s I p-values using similar code\n\ntm_shape(hunan.localMI)+\n  tm_fill(col='Pr.Ii',\n          breaks = c(-Inf,0.001,0.01,0.05,0.1,Inf),\n          palette='-Blues',\n          title = \"Local Moran's I p-values\")+\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\nMapping both local Moran’s I values and p-values\ntmap_arrange() can be used with the code chunks above to put the two plots of local Moran’s I values and p-values side by side.\n\nlocalMI.map &lt;- tm_shape(hunan.localMI)+\n  tm_fill(col='Ii',\n          style='pretty',\n          title = 'Local Moran Statistics')+\n  tm_borders(alpha=0.5)\n\npvalues.map &lt;- tm_shape(hunan.localMI)+\n  tm_fill(col='Pr.Ii',\n          breaks = c(-Inf,0.001, 0.01, 0.05, 0.1, Inf),\n          palette = '-Blues',\n          title = \"Local Moran's I p-values\")+\n  tm_borders(alpha = 0.5)\n\ntmap_arrange(localMI.map, pvalues.map, asp = 1, ncol = 2)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex2/Hands-on_Ex2_Local_Measures.html#creating-a-lisa-cluster-map",
    "href": "Hands-on_Ex/Hands-on_Ex2/Hands-on_Ex2_Local_Measures.html#creating-a-lisa-cluster-map",
    "title": "Hands-on_Ex2_Local_Measures",
    "section": "Creating a LISA Cluster Map",
    "text": "Creating a LISA Cluster Map\nThe LISA Cluster Map shows the significant locations color coded by type of spatial autocorrelation.\nThe first step before we can generate the LISA cluster map is to plot the Moran scatterplot.\n\nPlotting Moran Scatterplot\nThe Moran scatterplot is an illustration of the relationship between the values of the chosen attribute at each location and the average value of the same attribute at neighboring locations.\nmoran.plot() can be used to draw the Moran scatterplot.\n\nnci &lt;- moran.plot(hunan$GDPPC, rswm_q,\n                  labels = as.character(hunan$County),\n                  xlab = 'GDPPC 2012',\n                  ylab = 'Spatially Lag GDPPC 2012')\n\n\n\n\nThe plot is split in 4 quadrants: The top right corner belongs to area that have high GDPPC and are surrounded by other areas that have the average level GDPPC. This is the high-high locations in the lesson slide.\n\n\nPlotting Moran Scatterplot with Standardised Variable\nFirst, scale() can be used to center and scale the variable. Here, centering is done by subtracting the mean (omitting NAs) of the corresponding columns, and scaling is done by dividing the centered variable by their standard deviations.\n\n\n\n\n\n\nNote\n\n\n\nThe as.vector() added is to make sure that the data type we get out of the process is a vector which map neatly into the data frame.\n\n\n\nhunan$Z.GDPPC &lt;- scale(hunan$GDPPC)%&gt;%\n  as.vector()\n\nWe can then plot our new standardised variable onto a Moran scatterplot.\n\nnci2 &lt;- moran.plot(hunan$Z.GDPPC, rswm_q,\n                   labels = as.character(hunan$County),\n                   xlab = 'z-GDPPC 2012',\n                   ylab = 'Spatially Lag z-GDPPC 2012')\n\n\n\n\n\n\nPreparing LISA Map Classes\nThe code chunk below shows the steps to prepare a LISA cluster map. This code create a vector of ‘0’ with length being equal to the number of rows of localMI.\n\nquadrant &lt;- vector(mode='numeric',length = nrow(localMI))\n\nNext, we can derive the spatially lagged variable of interest and center it around its mean\n\nhunan$lag_GDPPC &lt;- lag.listw(rswm_q, hunan$GDPPC)\n\nDV &lt;- hunan$lag_GDPPC - mean(hunan$lag_GDPPC)\n\nNext, we center the local Moran’s around the mean\n\nLM_I &lt;- localMI[,1]-mean(localMI[,1])\n\nNext, we will set a statistical significance level for the local Moran.\n\nsignif &lt;- 0.05\n\nNow, we need to define the four different quadrants and one special quadrant for non-significant Moran:\n\n# Low - Low \nquadrant[DV &lt; 0 & LM_I &gt; 0] &lt;- 1\n# Low - High\nquadrant[DV &lt; 0 & LM_I &lt; 0] &lt;- 3\n# High - Low\nquadrant[DV &gt; 0 & LM_I &lt; 0] &lt;- 2\n# High- High\nquadrant[DV &gt; 0 & LM_I &gt; 0] &lt;- 4\n\n# Non-Significant Moran\nquadrant[localMI[,5]&gt;signif] &lt;- 0\n\n\n\nPlotting LISA map\nWe can build the LISA map using the code below\n\nhunan.localMI$quadrant &lt;- quadrant\ncolors &lt;- c(\"#ffffff\", \"#2c7bb6\", \"#abd9e9\", \"#fdae61\", \"#d7191c\")\nclusters &lt;- c('Insignificant','Low-Low','Low-High','High-Low','High-High')\n\ntm_shape(hunan.localMI) +\n  tm_fill(col = 'quadrant',\n          style='cat',\n          palette = colors[c(sort(unique(quadrant)))+1],\n          labels = clusters[c(sort(unique(quadrant)))+1],\n          popup.vars = c(''))+\n  tm_view(set.zoom.limits = c(11,17)) +\n  tm_borders(alpha = 0.5)\n\n\n\n\nFor effective interpretation, it is better to plot both the local Moran’s I values map and its corresponding p-values map next to each other\n\ngdppc &lt;- qtm(hunan, 'GDPPC')\n\nhunan.localMI$quadrant &lt;- quadrant\ncolors &lt;- c(\"#ffffff\", \"#2c7bb6\", \"#abd9e9\", \"#fdae61\", \"#d7191c\")\nclusters &lt;- c('Insignificant','Low-Low','Low-High','High-Low','High-High')\n\nLISAmap &lt;- tm_shape(hunan.localMI)+\n  tm_fill(col = 'quadrant',\n          style = 'cat',\n          palette = colors[sort(unique(quadrant))+1],\n          labels = clusters[sort(unique(quadrant))+1],\n          popup.vars = c(''))+\n  tm_view(set.zoom.limits = c(11,17))+\n  tm_borders(alpha = 0.5)\n\ntmap_arrange(gdppc, LISAmap, asp = 1, ncol = 2)\n\n\n\n\nWe can also bring up the local Moran’s I values and p-values map before for comparison\n\ntmap_arrange(localMI.map, pvalues.map, asp = 1, ncol = 2)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex2/Hands-on_Ex2_Local_Measures.html#hot-spot-and-cold-spot-area-analysis",
    "href": "Hands-on_Ex/Hands-on_Ex2/Hands-on_Ex2_Local_Measures.html#hot-spot-and-cold-spot-area-analysis",
    "title": "Hands-on_Ex2_Local_Measures",
    "section": "Hot Spot and Cold Spot Area Analysis",
    "text": "Hot Spot and Cold Spot Area Analysis\nBeside detecting cluster and outliers, localised spatial statistics can also be used to detect hot spot and/or cold spot areas.\nThe term ‘hot spot’ has been used generically across disciplines to describe a region or value that is higher relative to its surroundings (Lepers et al 2005, Aben et al 2012, Isobe et al 2015).\n\nGetis and Ord’s G-Statistics\nAn alternative spatial statistics to detect spatial anomalies is the Getis and Ord’s G-statistics (Getis and Ord, 1972; Ord and Getis, 1995).\nIt looks at neighbours within a defined proximity to identify whether either high or low values cluster spatially. Here, statistically significant hot-spots are recognised as areas of high values where other areas within a neighbourhood range also share high values too.\nThe analysis consists of three steps:\n\nDeriving spatial weight matrix\nComputing Gi statistics\nMapping Gi statistics\n\n\n\nDeriving Distance-based Weight Matrix\nFirst, we need to define a new set of neighbours. While the spatial autocorrelation considered units with shared borders, for Getis-Ord we are defining neighbours based on distance.\nThere are two types of distance-based proximity matrix, they are:\n\nfixed distance weight matrix; and\nadaptive distance weight matrix\n\n\nDeriving the Centroid\nWe will need points to associate with each polygon before we can make our connectivity graph. We will need to create a separate data frame to find the centroid for each polygon.\nThe needed dataframe can be created with map_dbl() which will map the function st_centroid() on the geometry column of each row of the hunan dataframe.\nFirst we find the longitude, which is in the geometry column at position [[1]].\n\nlongitude &lt;- map_dbl(hunan$geometry, ~st_centroid(.x)[[1]])\n\nNext, we find the latitude which is at position [[2]].\n\nlatitude &lt;- map_dbl(hunan$geometry, ~st_centroid(.x)[[2]])\n\nNow, we can combine them to create a dataframe with the centroids’ longitude and latitude\n\ncoords_hunan &lt;- cbind(longitude, latitude)\n\n\n\nDetermine the Cut-off Distance\nFirstly, we need to determine the upper limit for distance band by using the steps below:\n\nReturn a matrix with the indices of points belonging to the set of the k nearest neighbours of each other by using knearneigh().\nConvert the knn object into a neighbours list of class nb with a list of integer vectors containing the neighbour region number ids by using knn2nb().\nReturn the length of neighbour relationship edges by using nbdists(). The function returns in the units of the coordinates if the coordinates are projected, in km otherwise.\nRemove the list structure of the returned object by using unlist().\n\n\n\n\n\n\n\nNote\n\n\n\nIn simple terms, the goal of this step is to find the nearest neighbor for each centroid.\n\nThe first step is to identify the point coordinates of this neighbor.\nThe second step is to find the distance between the polygon and its neighbor.\n\nFrom this, we know the largest distance between a polygon and its neighbor. By setting this distance as the cut-off distance in our fixed distance weight matrix, we ensure that each polygon would have at least one neighbor.\n\n\n\nk1 &lt;- knn2nb(knearneigh(coords_hunan))\nk1dists &lt;- unlist(nbdists(k1, coords_hunan, longlat = TRUE))\nsummary(k1dists)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  24.79   32.57   38.01   39.07   44.52   61.79 \n\n\nThe summary report shows that the largest first nearest neighbour distance is 61.79 km, so using this as the upper threshold gives certainty that all units will have at least one neighbour.\n\n\nComputing Fixed Distance Weight Matrix\nNow that we have the cut-off distance, dnearneigh() can be used to compute the distance weight matrix.\n\nwm_d62 &lt;- dnearneigh(coords_hunan, 0, 62, longlat = TRUE)\nwm_d62\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 324 \nPercentage nonzero weights: 4.183884 \nAverage number of links: 3.681818 \n\n\nNext, nb2listw() is used to convert the nb object into spatial weights object.\n\n\n\n\n\n\nNote\n\n\n\nThe ‘B’ or binary style is used here which ascribe the value of 1 to each neighbor.\n\n\n\nwm62_lw &lt;- nb2listw(wm_d62, style = 'B')\nsummary(wm62_lw)\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 324 \nPercentage nonzero weights: 4.183884 \nAverage number of links: 3.681818 \nLink number distribution:\n\n 1  2  3  4  5  6 \n 6 15 14 26 20  7 \n6 least connected regions:\n6 15 30 32 56 65 with 1 link\n7 most connected regions:\n21 28 35 45 50 52 82 with 6 links\n\nWeights style: B \nWeights constants summary:\n   n   nn  S0  S1   S2\nB 88 7744 324 648 5440\n\n\n\n\nComputing Adaptive Distance Weight Matrix\nOne of the characteristics of fixed distance weight matrix is that more densely settled areas (usually the urban areas) tend to have more neighbours and the less densely settled areas (usually the rural counties) tend to have lesser neighbours. Having many neighbours smoothes the neighbour relationship across more neighbours.\nIt is possible to control the numbers of neighbours directly using k-nearest neighbours, either accepting asymmetric neighbours or imposing symmetry as shown in the code chunk below.\n\nknn &lt;- knn2nb(knearneigh(coords_hunan, k = 8))\nknn\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 704 \nPercentage nonzero weights: 9.090909 \nAverage number of links: 8 \nNon-symmetric neighbours list\n\n\nNext, nb2listw() is used to convert the nb object into spatial weights object.\n\nknn_lw &lt;- nb2listw(knn, style = 'B')\nsummary(knn_lw)\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 704 \nPercentage nonzero weights: 9.090909 \nAverage number of links: 8 \nNon-symmetric neighbours list\nLink number distribution:\n\n 8 \n88 \n88 least connected regions:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 with 8 links\n88 most connected regions:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 with 8 links\n\nWeights style: B \nWeights constants summary:\n   n   nn  S0   S1    S2\nB 88 7744 704 1300 23014"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex2/Hands-on_Ex2_Local_Measures.html#computing-gi-statistics",
    "href": "Hands-on_Ex/Hands-on_Ex2/Hands-on_Ex2_Local_Measures.html#computing-gi-statistics",
    "title": "Hands-on_Ex2_Local_Measures",
    "section": "Computing Gi Statistics",
    "text": "Computing Gi Statistics\n\nGi Statistics using Fixed Distance\n\nfips &lt;- order(hunan$County)\ngi.fixed &lt;- localG(hunan$GDPPC, wm62_lw)\ngi.fixed\n\n [1]  0.436075843 -0.265505650 -0.073033665  0.413017033  0.273070579\n [6] -0.377510776  2.863898821  2.794350420  5.216125401  0.228236603\n[11]  0.951035346 -0.536334231  0.176761556  1.195564020 -0.033020610\n[16]  1.378081093 -0.585756761 -0.419680565  0.258805141  0.012056111\n[21] -0.145716531 -0.027158687 -0.318615290 -0.748946051 -0.961700582\n[26] -0.796851342 -1.033949773 -0.460979158 -0.885240161 -0.266671512\n[31] -0.886168613 -0.855476971 -0.922143185 -1.162328599  0.735582222\n[36] -0.003358489 -0.967459309 -1.259299080 -1.452256513 -1.540671121\n[41] -1.395011407 -1.681505286 -1.314110709 -0.767944457 -0.192889342\n[46]  2.720804542  1.809191360 -1.218469473 -0.511984469 -0.834546363\n[51] -0.908179070 -1.541081516 -1.192199867 -1.075080164 -1.631075961\n[56] -0.743472246  0.418842387  0.832943753 -0.710289083 -0.449718820\n[61] -0.493238743 -1.083386776  0.042979051  0.008596093  0.136337469\n[66]  2.203411744  2.690329952  4.453703219 -0.340842743 -0.129318589\n[71]  0.737806634 -1.246912658  0.666667559  1.088613505 -0.985792573\n[76]  1.233609606 -0.487196415  1.626174042 -1.060416797  0.425361422\n[81] -0.837897118 -0.314565243  0.371456331  4.424392623 -0.109566928\n[86]  1.364597995 -1.029658605 -0.718000620\nattr(,\"internals\")\n               Gi      E(Gi)        V(Gi)        Z(Gi) Pr(z != E(Gi))\n [1,] 0.064192949 0.05747126 2.375922e-04  0.436075843   6.627817e-01\n [2,] 0.042300020 0.04597701 1.917951e-04 -0.265505650   7.906200e-01\n [3,] 0.044961480 0.04597701 1.933486e-04 -0.073033665   9.417793e-01\n [4,] 0.039475779 0.03448276 1.461473e-04  0.413017033   6.795941e-01\n [5,] 0.049767939 0.04597701 1.927263e-04  0.273070579   7.847990e-01\n [6,] 0.008825335 0.01149425 4.998177e-05 -0.377510776   7.057941e-01\n [7,] 0.050807266 0.02298851 9.435398e-05  2.863898821   4.184617e-03\n [8,] 0.083966739 0.04597701 1.848292e-04  2.794350420   5.200409e-03\n [9,] 0.115751554 0.04597701 1.789361e-04  5.216125401   1.827045e-07\n[10,] 0.049115587 0.04597701 1.891013e-04  0.228236603   8.194623e-01\n[11,] 0.045819180 0.03448276 1.420884e-04  0.951035346   3.415864e-01\n[12,] 0.049183846 0.05747126 2.387633e-04 -0.536334231   5.917276e-01\n[13,] 0.048429181 0.04597701 1.924532e-04  0.176761556   8.596957e-01\n[14,] 0.034733752 0.02298851 9.651140e-05  1.195564020   2.318667e-01\n[15,] 0.011262043 0.01149425 4.945294e-05 -0.033020610   9.736582e-01\n[16,] 0.065131196 0.04597701 1.931870e-04  1.378081093   1.681783e-01\n[17,] 0.027587075 0.03448276 1.385862e-04 -0.585756761   5.580390e-01\n[18,] 0.029409313 0.03448276 1.461397e-04 -0.419680565   6.747188e-01\n[19,] 0.061466754 0.05747126 2.383385e-04  0.258805141   7.957856e-01\n[20,] 0.057656917 0.05747126 2.371303e-04  0.012056111   9.903808e-01\n[21,] 0.066518379 0.06896552 2.820326e-04 -0.145716531   8.841452e-01\n[22,] 0.045599896 0.04597701 1.928108e-04 -0.027158687   9.783332e-01\n[23,] 0.030646753 0.03448276 1.449523e-04 -0.318615290   7.500183e-01\n[24,] 0.035635552 0.04597701 1.906613e-04 -0.748946051   4.538897e-01\n[25,] 0.032606647 0.04597701 1.932888e-04 -0.961700582   3.362000e-01\n[26,] 0.035001352 0.04597701 1.897172e-04 -0.796851342   4.255374e-01\n[27,] 0.012746354 0.02298851 9.812587e-05 -1.033949773   3.011596e-01\n[28,] 0.061287917 0.06896552 2.773884e-04 -0.460979158   6.448136e-01\n[29,] 0.014277403 0.02298851 9.683314e-05 -0.885240161   3.760271e-01\n[30,] 0.009622875 0.01149425 4.924586e-05 -0.266671512   7.897221e-01\n[31,] 0.014258398 0.02298851 9.705244e-05 -0.886168613   3.755267e-01\n[32,] 0.005453443 0.01149425 4.986245e-05 -0.855476971   3.922871e-01\n[33,] 0.043283712 0.05747126 2.367109e-04 -0.922143185   3.564539e-01\n[34,] 0.020763514 0.03448276 1.393165e-04 -1.162328599   2.451020e-01\n[35,] 0.081261843 0.06896552 2.794398e-04  0.735582222   4.619850e-01\n[36,] 0.057419907 0.05747126 2.338437e-04 -0.003358489   9.973203e-01\n[37,] 0.013497133 0.02298851 9.624821e-05 -0.967459309   3.333145e-01\n[38,] 0.019289310 0.03448276 1.455643e-04 -1.259299080   2.079223e-01\n[39,] 0.025996272 0.04597701 1.892938e-04 -1.452256513   1.464303e-01\n[40,] 0.016092694 0.03448276 1.424776e-04 -1.540671121   1.233968e-01\n[41,] 0.035952614 0.05747126 2.379439e-04 -1.395011407   1.630124e-01\n[42,] 0.031690963 0.05747126 2.350604e-04 -1.681505286   9.266481e-02\n[43,] 0.018750079 0.03448276 1.433314e-04 -1.314110709   1.888090e-01\n[44,] 0.015449080 0.02298851 9.638666e-05 -0.767944457   4.425202e-01\n[45,] 0.065760689 0.06896552 2.760533e-04 -0.192889342   8.470456e-01\n[46,] 0.098966900 0.05747126 2.326002e-04  2.720804542   6.512325e-03\n[47,] 0.085415780 0.05747126 2.385746e-04  1.809191360   7.042128e-02\n[48,] 0.038816536 0.05747126 2.343951e-04 -1.218469473   2.230456e-01\n[49,] 0.038931873 0.04597701 1.893501e-04 -0.511984469   6.086619e-01\n[50,] 0.055098610 0.06896552 2.760948e-04 -0.834546363   4.039732e-01\n[51,] 0.033405005 0.04597701 1.916312e-04 -0.908179070   3.637836e-01\n[52,] 0.043040784 0.06896552 2.829941e-04 -1.541081516   1.232969e-01\n[53,] 0.011297699 0.02298851 9.615920e-05 -1.192199867   2.331829e-01\n[54,] 0.040968457 0.05747126 2.356318e-04 -1.075080164   2.823388e-01\n[55,] 0.023629663 0.04597701 1.877170e-04 -1.631075961   1.028743e-01\n[56,] 0.006281129 0.01149425 4.916619e-05 -0.743472246   4.571958e-01\n[57,] 0.063918654 0.05747126 2.369553e-04  0.418842387   6.753313e-01\n[58,] 0.070325003 0.05747126 2.381374e-04  0.832943753   4.048765e-01\n[59,] 0.025947288 0.03448276 1.444058e-04 -0.710289083   4.775249e-01\n[60,] 0.039752578 0.04597701 1.915656e-04 -0.449718820   6.529132e-01\n[61,] 0.049934283 0.05747126 2.334965e-04 -0.493238743   6.218439e-01\n[62,] 0.030964195 0.04597701 1.920248e-04 -1.083386776   2.786368e-01\n[63,] 0.058129184 0.05747126 2.343319e-04  0.042979051   9.657182e-01\n[64,] 0.046096514 0.04597701 1.932637e-04  0.008596093   9.931414e-01\n[65,] 0.012459080 0.01149425 5.008051e-05  0.136337469   8.915545e-01\n[66,] 0.091447733 0.05747126 2.377744e-04  2.203411744   2.756574e-02\n[67,] 0.049575872 0.02298851 9.766513e-05  2.690329952   7.138140e-03\n[68,] 0.107907212 0.04597701 1.933581e-04  4.453703219   8.440175e-06\n[69,] 0.019616151 0.02298851 9.789454e-05 -0.340842743   7.332220e-01\n[70,] 0.032923393 0.03448276 1.454032e-04 -0.129318589   8.971056e-01\n[71,] 0.030317663 0.02298851 9.867859e-05  0.737806634   4.606320e-01\n[72,] 0.019437582 0.03448276 1.455870e-04 -1.246912658   2.124295e-01\n[73,] 0.055245460 0.04597701 1.932838e-04  0.666667559   5.049845e-01\n[74,] 0.074278054 0.05747126 2.383538e-04  1.088613505   2.763244e-01\n[75,] 0.013269580 0.02298851 9.719982e-05 -0.985792573   3.242349e-01\n[76,] 0.049407829 0.03448276 1.463785e-04  1.233609606   2.173484e-01\n[77,] 0.028605749 0.03448276 1.455139e-04 -0.487196415   6.261191e-01\n[78,] 0.039087662 0.02298851 9.801040e-05  1.626174042   1.039126e-01\n[79,] 0.031447120 0.04597701 1.877464e-04 -1.060416797   2.889550e-01\n[80,] 0.064005294 0.05747126 2.359641e-04  0.425361422   6.705732e-01\n[81,] 0.044606529 0.05747126 2.357330e-04 -0.837897118   4.020885e-01\n[82,] 0.063700493 0.06896552 2.801427e-04 -0.314565243   7.530918e-01\n[83,] 0.051142205 0.04597701 1.933560e-04  0.371456331   7.102977e-01\n[84,] 0.102121112 0.04597701 1.610278e-04  4.424392623   9.671399e-06\n[85,] 0.021901462 0.02298851 9.843172e-05 -0.109566928   9.127528e-01\n[86,] 0.064931813 0.04597701 1.929430e-04  1.364597995   1.723794e-01\n[87,] 0.031747344 0.04597701 1.909867e-04 -1.029658605   3.031703e-01\n[88,] 0.015893319 0.02298851 9.765131e-05 -0.718000620   4.727569e-01\nattr(,\"cluster\")\n [1] Low  Low  High High High High High High High Low  Low  High Low  Low  Low \n[16] High High High High Low  High High Low  Low  High Low  Low  Low  Low  Low \n[31] Low  Low  Low  High Low  Low  Low  Low  Low  Low  High Low  Low  Low  Low \n[46] High High Low  Low  Low  Low  High Low  Low  Low  Low  Low  High Low  Low \n[61] Low  Low  Low  High High High Low  High Low  Low  High Low  High High Low \n[76] High Low  Low  Low  Low  Low  Low  High High Low  High Low  Low \nLevels: Low High\nattr(,\"gstari\")\n[1] FALSE\nattr(,\"call\")\nlocalG(x = hunan$GDPPC, listw = wm62_lw)\nattr(,\"class\")\n[1] \"localG\"\n\n\nThe output of localG() is a vector of G or Gstart values, with attributes “gstari” set to TRUE of FALSE, “call” set to the function call, and class “localG”.\nThe Gi statistics is represented as a Z-score. Greater values represent a greater intensity of clustering and the direction (positive or negative) indicates high or low clusters.\nNext, we will join the Gi values to their corresponding county in hunan sf data frame.\n\nhunan.gi &lt;- cbind(hunan, as.matrix(gi.fixed)) %&gt;%\n  rename(gstat_fixed = as.matrix.gi.fixed.)\n\nThe code chunk above performs three tasks:\n\nConvert the output vector (gi.fixed) into a matrix object\nCombine hunan and gi.fixed to produce hunan.gi sf dataframe\nChange the field name of the gi values to gstat_fixed\n\n\n\nMapping Gi values with Fixed Distance Weights\nTo map the Gi values dervided using fixed distance weight matrix, simply use the tmap package and change the col argument of tm_fill()\n\ngdppc &lt;- qtm(hunan, fill = 'GDPPC')\n\nGimap &lt;- tm_shape(hunan.gi)+\n  tm_fill(col='gstat_fixed',\n          style = 'pretty',\n          palette = '-RdBu',\n          title = 'Local Gi')+\n  tm_borders(alpha = 0.5)\n\ntmap_arrange(gdppc, Gimap, asp = 1, ncol = 2)\n\n\n\n\n\n\nGi Statistics using Adaptive Distance\nTo calculate Gi statistic using adaptive distance, simply replace the weight list in localG with the adaptive distance weight list.\n\nfips &lt;- order(hunan$County)\ngi.adaptive &lt;- localG(hunan$GDPPC,knn_lw)\nhunan.gi &lt;- cbind(hunan, as.matrix(gi.adaptive)) %&gt;%\n  rename(gstat_adaptive = as.matrix.gi.adaptive.)\n\n\n\nMapping Gi values with Adaptive Distance Weights\n\ngdppc &lt;- qtm(hunan, 'GDPPC')\n\nGimap &lt;- tm_shape(hunan.gi)+\n  tm_fill(col='gstat_adaptive',\n          style='pretty',\n          palette ='-RdBu',\n          title = 'Local Gi')+\n  tm_borders(alpha = 0.5)\n\ntmap_arrange(gdppc, Gimap, asp = 1, ncol = 2)\n\n\n\n\nQuestion: What statistical observation can you draw from the Gi map above?\nAnswer: The Gi statistic calculated from the adaptive weight matrix display a higher value for counties on the right edge. Notably, the Gi values seems to be reversed between the fixed distance weights and adaptive distance weights list."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex3/data/geospatial/MPSZ-2019.html",
    "href": "Hands-on_Ex/Hands-on_Ex3/data/geospatial/MPSZ-2019.html",
    "title": "ISSS624",
    "section": "",
    "text": "&lt;!DOCTYPE qgis PUBLIC ‘http://mrcc.com/qgis.dtd’ ‘SYSTEM’&gt;     dataset\n\n\n        0 0     false"
  },
  {
    "objectID": "In-Class_Ex/In-Class_Ex1/data/geospatial/MPSZ-2019.html",
    "href": "In-Class_Ex/In-Class_Ex1/data/geospatial/MPSZ-2019.html",
    "title": "ISSS624",
    "section": "",
    "text": "&lt;!DOCTYPE qgis PUBLIC ‘http://mrcc.com/qgis.dtd’ ‘SYSTEM’&gt;     dataset\n\n\n        0 0     false"
  },
  {
    "objectID": "In-Class_Ex/In-Class_Ex2/In-class_Ex2.html",
    "href": "In-Class_Ex/In-Class_Ex2/In-class_Ex2.html",
    "title": "In-class Exercise 2",
    "section": "",
    "text": "Loading R packages\n\npacman::p_load(sf, sfdep, tmap, tidyverse, knitr)\n\n\n\n\nFor this In-class Exercise, the Hunan data sets will be used. They are:\n\nhunan: a geographical data set in ESRI shapefile format\nHunan_2012: an attribute data set in csv format\n\n\n\n\nst_read() can be used to read the shape file data set into an R sf dataframe\n\nhunan &lt;- st_read(dsn = 'data/geospatial',\n                 layer = 'Hunan')\n\nReading layer `Hunan' from data source \n  `D:\\phlong2023\\ISSS624\\In-Class_Ex\\In-Class_Ex2\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 88 features and 7 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 108.7831 ymin: 24.6342 xmax: 114.2544 ymax: 30.12812\nGeodetic CRS:  WGS 84\n\n\n\n\n\nread_csv() can be used to read the attribute file into an R data frame\n\nhunan_2012 &lt;- read_csv('data/aspatial/Hunan_2012.csv')\n\n\n\n\nleft_join() can be used to combine the two data sets\n\n\n\n\n\n\nNote\n\n\n\nIn order to retain the geospatial properties, the left data frame must be sf data.frame, in this case it is hunan\n\n\n\nhunan_GDPPC &lt;- left_join(hunan, hunan_2012,\n                            by = 'County')%&gt;%\n  select(1:4, 7, 15) #Retaining the city's name, ID, county name, county type, GDPPC, and geometry"
  },
  {
    "objectID": "In-Class_Ex/In-Class_Ex2/In-class_Ex2.html#getting-started",
    "href": "In-Class_Ex/In-Class_Ex2/In-class_Ex2.html#getting-started",
    "title": "In-class Exercise 2",
    "section": "",
    "text": "Loading R packages\n\npacman::p_load(sf, sfdep, tmap, tidyverse, knitr)\n\n\n\n\nFor this In-class Exercise, the Hunan data sets will be used. They are:\n\nhunan: a geographical data set in ESRI shapefile format\nHunan_2012: an attribute data set in csv format\n\n\n\n\nst_read() can be used to read the shape file data set into an R sf dataframe\n\nhunan &lt;- st_read(dsn = 'data/geospatial',\n                 layer = 'Hunan')\n\nReading layer `Hunan' from data source \n  `D:\\phlong2023\\ISSS624\\In-Class_Ex\\In-Class_Ex2\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 88 features and 7 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 108.7831 ymin: 24.6342 xmax: 114.2544 ymax: 30.12812\nGeodetic CRS:  WGS 84\n\n\n\n\n\nread_csv() can be used to read the attribute file into an R data frame\n\nhunan_2012 &lt;- read_csv('data/aspatial/Hunan_2012.csv')\n\n\n\n\nleft_join() can be used to combine the two data sets\n\n\n\n\n\n\nNote\n\n\n\nIn order to retain the geospatial properties, the left data frame must be sf data.frame, in this case it is hunan\n\n\n\nhunan_GDPPC &lt;- left_join(hunan, hunan_2012,\n                            by = 'County')%&gt;%\n  select(1:4, 7, 15) #Retaining the city's name, ID, county name, county type, GDPPC, and geometry"
  },
  {
    "objectID": "In-Class_Ex/In-Class_Ex2/In-class_Ex2.html#deriving-contiguity-weights-queens-model",
    "href": "In-Class_Ex/In-Class_Ex2/In-class_Ex2.html#deriving-contiguity-weights-queens-model",
    "title": "In-class Exercise 2",
    "section": "Deriving Contiguity Weights: Queen’s Model",
    "text": "Deriving Contiguity Weights: Queen’s Model\nThe sfdep method entails the creation of a tibble data frame which contains the original data as well as the neighbors list and weights for each polygon , as opposed to creating the contiguity and weight separately in spdep.\n\nwm_q &lt;- hunan_GDPPC %&gt;%\n  mutate(nb = st_contiguity(geometry), # Default is Queen\n         wt = st_weights(nb,\n                         style='W'),\n         .before = 1)"
  },
  {
    "objectID": "In-Class_Ex/In-Class_Ex2/In-class_Ex2.html#computing-global-morans-i-old-spdep-method",
    "href": "In-Class_Ex/In-Class_Ex2/In-class_Ex2.html#computing-global-morans-i-old-spdep-method",
    "title": "In-class Exercise 2",
    "section": "Computing Global Moran’s I (old spdep method)",
    "text": "Computing Global Moran’s I (old spdep method)\n\nmoranI &lt;- global_moran(wm_q$GDPPC,\n                       wm_q$nb,\n                       wm_q$wt)\n\nmoranI\n\n$I\n[1] 0.30075\n\n$K\n[1] 7.640659"
  },
  {
    "objectID": "In-Class_Ex/In-Class_Ex2/In-class_Ex2.html#computing-local-morans-i-with-sfdep-method",
    "href": "In-Class_Ex/In-Class_Ex2/In-class_Ex2.html#computing-local-morans-i-with-sfdep-method",
    "title": "In-class Exercise 2",
    "section": "Computing Local Moran’s I (with sfdep method)",
    "text": "Computing Local Moran’s I (with sfdep method)\n\nlisa &lt;- wm_q %&gt;%\n  mutate(local_moran = local_moran(GDPPC, nb, wt, nsim = 99),\n         .before = 1) %&gt;%\n  unnest(local_moran)"
  },
  {
    "objectID": "In-Class_Ex/In-Class_Ex2/In-class_Ex2_GLSA.html",
    "href": "In-Class_Ex/In-Class_Ex2/In-class_Ex2_GLSA.html",
    "title": "In-class Exercise 2",
    "section": "",
    "text": "Loading R packages\n\npacman::p_load(sf, sfdep, tmap, tidyverse, knitr)\n\n\n\n\nFor this In-class Exercise, the Hunan data sets will be used. They are:\n\nhunan: a geographical data set in ESRI shapefile format\nHunan_2012: an attribute data set in csv format\n\n\n\n\nst_read() can be used to read the shape file data set into an R sf dataframe\n\nhunan &lt;- st_read(dsn = 'data/geospatial',\n                 layer = 'Hunan')\n\nReading layer `Hunan' from data source \n  `D:\\phlong2023\\ISSS624\\In-Class_Ex\\In-Class_Ex2\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 88 features and 7 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 108.7831 ymin: 24.6342 xmax: 114.2544 ymax: 30.12812\nGeodetic CRS:  WGS 84\n\n\n\n\n\nread_csv() can be used to read the attribute file into an R data frame\n\nhunan_2012 &lt;- read_csv('data/aspatial/Hunan_2012.csv')\n\n\n\n\nleft_join() can be used to combine the two data sets\n\n\n\n\n\n\nNote\n\n\n\nIn order to retain the geospatial properties, the left data frame must be sf data.frame, in this case it is hunan\n\n\n\nhunan_GDPPC &lt;- left_join(hunan, hunan_2012,\n                            by = 'County')%&gt;%\n  select(1:4, 7, 15) #Retaining the city's name, ID, county name, county type, GDPPC, and geometry"
  },
  {
    "objectID": "In-Class_Ex/In-Class_Ex2/In-class_Ex2_GLSA.html#getting-started",
    "href": "In-Class_Ex/In-Class_Ex2/In-class_Ex2_GLSA.html#getting-started",
    "title": "In-class Exercise 2",
    "section": "",
    "text": "Loading R packages\n\npacman::p_load(sf, sfdep, tmap, tidyverse, knitr)\n\n\n\n\nFor this In-class Exercise, the Hunan data sets will be used. They are:\n\nhunan: a geographical data set in ESRI shapefile format\nHunan_2012: an attribute data set in csv format\n\n\n\n\nst_read() can be used to read the shape file data set into an R sf dataframe\n\nhunan &lt;- st_read(dsn = 'data/geospatial',\n                 layer = 'Hunan')\n\nReading layer `Hunan' from data source \n  `D:\\phlong2023\\ISSS624\\In-Class_Ex\\In-Class_Ex2\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 88 features and 7 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 108.7831 ymin: 24.6342 xmax: 114.2544 ymax: 30.12812\nGeodetic CRS:  WGS 84\n\n\n\n\n\nread_csv() can be used to read the attribute file into an R data frame\n\nhunan_2012 &lt;- read_csv('data/aspatial/Hunan_2012.csv')\n\n\n\n\nleft_join() can be used to combine the two data sets\n\n\n\n\n\n\nNote\n\n\n\nIn order to retain the geospatial properties, the left data frame must be sf data.frame, in this case it is hunan\n\n\n\nhunan_GDPPC &lt;- left_join(hunan, hunan_2012,\n                            by = 'County')%&gt;%\n  select(1:4, 7, 15) #Retaining the city's name, ID, county name, county type, GDPPC, and geometry"
  },
  {
    "objectID": "In-Class_Ex/In-Class_Ex2/In-class_Ex2_GLSA.html#deriving-contiguity-weights-queens-model",
    "href": "In-Class_Ex/In-Class_Ex2/In-class_Ex2_GLSA.html#deriving-contiguity-weights-queens-model",
    "title": "In-class Exercise 2",
    "section": "Deriving Contiguity Weights: Queen’s Model",
    "text": "Deriving Contiguity Weights: Queen’s Model\nThe sfdep method entails the creation of a tibble data frame which contains the original data as well as the neighbors list and weights for each polygon , as opposed to creating the contiguity and weight separately in spdep.\n\nwm_q &lt;- hunan_GDPPC %&gt;%\n  mutate(nb = st_contiguity(geometry), # Default is Queen\n         wt = st_weights(nb,\n                         style='W'),\n         .before = 1)"
  },
  {
    "objectID": "In-Class_Ex/In-Class_Ex2/In-class_Ex2_GLSA.html#computing-global-morans-i-old-spdep-method",
    "href": "In-Class_Ex/In-Class_Ex2/In-class_Ex2_GLSA.html#computing-global-morans-i-old-spdep-method",
    "title": "In-class Exercise 2",
    "section": "Computing Global Moran’s I (old spdep method)",
    "text": "Computing Global Moran’s I (old spdep method)\n\nmoranI &lt;- global_moran(wm_q$GDPPC,\n                       wm_q$nb,\n                       wm_q$wt)\n\nmoranI\n\n$I\n[1] 0.30075\n\n$K\n[1] 7.640659"
  },
  {
    "objectID": "In-Class_Ex/In-Class_Ex2/In-class_Ex2_GLSA.html#computing-local-morans-i-with-sfdep-method",
    "href": "In-Class_Ex/In-Class_Ex2/In-class_Ex2_GLSA.html#computing-local-morans-i-with-sfdep-method",
    "title": "In-class Exercise 2",
    "section": "Computing Local Moran’s I (with sfdep method)",
    "text": "Computing Local Moran’s I (with sfdep method)\n\nlisa &lt;- wm_q %&gt;%\n  mutate(local_moran = local_moran(GDPPC, nb, wt, nsim = 99),\n         .before = 1) %&gt;%\n  unnest(local_moran)"
  },
  {
    "objectID": "In-Class_Ex/In-Class_Ex3/In-Class_Ex3.html",
    "href": "In-Class_Ex/In-Class_Ex3/In-Class_Ex3.html",
    "title": "In-class_Ex3",
    "section": "",
    "text": "pacman::p_load(tmap, sf, sp, DT, performance, reshape2, ggpubr, units, tidyverse)"
  },
  {
    "objectID": "In-Class_Ex/In-Class_Ex3/In-Class_Ex3.html#getting-started",
    "href": "In-Class_Ex/In-Class_Ex3/In-Class_Ex3.html#getting-started",
    "title": "In-class_Ex3",
    "section": "",
    "text": "pacman::p_load(tmap, sf, sp, DT, performance, reshape2, ggpubr, units, tidyverse)"
  },
  {
    "objectID": "In-Class_Ex/In-Class_Ex3/In-Class_Ex3.html#the-data",
    "href": "In-Class_Ex/In-Class_Ex3/In-Class_Ex3.html#the-data",
    "title": "In-class_Ex3",
    "section": "The Data",
    "text": "The Data\nThe following data will be used, as a continuation of Hands-on_Ex3:\n\nod_data.rds: weekday morning peak passenger flows at planning subzone level.\nmpsz.rds: URA Master Plan 2019 Planning Subzone boundary in simple feature tibble data frame format.\n\nBeside these two data sets, an additional attribute data file called pop.csv will be provided.\n\nImporting Geospatial Data\n\nmpsz &lt;- read_rds('data/rds/mpsz.rds')\n\n\n\nConverting from sf data.table to SpatialPolygonsDataFrame\nThis is a way to convert a sf data.table to SpatialPolygonsDataFrame. However, the sf data.table is still the preferred format for analysis.\n\n# Untidy way\nmpsz_sp &lt;- as(mpsz, 'Spatial')"
  },
  {
    "objectID": "In-Class_Ex/In-Class_Ex3/In-Class_Ex3.html#distance-matrix-between-centroids-of-each-zone.",
    "href": "In-Class_Ex/In-Class_Ex3/In-Class_Ex3.html#distance-matrix-between-centroids-of-each-zone.",
    "title": "In-class_Ex3",
    "section": "Distance matrix between centroids of each zone.",
    "text": "Distance matrix between centroids of each zone.\nspDists() of sp can be used to calculate the cent\n\ndist &lt;- spDists(mpsz_sp, longlat = FALSE)\n\n\nLabelling columns and row headers of distance matrix\n\nsz_names &lt;- mpsz$SUBZONE_C\n\nNext we will attach SUBZONE_C names to the rows and columns of the distance matrix.\n\ncolnames(dist) &lt;- paste0(sz_names)\nrownames(dist) &lt;- paste0(sz_names)\n\n\n\nPivoting distance value by SUBZONE_C\nNext, we will pivot the distance matrix into a long table by using melt.\n\ndistPair &lt;- melt(dist) %&gt;%\n  rename(dist = value)\n\n\n\nUpdating intra-zonal distances\n\ndistPair %&gt;%\n  filter(dist &gt; 0) %&gt;%\n  summary()\n\n      Var1             Var2             dist        \n MESZ01 :   331   MESZ01 :   331   Min.   :  173.8  \n RVSZ05 :   331   RVSZ05 :   331   1st Qu.: 7149.5  \n SRSZ01 :   331   SRSZ01 :   331   Median :11890.0  \n WISZ01 :   331   WISZ01 :   331   Mean   :12229.4  \n MUSZ02 :   331   MUSZ02 :   331   3rd Qu.:16401.7  \n MPSZ05 :   331   MPSZ05 :   331   Max.   :49894.4  \n (Other):107906   (Other):107906                    \n\n\nSince Min inter-zonal distances is 173, we can roughly estimate the intra-zonal distance (distance between the centroid and the polygon’s boundary).\n\ndistPair$dist &lt;- ifelse(distPair$dist == 0,\n                        50, distPair$dist)\n\nWe can also rename the column names in the table for easier manipulation further on.\n\ndistPair &lt;- distPair %&gt;%\n  rename(orig = Var1,\n         dest = Var2)\n\nWe can save distPair as an rds file for future use.\n\nwrite_rds(distPair, 'data/rds/distPair.rds')"
  },
  {
    "objectID": "In-Class_Ex/In-Class_Ex3/In-Class_Ex3.html#preparing-flow-data",
    "href": "In-Class_Ex/In-Class_Ex3/In-Class_Ex3.html#preparing-flow-data",
    "title": "In-class_Ex3",
    "section": "Preparing flow data",
    "text": "Preparing flow data\nWe can import od_data.rds created previously into R using read_rds().\n\nod_data &lt;- read_rds('data/rds/od_data.rds')\n\nNext, the total passenger trip between and within planning subzones will be calculated.\n\nflow_data &lt;- od_data %&gt;%\n  group_by(ORIGIN_SZ, DESTIN_SZ) %&gt;%\n  summarize(TRIPS = sum(MORNING_PEAK))\n\nWe can see the top 5 rows of flow_data using head()\n\nhead(flow_data)\n\n# A tibble: 6 × 3\n# Groups:   ORIGIN_SZ [1]\n  ORIGIN_SZ DESTIN_SZ TRIPS\n  &lt;chr&gt;     &lt;chr&gt;     &lt;dbl&gt;\n1 AMSZ01    AMSZ01     2694\n2 AMSZ01    AMSZ02    10591\n3 AMSZ01    AMSZ03    14980\n4 AMSZ01    AMSZ04     3106\n5 AMSZ01    AMSZ05     7734\n6 AMSZ01    AMSZ06     2306\n\n\n\nSeparating intra-flow from passenger volume df\nWe can add three new fields into flow_data:\n\nFlowNoIntra: Number of trips, excluding intra-zonal trips.\noffset: A neglible value used for later analysis to offset intra-zonal trips.\n\n\nflow_data$FlowNoIntra &lt;- ifelse(flow_data$ORIGIN_SZ == flow_data$DESTIN_SZ, 0, flow_data$TRIPS)\n\nflow_data$offset &lt;- ifelse(flow_data$ORIGIN_SZ == flow_data$DESTIN_SZ, 0.000001, 1)\n\n\n\nCombining passenger volume data with distance value\nNow that we have the distance between the subzones (distPair) and the number of trips between them (flow_data), we can try to combine them into one data frame.\nBefore we can join flow_data and distPair, we need to convert data value type of ORIGIN_SZ and DESTIN_SZ of flow_data into factor type.\n\nflow_data$ORIGIN_SZ &lt;- as.factor(flow_data$ORIGIN_SZ)\nflow_data$DESTIN_SZ &lt;- as.factor(flow_data$DESTIN_SZ)\n\nleft_join() can be used to combine flow_data and distPair\n\nflow_data1 &lt;- flow_data %&gt;%\n  left_join(distPair,\n            by = c('ORIGIN_SZ' = 'orig',\n                   'DESTIN_SZ' = 'dest'))"
  },
  {
    "objectID": "In-Class_Ex/In-Class_Ex3/In-Class_Ex3.html#preparing-origin-and-destination-attributes",
    "href": "In-Class_Ex/In-Class_Ex3/In-Class_Ex3.html#preparing-origin-and-destination-attributes",
    "title": "In-class_Ex3",
    "section": "Preparing Origin and Destination Attributes",
    "text": "Preparing Origin and Destination Attributes\n\nImporting Population Data\n\npop &lt;- read_csv('data/aspatial/pop.csv')\n\n\n\nGeospatial Data Wrangling\n\npop &lt;- pop %&gt;%\n  left_join(mpsz,\n            by = c('PA' = 'PLN_AREA_N',\n                   'SZ' = 'SUBZONE_N')) %&gt;%\n  select(1:6) %&gt;%\n  rename(SZ_NAME = SZ,\n         SZ = SUBZONE_C)\n\n\n\nPreparing Origin Attribute\n\nflow_data1 &lt;- flow_data1 %&gt;%\n  left_join(pop,\n            by = c(ORIGIN_SZ = 'SZ')) %&gt;%\n  rename(ORIGIN_AGE7_12 = AGE7_12,\n         ORIGIN_AGE13_24 = AGE13_24,\n         ORIGIN_AGE25_64 = AGE25_64) %&gt;%\n  select(-c(PA, SZ_NAME))\n\n\n\nPreparing Destination Attribute\n\nflow_data1 &lt;- flow_data1 %&gt;%\n  left_join(pop,\n            by = c(DESTIN_SZ = 'SZ')) %&gt;%\n  rename(DESTIN_AGE7_12 = AGE7_12,\n         DESTIN_AGE13_24 = AGE13_24,\n         DESTIN_AGE25_64 = AGE25_64) %&gt;%\n  select(-c(PA, SZ_NAME))"
  },
  {
    "objectID": "In-Class_Ex/In-Class_Ex3/In-Class_Ex3.html#calibrating-spatial-interaction-models",
    "href": "In-Class_Ex/In-Class_Ex3/In-Class_Ex3.html#calibrating-spatial-interaction-models",
    "title": "In-class_Ex3",
    "section": "Calibrating Spatial Interaction Models",
    "text": "Calibrating Spatial Interaction Models\nWe will use the Poisson Regression Method to calibrate Spatial Interaction Models\n\nImporting the Modelling Data\n\nSIM_data &lt;- read_rds('data/rds/SIM_data.rds')\n\n\n\nVisualizing the Dependent Variable\nLet’s plot the distribution of the dependent variable (i.e. TRIPS) by making a histogram with ggplot2.\n\nggplot(SIM_data,\n       aes(x=TRIPS))+\n  geom_histogram()\n\n\n\n\nThe distribution is highly skewed and does not resemble a bell shape, also known as the normal distribution.\nNow, we can visualize the relation between the dependent variable and one of the key independent variable (distance between zones) in the Spatial Interaction Mode.\n\nggplot(SIM_data,\n       aes(x = dist,\n           y = TRIPS))+\n  geom_point()+\n  geom_smooth(method=lm)\n\n\n\n\nThe relationship does not resemble a linear relationship.\nHowever, if we create a scatter plot using the log transformed version of these variables, the relationship will better resemble a linear relationship.\n\nggplot(SIM_data,\n       aes(x = log(dist),\n           y = log(TRIPS)))+\n  geom_point()+\n  geom_smooth(method = lm)\n\n\n\n\n\n\nChecking for Variables with Zero Values\nSince Poisson Regression is based of log and log 0 is undefined, it is important for us to ensure that there is no 0 in the explanatory variables.\nsummary() can be used to compute the summary statistics of all variables in the data frame.\n\nsummary(SIM_data)\n\n  ORIGIN_SZ          DESTIN_SZ             TRIPS           FlowNoIntra      \n Length:14274       Length:14274       Min.   :     1.0   Min.   :     1.0  \n Class :character   Class :character   1st Qu.:    11.0   1st Qu.:    11.0  \n Mode  :character   Mode  :character   Median :    56.0   Median :    56.0  \n                                       Mean   :   664.3   Mean   :   664.3  \n                                       3rd Qu.:   296.0   3rd Qu.:   296.0  \n                                       Max.   :104167.0   Max.   :104167.0  \n     offset       dist         ORIGIN_AGE7_12 ORIGIN_AGE13_24 ORIGIN_AGE25_64\n Min.   :1   Min.   :  173.8   Min.   :   0   Min.   :    0   Min.   :    0  \n 1st Qu.:1   1st Qu.: 3465.4   1st Qu.: 240   1st Qu.:  460   1st Qu.: 2210  \n Median :1   Median : 6121.0   Median : 710   Median : 1400   Median : 7030  \n Mean   :1   Mean   : 6951.8   Mean   :1037   Mean   : 2278   Mean   :10536  \n 3rd Qu.:1   3rd Qu.: 9725.1   3rd Qu.:1500   3rd Qu.: 3282   3rd Qu.:15830  \n Max.   :1   Max.   :26135.8   Max.   :6340   Max.   :16380   Max.   :74610  \n DESTIN_AGE7_12 DESTIN_AGE13_24 DESTIN_AGE25_64\n Min.   :   0   Min.   :    0   Min.   :    0  \n 1st Qu.: 250   1st Qu.:  460   1st Qu.: 2210  \n Median : 720   Median : 1430   Median : 7120  \n Mean   :1040   Mean   : 2305   Mean   :10648  \n 3rd Qu.:1500   3rd Qu.: 3290   3rd Qu.:15830  \n Max.   :6340   Max.   :16380   Max.   :74610  \n\n\nThe print report above reveals that variables ORIGIN_AGE7_12, ORIGIN_AGE13_24, ORIGIN_AGE25_64,DESTIN_AGE7_12, DESTIN_AGE13_24, DESTIN_AGE25_64 consist of 0 values.\nWe can use ifelse() functions to replace 0 values to 0.99 so that the log function would not result in minus infinity (for 0) or 0 (for 1).\n\nSIM_data$ORIGIN_AGE7_12 &lt;- ifelse(SIM_data$ORIGIN_AGE7_12 == 0, 0.99, SIM_data$ORIGIN_AGE7_12)\n\nSIM_data$ORIGIN_AGE13_24 &lt;- ifelse(SIM_data$ORIGIN_AGE13_24 == 0, 0.99, SIM_data$ORIGIN_AGE13_24)\n\nSIM_data$ORIGIN_AGE25_64 &lt;- ifelse(SIM_data$ORIGIN_AGE25_64 == 0, 0.99, SIM_data$ORIGIN_AGE25_64)\n\nSIM_data$DESTIN_AGE7_12 &lt;- ifelse(SIM_data$DESTIN_AGE7_12 == 0, 0.99, SIM_data$DESTIN_AGE7_12)\n\nSIM_data$DESTIN_AGE13_24 &lt;- ifelse(SIM_data$DESTIN_AGE13_24 == 0, 0.99, SIM_data$DESTIN_AGE13_24)\n\nSIM_data$DESTIN_AGE25_64 &lt;- ifelse(SIM_data$DESTIN_AGE25_64 == 0, 0.99, SIM_data$DESTIN_AGE25_64)\n\nLet’s check the summary() again.\n\nsummary(SIM_data)\n\n  ORIGIN_SZ          DESTIN_SZ             TRIPS           FlowNoIntra      \n Length:14274       Length:14274       Min.   :     1.0   Min.   :     1.0  \n Class :character   Class :character   1st Qu.:    11.0   1st Qu.:    11.0  \n Mode  :character   Mode  :character   Median :    56.0   Median :    56.0  \n                                       Mean   :   664.3   Mean   :   664.3  \n                                       3rd Qu.:   296.0   3rd Qu.:   296.0  \n                                       Max.   :104167.0   Max.   :104167.0  \n     offset       dist         ORIGIN_AGE7_12    ORIGIN_AGE13_24   \n Min.   :1   Min.   :  173.8   Min.   :   0.99   Min.   :    0.99  \n 1st Qu.:1   1st Qu.: 3465.4   1st Qu.: 240.00   1st Qu.:  460.00  \n Median :1   Median : 6121.0   Median : 710.00   Median : 1400.00  \n Mean   :1   Mean   : 6951.8   Mean   :1036.73   Mean   : 2278.59  \n 3rd Qu.:1   3rd Qu.: 9725.1   3rd Qu.:1500.00   3rd Qu.: 3282.50  \n Max.   :1   Max.   :26135.8   Max.   :6340.00   Max.   :16380.00  \n ORIGIN_AGE25_64    DESTIN_AGE7_12    DESTIN_AGE13_24    DESTIN_AGE25_64   \n Min.   :    0.99   Min.   :   0.99   Min.   :    0.99   Min.   :    0.99  \n 1st Qu.: 2210.00   1st Qu.: 250.00   1st Qu.:  460.00   1st Qu.: 2210.00  \n Median : 7030.00   Median : 720.00   Median : 1430.00   Median : 7120.00  \n Mean   :10535.93   Mean   :1039.98   Mean   : 2305.33   Mean   :10647.95  \n 3rd Qu.:15830.00   3rd Qu.:1500.00   3rd Qu.: 3290.00   3rd Qu.:15830.00  \n Max.   :74610.00   Max.   :6340.00   Max.   :16380.00   Max.   :74610.00  \n\n\nAll 0 values have been replaced by 0.99\n\n\nUnconstrained Spatial Interaction Model\nglm() can be used to calibrate an unconstrained spatial interaction model. Our explanatory variables are origin population by different age cohort, destination population by age cohort and distance between origin and destination in km (i.e. dist).\n\nuncSIM &lt;- glm(formula = TRIPS ~ \n                log(ORIGIN_AGE25_64)+\n                log(DESTIN_AGE25_64)+\n                log(dist),\n              family = poisson(link = 'log'),\n              data = SIM_data,\n              na.action = na.exclude)\n\nuncSIM\n\n\nCall:  glm(formula = TRIPS ~ log(ORIGIN_AGE25_64) + log(DESTIN_AGE25_64) + \n    log(dist), family = poisson(link = \"log\"), data = SIM_data, \n    na.action = na.exclude)\n\nCoefficients:\n         (Intercept)  log(ORIGIN_AGE25_64)  log(DESTIN_AGE25_64)  \n            17.00287               0.21001               0.01289  \n           log(dist)  \n            -1.51785  \n\nDegrees of Freedom: 14273 Total (i.e. Null);  14270 Residual\nNull Deviance:      36120000 \nResidual Deviance: 19960000     AIC: 20040000"
  },
  {
    "objectID": "Take-Home_Ex/Take-Home_Ex1/data/geospatial/hexagon.html",
    "href": "Take-Home_Ex/Take-Home_Ex1/data/geospatial/hexagon.html",
    "title": "ISSS624",
    "section": "",
    "text": "&lt;!DOCTYPE qgis PUBLIC ‘http://mrcc.com/qgis.dtd’ ‘SYSTEM’&gt;     dataset\n\n\n                 +proj=tmerc +lat_0=1.36666666666667 +lon_0=103.833333333333 +k=1 +x_0=28001.642 +y_0=38744.572 +ellps=WGS84 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs 0 0     false"
  }
]